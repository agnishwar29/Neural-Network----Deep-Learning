{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc9d9e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4000000000000001"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Single Neuron\n",
    "\n",
    "inputs = [1,2,3]\n",
    "weights = [0.2, 0.5, -0.6]\n",
    "\n",
    "bias = 2\n",
    "\n",
    "result = 0\n",
    "\n",
    "for i in range(len(inputs)):\n",
    "    result += inputs[i] * weights[i]\n",
    "\n",
    "result += bias\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bbc24b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.8       ,  5.5       , -1.29999995])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A layer of Neurons\n",
    "import numpy as np\n",
    "\n",
    "inputs = np.array([1,2,3,4])\n",
    "\n",
    "weights = np.array([[0.2, 0.5, -0.6, 0.1], [0.4, 0.1, -0.5, 0.6], [0.2, 0.5, -0.6, -0.3]] )\n",
    "bias = [2,4,0.5]\n",
    "\n",
    "output = np.dot(weights, inputs) + bias\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51044c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = [[1, 2, 3, 2.5],\n",
    "          [2, 5, -1, 2],\n",
    "         [-1.5, 2.7, 3.5, -0.5]]\n",
    "\n",
    "weights = [[0.2,2.1,-0.3,1.1],\n",
    "         [0.5,-1.4,-1.3,2.1],\n",
    "         [-1.1,1.7,0.5,-0.5]]\n",
    "\n",
    "biases = [2,3,0]\n",
    "\n",
    "layer1_output = np.dot(inputs, np.array(weights).T) +biases\n",
    "\n",
    "weights2 = [[0.1,1.3,-0.5],\n",
    "         [0.1,-1,3],\n",
    "         [1,-1,0.3]]\n",
    "\n",
    "biases2  = [-1, 2, -3]\n",
    "\n",
    "print(\"Layer 1\")\n",
    "print(layer1_output)\n",
    "\n",
    "layer2_output = np.dot(layer1_output, np.array(weights2).T) +biases2\n",
    "\n",
    "print(\"Layer 2\")\n",
    "print(layer2_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19780200",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nnfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623ba77c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897c94a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "x,y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "plt.scatter(x[:,0], x[:,1], c=y, cmap='brg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9d739f",
   "metadata": {},
   "source": [
    "## Dense Layer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38896ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        \n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        \n",
    "    \n",
    "# Create Dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1 = Layer_Dense(2,3)\n",
    "\n",
    "dense1.forward(X)\n",
    "\n",
    "print(dense1.output[:5])\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff9df4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6910b117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relu Activation Function Code\n",
    "\n",
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
    "\n",
    "output = []\n",
    "\n",
    "for i in inputs:\n",
    "    output.append(max(0,i))\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035533da",
   "metadata": {},
   "source": [
    "# Relu Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01abdbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an Activation function class\n",
    "class Activation_Relu:\n",
    "    \n",
    "    # The method which takes the input\n",
    "    def forward(self, inputs):\n",
    "        # returns output with the maximum of 0 and the input value\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "# Creating a basic dataset\n",
    "X,y = spiral_data(samples=500, classes=3)\n",
    "\n",
    "# Initializing the Layer_Dense class object\n",
    "dense1 = Layer_Dense(2,3)\n",
    "\n",
    "# Initializing the Activation_Relu class object\n",
    "activation1  = Activation_Relu()\n",
    "\n",
    "# forwarding the X value in the class\n",
    "dense1.forward(X)\n",
    "\n",
    "# passing the output from the dense layer class to the activation function\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# getting the output which will always be greater than zero or zero\n",
    "print(activation1.output[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4624f3",
   "metadata": {},
   "source": [
    "# Softmax Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ea398e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing Exponential function \n",
    "\n",
    "import math\n",
    "\n",
    "layer_outputs = [4.8, 1.21, 2.385]\n",
    "\n",
    "# e = 2.71828182846\n",
    "e = math.e\n",
    "exp_values = []\n",
    "\n",
    "for output in layer_outputs:\n",
    "    exp_values.append(e** output)\n",
    "    \n",
    "print(exp_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5229fae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the values\n",
    "\n",
    "\n",
    "norm_base = sum(exp_values)\n",
    "\n",
    "norm_values = []\n",
    "\n",
    "for value in exp_values:\n",
    "    norm_values.append(value/ norm_base)\n",
    "    \n",
    "print(\"norm_base \",norm_base)\n",
    "print(\"exp_values \", exp_values)\n",
    "print(\"norm_values \",norm_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d23f9ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7b3245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "   \n",
    "layer_outputs = [4.8, 1.21, 2.385]\n",
    "\n",
    "# here we are using the numpy's exp function and passing the values\n",
    "exp_values = np.exp(layer_outputs)\n",
    "\n",
    "# and for the normalized values dividing the exp values by the sum of exp_values\n",
    "norm_values = exp_values / np.sum(exp_values)\n",
    "    \n",
    "print(\"exp_values \", exp_values)\n",
    "print(\"norm_values \",norm_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f22c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daba054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# input \n",
    "layer_outputs = [[4.8, 1.21, 2.385],\n",
    "                [8.9, -1.81, 0.2],\n",
    "                [1.41, 1.051, 0.026]]\n",
    "\n",
    "# subtracting the max of the layer_output to solve the exponential value outbursting\n",
    "exp_values =  np.exp(inputs - np.max(layer_outputs, axis=1, keepdims=True))\n",
    "\n",
    "print(exp_values)\n",
    "\n",
    "norm_values = exp_values / np.sum(layer_outputs, axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ed2fbe",
   "metadata": {},
   "source": [
    "## Softmax Activation Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e5677b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        \n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "\n",
    "class Activation_Relu:\n",
    "    \n",
    "    # The method which takes the input\n",
    "    def forward(self, inputs):\n",
    "        # returns output with the maximum of 0 and the input value\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "        \n",
    "# Creating an Activation function class\n",
    "class Activation_Softmax:\n",
    "    \n",
    "    # The method which takes the input\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        \n",
    "        norm_vals = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        \n",
    "        self.output = norm_vals\n",
    "\n",
    "#.......................................................................\n",
    "        \n",
    "# Creating a basic dataset\n",
    "X,y = spiral_data(samples=500, classes=3)\n",
    "\n",
    "# Initializing the Layer_Dense class object\n",
    "dense1 = Layer_Dense(2,3)\n",
    "\n",
    "activation1 = Activation_Relu()\n",
    "\n",
    "dense2 = Layer_Dense(3,3)\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "print(activation2.output[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c701ff",
   "metadata": {},
   "source": [
    "## Categorical Crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb3724b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "b = 5.2\n",
    "\n",
    "print(np.log(b))\n",
    "\n",
    "print(math.e ** 1.6486586255873816)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9981fef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# creating a list of output from softmax activation function\n",
    "softmax_output = [0.7, 0.1, 0.2]\n",
    "\n",
    "# one-hot encoded vector list\n",
    "target_output = [1,0,0]\n",
    "\n",
    "# calculating the loss by multiplying the log(output) with the target output\n",
    "loss = -(math.log(softmax_output[0])*target_output[0] +\n",
    "         math.log(softmax_output[1])*target_output[1] +\n",
    "         math.log(softmax_output[2])*target_output[2])\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3e08fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "softmax_output = np.array([[0.7, 0.1, 0.2],\n",
    "                          [0.1, 0.6, 0.4],\n",
    "                          [0.02, 0.9, 0.08]])\n",
    "\n",
    "class_targets = [0, 1 , 1]\n",
    "\n",
    "print(-(np.log(softmax_output[range(len(softmax_output)), class_targets])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdd2d11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d8d648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "softmax_ouputs = np.array([[0.7, 0.1, 0.2],\n",
    "                          [0.1, 0.6, 0.4],\n",
    "                          [0.02, 0.9, 0.08]])\n",
    "\n",
    "\n",
    "class_targets = np.array([0,1,1])\n",
    "\n",
    "class_targets2 = np.array([0,1,0])\n",
    "\n",
    "\n",
    "# getting the max value from the softmax activation function output\n",
    "predictions = np.argmax(softmax_ouputs, axis =1)\n",
    "\n",
    "if len(class_targets.shape) == 2:\n",
    "    class_targets = np.argmax(class_targets, axis=1)\n",
    "    \n",
    "# getting the mean of predictions and class targets\n",
    "accuracy = np.mean(predictions == class_targets) \n",
    "accuracy2 = np.mean(predictions == class_targets2)\n",
    "\n",
    "print(\"Accuracy \",accuracy)\n",
    "print(\"Accuracy2 \", accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8f809c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        \n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "\n",
    "class Activation_Relu:\n",
    "    \n",
    "    # The method which takes the input\n",
    "    def forward(self, inputs):\n",
    "        # returns output with the maximum of 0 and the input value\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "        \n",
    "# Creating an Activation function class\n",
    "class Activation_Softmax:\n",
    "    \n",
    "    # The method which takes the input\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        \n",
    "        norm_vals = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        \n",
    "        self.output = norm_vals\n",
    "        \n",
    "        \n",
    "class Loss:\n",
    "    \n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        \n",
    "        return data_loss\n",
    "    \n",
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "    def forward(self, y_pred, y_target):\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        # clipping the values so we don't end up with inf error\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "\n",
    "        # checking if 1D array so we just need to get that index value\n",
    "        if len(y_target.shape) == 1:\n",
    "            correct = y_pred_clipped[range(samples), y_target]\n",
    "        # checking if one-hot-encoded values, then multiplying and summing up\n",
    "        elif len(y_target.shape) == 2:\n",
    "            correct = np.sum(y_pred_clipped * y_target, axis=1)\n",
    "        \n",
    "        negative_log = -np.log(correct)\n",
    "        \n",
    "        return negative_log\n",
    "\n",
    "\n",
    "class Accuracy:\n",
    "    def get_accuracy(self, y_pred, y_true):\n",
    "        \n",
    "        predictions = np.argmax(y_pred, axis=1)\n",
    "                \n",
    "        if len(y_true.shape) == 2:\n",
    "            actual = np.argmax(y_true, axis=1)\n",
    "        else:\n",
    "            actual = np.argmax(y_true)\n",
    "        \n",
    "        accuracy = np.mean(predictions == y)\n",
    "        \n",
    "        print(\"accuracy : \", accuracy)\n",
    "        \n",
    "\n",
    "#.......................................................................\n",
    "        \n",
    "# Creating a basic dataset\n",
    "X,y = spiral_data(samples=500, classes=3)\n",
    "\n",
    "# Initializing the Layer_Dense class object\n",
    "dense1 = Layer_Dense(2,3)\n",
    "\n",
    "activation1 = Activation_Relu()\n",
    "\n",
    "dense2 = Layer_Dense(3,3)\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "\n",
    "loss_function = Loss_CategoricalCrossEntropy()\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "print(\"Loss: \", loss)\n",
    "\n",
    "accuracy = Accuracy()\n",
    "accuracy.get_accuracy(activation2.output, np.array(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbfed26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ebdc1f8a",
   "metadata": {},
   "source": [
    "##  Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b1a299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    return 2*x\n",
    "\n",
    "x = np.arange(0, 50, 0.001)\n",
    "y = f(x)\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ecf3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 2*x**2\n",
    "\n",
    "x = np.arange(0, 50, 0.001)\n",
    "y = f(x)\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7851f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 2*x**2\n",
    "\n",
    "x = np.arange(0, 50, 0.001)\n",
    "y = f(x)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(x,y)\n",
    "\n",
    "p2_delta = 0.0001\n",
    "x1 = 2\n",
    "x2 = x1 + p2_delta\n",
    "\n",
    "y1 = f(x1)\n",
    "y2 = f(x2)\n",
    "\n",
    "print((x1, y1), (x2, y2))\n",
    "\n",
    "approximate_derivative = (y2-y1) / (x2-x1)\n",
    "b = y2 - approximate_derivative * x2\n",
    "\n",
    "def approximate_tangent_line(x):\n",
    "    return approximate_derivative * x + b # max + b\n",
    "\n",
    "to_plot = [x1-0.9, x1, x1+0.9]\n",
    "\n",
    "plt.plot(to_plot, [approximate_tangent_line(point) for point in to_plot])\n",
    "\n",
    "print(\"Approximate derivarive for f(x)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69410f89",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e68097",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------Forward pass-------------------------------\n",
    "\n",
    "x = [1.0, -2.0, 3.0]  #input values\n",
    "weights = [-3.0, -1.0, 2.0]  #weights\n",
    "bias = 1.0  #bias\n",
    "\n",
    "# multiplying inputs by weights\n",
    "xw0, xw1, xw2 = [val * weight for val, weight in zip(x, weights)]\n",
    "\n",
    "# adding weighted inputs and the bias\n",
    "z = sum([xw0, xw1, xw2]) + bias\n",
    "\n",
    "# applying relu activation function\n",
    "y = max(z, 0)\n",
    "\n",
    "\n",
    "#------------------------Backward pass-------------------------\n",
    "\n",
    "\n",
    "# the derivative from the next layer\n",
    "\n",
    "dvalue = 1.0 # d -> derivative\n",
    "\n",
    "drelu_dz = dvalue * (1. if z > 0 else 0.)\n",
    "print(drelu_dz)\n",
    "\n",
    "# Partial derivative of the multiplication of the inputs and the weights\n",
    "dsum_dxw0 = 1\n",
    "dsum_dxw1 = 1\n",
    "dsum_dxw2 = 1\n",
    "\n",
    "# partial derivative of the bias also\n",
    "dsum_db = 1\n",
    "\n",
    "drelu_dxw0 = drelu_dz * dsum_dxw0\n",
    "drelu_dxw1 = drelu_dz * dsum_dxw1\n",
    "drelu_dxw2 = drelu_dz * dsum_dxw2\n",
    "drelu_db = drelu_dz * dsum_db\n",
    "\n",
    "print(drelu_dxw0,drelu_dxw1,drelu_dxw2)\n",
    "\n",
    "drelu_dx0 = drelu_dxw0 * x[0]\n",
    "drelu_dw0 = drelu_dxw0 * weights[0]\n",
    "\n",
    "drelu_dx1 = drelu_dxw1 * x[1]\n",
    "drelu_dw1 = drelu_dxw1 * weights[1]\n",
    "\n",
    "drelu_dx2 = drelu_dxw0 * x[2]\n",
    "drelu_dw2 = drelu_dxw0 * weights[2]\n",
    "\n",
    "\n",
    "print(drelu_dw0, drelu_dx0, drelu_dw1, drelu_dx1, drelu_dw2, drelu_dx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37961cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = [drelu_dx0, drelu_dx1, drelu_dx2]\n",
    "dw = [drelu_dw0, drelu_dw1, drelu_dw2]\n",
    "\n",
    "db = drelu_db\n",
    "\n",
    "print(weights,bias)\n",
    "\n",
    "# tweaking the parametere a bit\n",
    "weights[0] += -0.001 * dw[0]\n",
    "weights[1] += -0.001 * dw[1]\n",
    "weights[2] += -0.001 * dw[2]\n",
    "\n",
    "bias += -0.001 * db\n",
    "\n",
    "print(weights,bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b89864",
   "metadata": {},
   "outputs": [],
   "source": [
    "xw0 = x[0] * weights[0]\n",
    "xw1 = x[1] * weights[1]\n",
    "xw2 = x[2] * weights[2]\n",
    "\n",
    "z = sum([xw0,xw1,xw2]) + bias\n",
    "\n",
    "y = max(z, 0)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4ba9a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c172dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# The gradient from the next layer\n",
    "dvalues = np.array([[1., 1., 1.]])\n",
    "\n",
    "# Three sets of weights, for each set we have 4 weights\n",
    "weights = np.array([[0.2, 0.8, -0.5, 1],[0.5, -0.91, 0.26, -0.5],[-0.26, -0.27, 0.17, 0.87]]).T\n",
    "\n",
    "weights\n",
    "\n",
    "dinputs = np.dot(dvalues[0], weights.T)\n",
    "\n",
    "print(dinputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69405e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9261523f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3282d7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    \n",
    "    def __init__(self, inputs, neurons):\n",
    "        self.weights = 0.01 * np.random.randn(inputs, neurons)\n",
    "        self.biases = np.zeros((1,neurons))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        self.inputs = inputs\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims = True)\n",
    "        \n",
    "        # Gradient values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "        \n",
    "class Activation_ReLu:\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8410ff1f",
   "metadata": {},
   "source": [
    "## Categorical Cross-Entropy Loss Derivatice Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8609a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    \n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        \n",
    "        return data_loss\n",
    "\n",
    "    \n",
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "        \n",
    "        if len(y_true.shape) ==1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "            \n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd202f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.eye(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9c9885",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.eye(5)[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe04554",
   "metadata": {},
   "source": [
    "## Softmax Activation derivative code implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "30825339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7 0.  0. ]\n",
      " [0.  0.1 0. ]\n",
      " [0.  0.  0.2]]\n",
      "[[0.49 0.07 0.14]\n",
      " [0.07 0.01 0.02]\n",
      " [0.14 0.02 0.04]]\n",
      "[[ 0.20999999 -0.07       -0.14      ]\n",
      " [-0.07        0.09       -0.02      ]\n",
      " [-0.14       -0.02        0.16      ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# sample output\n",
    "softmax_outputs = [0.7, 0.1, 0.2]\n",
    "\n",
    "# shaping it as list of samples\n",
    "softmax_outputs = np.array(softmax_outputs).reshape(-1,1)\n",
    "\n",
    "# first approach\n",
    "#print(softmax_ouputs * np.eye(softmax_ouputs.shape[0])) \n",
    "\n",
    "# second approach\n",
    "print(np.diagflat(softmax_outputs))\n",
    "\n",
    "print(np.dot(softmax_outputs, softmax_outputs.T))\n",
    "\n",
    "print(np.diagflat(softmax_outputs) - np.dot(softmax_outputs, softmax_outputs.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451eef12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "class Layer_Dense:\n",
    "    \n",
    "    # getting the number of inputs and neurons\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        \n",
    "        #initializing random weights in range \n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "    # getting the output with (mx +b)\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    \n",
    "    # backward pass which takes the derivative values from the next function \n",
    "    def backward(self, dvalues):\n",
    "        # getting the partial derivative with the inputs\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims = True)\n",
    "        \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Activation_Relu:\n",
    "    \n",
    "    # The method which takes the input\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        self.inputs = inputs\n",
    "        # returns output with the maximum of 0 and the input value\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <=0] = 0\n",
    "        \n",
    "#---------------------------------------------------------------------\n",
    "        \n",
    "# Creating an Activation function class\n",
    "class Activation_Softmax:\n",
    "    \n",
    "    # The method which takes the input\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        \n",
    "        norm_vals = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        \n",
    "        self.output = norm_vals\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        \n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "                \n",
    "                single_output = single_output.reshape(-1,1)\n",
    "                \n",
    "                jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "                \n",
    "                self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "                \n",
    "#---------------------------------------------------------------------\n",
    "                \n",
    "class Loss:\n",
    "    \n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        \n",
    "        return data_loss\n",
    "\n",
    "    \n",
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "    \n",
    "    def forward(self, y_pred, y_target):\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        # clipping the values so we don't end up with inf error\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "\n",
    "        # checking if 1D array so we just need to get that index value\n",
    "        if len(y_target.shape) == 1:\n",
    "            correct = y_pred_clipped[range(samples), y_target]\n",
    "        # checking if one-hot-encoded values, then multiplying and summing up\n",
    "        elif len(y_target.shape) == 2:\n",
    "            correct = np.sum(y_pred_clipped * y_target, axis=1)\n",
    "        \n",
    "        negative_log = -np.log(correct)\n",
    "        \n",
    "        return negative_log\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "        \n",
    "        if len(y_true.shape) ==1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "            \n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples\n",
    "    \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Accuracy:\n",
    "    def get_accuracy(self, y_pred, y_true):\n",
    "        \n",
    "        predictions = np.argmax(y_pred, axis=1)\n",
    "                \n",
    "        if len(y_true.shape) == 2:\n",
    "            actual = np.argmax(y_true, axis=1)\n",
    "        else:\n",
    "            actual = np.argmax(y_true)\n",
    "        \n",
    "        accuracy = np.mean(predictions == y)\n",
    "        \n",
    "        print(\"accuracy : \", accuracy)\n",
    "        \n",
    "\n",
    "#.......................................................................\n",
    "\n",
    "class Activation_Softmax_Loss_CategoricalCrossEntropy():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # initializing the activation function\n",
    "        self.activation = Activation_Softmax()\n",
    "        \n",
    "        # initializing the loss function\n",
    "        self.loss = Loss_CategoricalCrossEntropy()\n",
    "    \n",
    "    # forward pass method, takes the input from \n",
    "    # the dense layer and the actual value\n",
    "    def forward(self, inputs, y_true):\n",
    "        \n",
    "        # forward passing the inputs in the activation function\n",
    "        self.activation.forward(inputs)\n",
    "        \n",
    "        # getting the output from the activation function\n",
    "        self.output = self.activation.output\n",
    "        \n",
    "        # calculating loss with the activation function output and actual value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        # checking if the values are one-hot-encoded\n",
    "        if len(y_true.shape) ==2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "            \n",
    "        \n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "        # calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -=1\n",
    "        \n",
    "        # normalizing gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "    \n",
    "\n",
    "#.......................................................................\n",
    "\n",
    "\n",
    "        \n",
    "# Creating a basic dataset\n",
    "X,y = spiral_data(samples=500, classes=3)\n",
    "\n",
    "# Initializing the Layer_Dense class object\n",
    "dense1 = Layer_Dense(2,3)\n",
    "\n",
    "activation1 = Activation_Relu()\n",
    "\n",
    "dense2 = Layer_Dense(3,3)\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# print(\"Output: \", activation2.output[:5])\n",
    "\n",
    "loss_function = Loss_CategoricalCrossEntropy()\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "print(\"Loss: \", loss)\n",
    "\n",
    "accuracy = Accuracy()\n",
    "accuracy.get_accuracy(activation2.output, np.array(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5616f710",
   "metadata": {},
   "source": [
    "Common Categorical Cross Entropy and Softmax Activation Derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224676f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax_Loss_CategoricalCrossEntropy():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # initializing the activation function\n",
    "        self.activation = Activation_Softmax()\n",
    "        \n",
    "        # initializing the loss function\n",
    "        self.loss = Loss_CategoricalCrossEntropy()\n",
    "    \n",
    "    # forward pass method, takes the input from \n",
    "    # the dense layer and the actual value\n",
    "    def forward(self, inputs, y_true):\n",
    "        \n",
    "        # forward passing the inputs in the activation function\n",
    "        self.activation.forward(inputs)\n",
    "        \n",
    "        # getting the output from the activation function\n",
    "        self.output = self.activation.output\n",
    "        \n",
    "        # calculating loss with the activation function output and actual value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        # checking if the values are one-hot-encoded\n",
    "        if len(y_true.shape) ==2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "            \n",
    "        \n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "        # calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -=1\n",
    "        \n",
    "        # normalizing gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7805aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_true = np.array([[1,0,0], [0,0,1], [0,1,0]])\n",
    "\n",
    "np.argmax(y_true, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0b1c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "# creating a softmax output arrray\n",
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                          [0.1, 0.6, 0.4],\n",
    "                          [0.02, 0.9, 0.08]])\n",
    "# class targets\n",
    "class_targets = np.array([0,1,1])\n",
    "\n",
    "#----------------Combined Activation and Loss--------------------------\n",
    "\n",
    "\n",
    "# initializing the combined activation and loss function\n",
    "softmax_loss = Activation_Softmax_Loss_CategoricalCrossEntropy()\n",
    "\n",
    "# backward passing the softmax outputs and the targets\n",
    "softmax_loss.backward(softmax_ouputs, class_targets)\n",
    "\n",
    "# storing the gradients the dinputs\n",
    "dvalues1 = softmax_loss.dinputs\n",
    "\n",
    "#----------------Seperate Activation and Loss--------------------------\n",
    "\n",
    "# Initializing the activation function\n",
    "activation = Activation_Softmax()\n",
    "\n",
    "# for testing purpose setting the activation output as softmax output we created\n",
    "activation.output = softmax_ouputs\n",
    "\n",
    "# initializing the loss function\n",
    "loss = Loss_CategoricalCrossEntropy()\n",
    "\n",
    "# calling the backward pass for loss function \n",
    "# passing the activation function output and class targets\n",
    "loss.backward(softmax_ouputs, class_targets)\n",
    "\n",
    "# calling the backward pass for the activation function\n",
    "activation.backward(loss.dinputs)\n",
    "\n",
    "# storing the gradients\n",
    "dvalues2 = activation.dinputs\n",
    "\n",
    "print(\"Gradients: combined loss and activation: \")\n",
    "print(dvalues1)\n",
    "print(\"------------------------------------------\")\n",
    "print(\"Gradients: separate loss and activation: \")\n",
    "print(dvalues2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed305524",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.empty_like(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd1a758",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "334eae0a",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d79098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "class Layer_Dense:\n",
    "    \n",
    "    # getting the number of inputs and neurons\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        \n",
    "        #initializing random weights in range \n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "    # getting the output with (mx +b)\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    \n",
    "    # backward pass which takes the derivative values from the next function \n",
    "    def backward(self, dvalues):\n",
    "        # getting the partial derivative with the inputs\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims = True)\n",
    "        \n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "        \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Activation_Relu:\n",
    "    \n",
    "    # The method which takes the input\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        self.inputs = inputs\n",
    "        # returns output with the maximum of 0 and the input value\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <=0] = 0\n",
    "        \n",
    "#---------------------------------------------------------------------\n",
    "        \n",
    "# Creating an Activation function class\n",
    "class Activation_Softmax:\n",
    "    \n",
    "    # The method which takes the input\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        \n",
    "        norm_vals = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        \n",
    "        self.output = norm_vals\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        \n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "                \n",
    "                single_output = single_output.reshape(-1,1)\n",
    "                \n",
    "                jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "                \n",
    "                self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "                \n",
    "#---------------------------------------------------------------------\n",
    "                \n",
    "class Loss:\n",
    "    \n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        \n",
    "        return data_loss\n",
    "\n",
    "    \n",
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "    \n",
    "    def forward(self, y_pred, y_target):\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        # clipping the values so we don't end up with inf error\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "\n",
    "        # checking if 1D array so we just need to get that index value\n",
    "        if len(y_target.shape) == 1:\n",
    "            correct = y_pred_clipped[range(samples), y_target]\n",
    "        # checking if one-hot-encoded values, then multiplying and summing up\n",
    "        elif len(y_target.shape) == 2:\n",
    "            correct = np.sum(y_pred_clipped * y_target, axis=1)\n",
    "        \n",
    "        negative_log = -np.log(correct)\n",
    "        \n",
    "        return negative_log\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "        \n",
    "        if len(y_true.shape) ==1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "            \n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples\n",
    "    \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Accuracy:\n",
    "    def get_accuracy(self,y_pred, y_true):\n",
    "        \n",
    "        predictions = np.argmax(y_pred, axis=1)\n",
    "                \n",
    "        if len(y_true.shape) == 2:\n",
    "            actual = np.argmax(y_true, axis=1)\n",
    "        else:\n",
    "            actual = np.argmax(y_true)\n",
    "        \n",
    "        accuracy = np.mean(predictions == y)\n",
    "        \n",
    "        print(\"accuracy : \", accuracy)\n",
    "        \n",
    "\n",
    "#.......................................................................\n",
    "\n",
    "class Activation_Softmax_Loss_CategoricalCrossEntropy():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # initializing the activation function\n",
    "        self.activation = Activation_Softmax()\n",
    "        \n",
    "        # initializing the loss function\n",
    "        self.loss = Loss_CategoricalCrossEntropy()\n",
    "    \n",
    "    # forward pass method, takes the input from \n",
    "    # the dense layer and the actual value\n",
    "    def forward(self, inputs, y_true):\n",
    "        \n",
    "        # forward passing the inputs in the activation function\n",
    "        self.activation.forward(inputs)\n",
    "        \n",
    "        # getting the output from the activation function\n",
    "        self.output = self.activation.output\n",
    "        \n",
    "        # calculating loss with the activation function output and actual value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        # checking if the values are one-hot-encoded\n",
    "        if len(y_true.shape) ==2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "            \n",
    "        \n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "        # calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -=1\n",
    "        \n",
    "        # normalizing gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "    \n",
    "class Optimizer_SGD:\n",
    "    \n",
    "    # setting learning rate default as 1.\n",
    "    def __init__(self, learning_rate = 1.0):\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases += -self.learning_rate * layer.dbiases\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04993ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the dataset that has three classes\n",
    "X, y = spiral_data(samples = 100, classes=3)\n",
    "\n",
    "# creating the first layer with 2 inputs and 64 output values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "\n",
    "# initializing the activation function\n",
    "activation1 = Activation_ReLu()\n",
    "\n",
    "# creating the second layer with 64 inputs and 3 output values\n",
    "# as we have 64 outputs from the previous layer dense1\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "# initializing the loss activation function\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossEntropy()\n",
    "\n",
    "# initializing the optimizer SGD\n",
    "optimizer = Optimizer_SGD()\n",
    "\n",
    "\n",
    "# Performing the action --------------------\n",
    "\n",
    "# forwarding the data with the dense layer 1\n",
    "dense1.forward(X)\n",
    "\n",
    "# forwarding the output from the dense layer1 with the activation function\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# forwarding the activation1 function output with the dense layer 2\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# generating the loss from the dense layer 2 output and the truth\n",
    "loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "# pringing the loss value till this point\n",
    "print(\"Loss :\", loss)\n",
    "\n",
    "\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(y, axis=1)\n",
    "    \n",
    "accuracy = np.mean(predictions == y)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "\n",
    "#---------------------Backpropagation----------------------\n",
    "\n",
    "# backward passing from the loss activation function\n",
    "loss_activation.backward(loss_activation.output, y)\n",
    "\n",
    "# backward passing from dense2 layer with the gradient\n",
    "# from loss activation function\n",
    "dense2.backward(loss_activation.dinputs)\n",
    "\n",
    "# backward passing the activation 1 function with the gradient from dense layer 2\n",
    "activation1.backward(dense2.dinputs)\n",
    "\n",
    "# backward passing the gradient from activation1 with the dense layer 1\n",
    "dense1.backward(activation1.dinputs)\n",
    "\n",
    "# updating the weights and biases of dense layer 1 and 2\n",
    "optimizer.update_params(dense1)\n",
    "optimizer.update_params(dense2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9683702",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db76ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the dataset that has three classes\n",
    "X, y = spiral_data(samples = 100, classes=3)\n",
    "\n",
    "# creating the first layer with 2 inputs and 64 output values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "\n",
    "# initializing the activation function\n",
    "activation1 = Activation_ReLu()\n",
    "\n",
    "# creating the second layer with 64 inputs and 3 output values\n",
    "# as we have 64 outputs from the previous layer dense1\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "# initializing the loss activation function\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossEntropy()\n",
    "\n",
    "# initializing the optimizer SGD\n",
    "optimizer = Optimizer_SGD()\n",
    "\n",
    "for epoch in range(10001):\n",
    "    \n",
    "    dense1.forward(X)\n",
    "    \n",
    "    activation1.forward(dense1.output)\n",
    "    \n",
    "    dense2.forward(activation1.output)\n",
    "    \n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "    \n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "\n",
    "    accuracy = np.mean(predictions == y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'Epoch: {epoch}, ' + f'acc: {accuracy}, '+ f'loss: {loss}')\n",
    "    \n",
    "    \n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "\n",
    "    # backward passing from dense2 layer with the gradient\n",
    "    # from loss activation function\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "\n",
    "    # backward passing the activation 1 function with the gradient from dense layer 2\n",
    "    activation1.backward(dense2.dinputs)\n",
    "\n",
    "    # backward passing the gradient from activation1 with the dense layer 1\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # updating the weights and biases of dense layer 1 and 2\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b013e90",
   "metadata": {},
   "source": [
    "# Learning Rate Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fe57db",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_learning_rate = 1.\n",
    "learning_rate_decay = 0.1\n",
    "step =20\n",
    "\n",
    "learning_rate = starting_learning_rate * (1. / (1 + learning_rate_decay * step))\n",
    "\n",
    "print(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4990a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_learning_rate = 1.\n",
    "learning_rate_decay = 0.1\n",
    "\n",
    "for step in range(20):\n",
    "    learning_rate = starting_learning_rate * (1. / (1 + learning_rate_decay * step))\n",
    "    print(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5982ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_SGD:\n",
    "    \n",
    "    # setting learning rate default as 1.\n",
    "    def __init__(self, learning_rate = 1.0, decay=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        \n",
    "    def pre_update_parameters(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "        \n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases\n",
    "        \n",
    "    def post_update_parameters(self):\n",
    "        self.iterations +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50d78ef",
   "metadata": {},
   "source": [
    "# Training the Model with updated Optimizer added with Learning Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4ab66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np \n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "class Layer_Dense:\n",
    "    \n",
    "    # getting the number of inputs and neurons\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        \n",
    "        #initializing random weights in range \n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "    # getting the output with (mx +b)\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    \n",
    "    # backward pass which takes the derivative values from the next function \n",
    "    def backward(self, dvalues):\n",
    "        # getting the partial derivative with the inputs\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims = True)\n",
    "        \n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "        \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Activation_Relu:\n",
    "    \n",
    "    # The method which takes the input\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        self.inputs = inputs\n",
    "        # returns output with the maximum of 0 and the input value\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <=0] = 0\n",
    "        \n",
    "#---------------------------------------------------------------------\n",
    "        \n",
    "# Creating an Activation function class\n",
    "class Activation_Softmax:\n",
    "    \n",
    "    # The method which takes the input\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        \n",
    "        norm_vals = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        \n",
    "        self.output = norm_vals\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        \n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "                \n",
    "                single_output = single_output.reshape(-1,1)\n",
    "                \n",
    "                jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "                \n",
    "                self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "                \n",
    "#---------------------------------------------------------------------\n",
    "                \n",
    "class Loss:\n",
    "    \n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        \n",
    "        return data_loss\n",
    "\n",
    "    \n",
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "    \n",
    "    def forward(self, y_pred, y_target):\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        # clipping the values so we don't end up with inf error\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "\n",
    "        # checking if 1D array so we just need to get that index value\n",
    "        if len(y_target.shape) == 1:\n",
    "            correct = y_pred_clipped[range(samples), y_target]\n",
    "        # checking if one-hot-encoded values, then multiplying and summing up\n",
    "        elif len(y_target.shape) == 2:\n",
    "            correct = np.sum(y_pred_clipped * y_target, axis=1)\n",
    "        \n",
    "        negative_log = -np.log(correct)\n",
    "        \n",
    "        return negative_log\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "        \n",
    "        if len(y_true.shape) ==1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "            \n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples\n",
    "    \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Accuracy:\n",
    "    def get_accuracy(self,y_pred, y_true):\n",
    "        \n",
    "        predictions = np.argmax(y_pred, axis=1)\n",
    "                \n",
    "        if len(y_true.shape) == 2:\n",
    "            actual = np.argmax(y_true, axis=1)\n",
    "        else:\n",
    "            actual = np.argmax(y_true)\n",
    "        \n",
    "        accuracy = np.mean(predictions == y)\n",
    "        \n",
    "        print(\"accuracy : \", accuracy)\n",
    "        \n",
    "\n",
    "#.......................................................................\n",
    "\n",
    "class Activation_Softmax_Loss_CategoricalCrossEntropy():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # initializing the activation function\n",
    "        self.activation = Activation_Softmax()\n",
    "        \n",
    "        # initializing the loss function\n",
    "        self.loss = Loss_CategoricalCrossEntropy()\n",
    "    \n",
    "    # forward pass method, takes the input from \n",
    "    # the dense layer and the actual value\n",
    "    def forward(self, inputs, y_true):\n",
    "        \n",
    "        # forward passing the inputs in the activation function\n",
    "        self.activation.forward(inputs)\n",
    "        \n",
    "        # getting the output from the activation function\n",
    "        self.output = self.activation.output\n",
    "        \n",
    "        # calculating loss with the activation function output and actual value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        # checking if the values are one-hot-encoded\n",
    "        if len(y_true.shape) ==2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "            \n",
    "        \n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "        # calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -=1\n",
    "        \n",
    "        # normalizing gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "    \n",
    "class Optimizer_SGD:\n",
    "    \n",
    "    # setting learning rate default as 1.\n",
    "    def __init__(self, learning_rate = 1.0, decay=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        \n",
    "    def pre_update_parameters(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "        \n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases\n",
    "        \n",
    "    def post_update_parameters(self):\n",
    "        self.iterations +=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97eb386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the dataset that has three classes\n",
    "X, y = spiral_data(samples = 100, classes=3)\n",
    "\n",
    "# creating the first layer with 2 inputs and 64 output values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "\n",
    "# initializing the activation function\n",
    "activation1 = Activation_Relu()\n",
    "\n",
    "# creating the second layer with 64 inputs and 3 output values\n",
    "# as we have 64 outputs from the previous layer dense1\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "# initializing the loss activation function\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossEntropy()\n",
    "\n",
    "# initializing the optimizer SGD\n",
    "optimizer = Optimizer_SGD(decay = 1e-3)\n",
    "\n",
    "for epoch in range(10001):\n",
    "    \n",
    "    dense1.forward(X)\n",
    "    \n",
    "    activation1.forward(dense1.output)\n",
    "    \n",
    "    dense2.forward(activation1.output)\n",
    "    \n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "    \n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "\n",
    "    accuracy = np.mean(predictions == y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'Epoch: {epoch}, ' + f'acc: {accuracy}, '+ f'loss: {loss}', f'lr: {optimizer.current_learning_rate} ')\n",
    "    \n",
    "    \n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "\n",
    "    # backward passing from dense2 layer with the gradient\n",
    "    # from loss activation function\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "\n",
    "    # backward passing the activation 1 function with the gradient from dense layer 2\n",
    "    activation1.backward(dense2.dinputs)\n",
    "\n",
    "    # backward passing the gradient from activation1 with the dense layer 1\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # updating the weights and biases of dense layer 1 and 2\n",
    "    optimizer.pre_update_parameters()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0486d1",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00568b33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd31b766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np \n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "class Layer_Dense:\n",
    "    \n",
    "    # getting the number of inputs and neurons\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        \n",
    "        #initializing random weights in range \n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "    # getting the output with (mx +b)\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    \n",
    "    # backward pass which takes the derivative values from the next function \n",
    "    def backward(self, dvalues):\n",
    "        # getting the partial derivative with the inputs\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims = True)\n",
    "        \n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "        \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Activation_Relu:\n",
    "    \n",
    "    # The method which takes the input\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        self.inputs = inputs\n",
    "        # returns output with the maximum of 0 and the input value\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <=0] = 0\n",
    "        \n",
    "#---------------------------------------------------------------------\n",
    "        \n",
    "# Creating an Activation function class\n",
    "class Activation_Softmax:\n",
    "    \n",
    "    # The method which takes the input\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        \n",
    "        norm_vals = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        \n",
    "        self.output = norm_vals\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        \n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "                \n",
    "                single_output = single_output.reshape(-1,1)\n",
    "                \n",
    "                jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "                \n",
    "                self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "                \n",
    "#---------------------------------------------------------------------\n",
    "                \n",
    "class Loss:\n",
    "    \n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        \n",
    "        return data_loss\n",
    "\n",
    "    \n",
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "    \n",
    "    def forward(self, y_pred, y_target):\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        # clipping the values so we don't end up with inf error\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "\n",
    "        # checking if 1D array so we just need to get that index value\n",
    "        if len(y_target.shape) == 1:\n",
    "            correct = y_pred_clipped[range(samples), y_target]\n",
    "        # checking if one-hot-encoded values, then multiplying and summing up\n",
    "        elif len(y_target.shape) == 2:\n",
    "            correct = np.sum(y_pred_clipped * y_target, axis=1)\n",
    "        \n",
    "        negative_log = -np.log(correct)\n",
    "        \n",
    "        return negative_log\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "        \n",
    "        if len(y_true.shape) ==1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "            \n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples\n",
    "    \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Accuracy:\n",
    "    def get_accuracy(self,y_pred, y_true):\n",
    "        \n",
    "        predictions = np.argmax(y_pred, axis=1)\n",
    "                \n",
    "        if len(y_true.shape) == 2:\n",
    "            actual = np.argmax(y_true, axis=1)\n",
    "        else:\n",
    "            actual = np.argmax(y_true)\n",
    "        \n",
    "        accuracy = np.mean(predictions == y)\n",
    "        \n",
    "        print(\"accuracy : \", accuracy)\n",
    "        \n",
    "\n",
    "#.......................................................................\n",
    "\n",
    "class Activation_Softmax_Loss_CategoricalCrossEntropy():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # initializing the activation function\n",
    "        self.activation = Activation_Softmax()\n",
    "        \n",
    "        # initializing the loss function\n",
    "        self.loss = Loss_CategoricalCrossEntropy()\n",
    "    \n",
    "    # forward pass method, takes the input from \n",
    "    # the dense layer and the actual value\n",
    "    def forward(self, inputs, y_true):\n",
    "        \n",
    "        # forward passing the inputs in the activation function\n",
    "        self.activation.forward(inputs)\n",
    "        \n",
    "        # getting the output from the activation function\n",
    "        self.output = self.activation.output\n",
    "        \n",
    "        # calculating loss with the activation function output and actual value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        # checking if the values are one-hot-encoded\n",
    "        if len(y_true.shape) ==2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "            \n",
    "        \n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "        # calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -=1\n",
    "        \n",
    "        # normalizing gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "    \n",
    "#---------------------------------------------------------------------\n",
    "    \n",
    "class Optimizer_SGD:\n",
    "    \n",
    "    # setting learning rate default as 1.\n",
    "    def __init__(self, learning_rate = 1.0, decay=0., momentum =0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def pre_update_parameters(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "        \n",
    "    def update_params(self, layer):\n",
    "        \n",
    "        # if we use momentum\n",
    "        if self.momentum:\n",
    "            # if the layer does not contain momentum arrays\n",
    "            if not hasattr(layer, \"weight_momentums\"):\n",
    "                # creating weight_momentums and bias_momentums arrays initialized with zero\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            \n",
    "            # getting the weight updates with momentum - take previous factors\n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "            \n",
    "            # getting bias updates\n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "        \n",
    "        else:\n",
    "\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates =  -self.current_learning_rate * layer.dbiases\n",
    "            \n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "        \n",
    "    def post_update_parameters(self):\n",
    "        self.iterations +=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ba972d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating the dataset that has three classes\n",
    "X, y = spiral_data(samples = 100, classes=3)\n",
    "\n",
    "# creating the first layer with 2 inputs and 64 output values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "\n",
    "# initializing the activation function\n",
    "activation1 = Activation_Relu()\n",
    "\n",
    "# creating the second layer with 64 inputs and 3 output values\n",
    "# as we have 64 outputs from the previous layer dense1\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "# initializing the loss activation function\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossEntropy()\n",
    "\n",
    "# initializing the optimizer SGD\n",
    "optimizer = Optimizer_SGD(decay = 1e-3, momentum=0.5)\n",
    "\n",
    "for epoch in range(10001):\n",
    "    \n",
    "    dense1.forward(X)\n",
    "    \n",
    "    activation1.forward(dense1.output)\n",
    "    \n",
    "    dense2.forward(activation1.output)\n",
    "    \n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "    \n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "\n",
    "    accuracy = np.mean(predictions == y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'Epoch: {epoch}, ' + f'acc: {accuracy}, '+ f'loss: {loss}', f'lr: {optimizer.current_learning_rate} ')\n",
    "    \n",
    "    \n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "\n",
    "    # backward passing from dense2 layer with the gradient\n",
    "    # from loss activation function\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "\n",
    "    # backward passing the activation 1 function with the gradient from dense layer 2\n",
    "    activation1.backward(dense2.dinputs)\n",
    "\n",
    "    # backward passing the gradient from activation1 with the dense layer 1\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # updating the weights and biases of dense layer 1 and 2\n",
    "    optimizer.pre_update_parameters()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761821aa",
   "metadata": {},
   "source": [
    "# AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6d6eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nnfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee0961d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np \n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "class Layer_Dense:\n",
    "    \n",
    "    # getting the number of inputs and neurons\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        \n",
    "        #initializing random weights in range \n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "    # getting the output with (mx +b)\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    \n",
    "    # backward pass which takes the derivative values from the next function \n",
    "    def backward(self, dvalues):\n",
    "        # getting the partial derivative with the inputs\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims = True)\n",
    "        \n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "        \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Activation_Relu:\n",
    "    \n",
    "    # The method which takes the input\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        self.inputs = inputs\n",
    "        # returns output with the maximum of 0 and the input value\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <=0] = 0\n",
    "        \n",
    "#---------------------------------------------------------------------\n",
    "        \n",
    "# Creating an Activation function class\n",
    "class Activation_Softmax:\n",
    "    \n",
    "    # The method which takes the input\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        \n",
    "        norm_vals = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        \n",
    "        self.output = norm_vals\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        \n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "                \n",
    "                single_output = single_output.reshape(-1,1)\n",
    "                \n",
    "                jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "                \n",
    "                self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "                \n",
    "#---------------------------------------------------------------------\n",
    "                \n",
    "class Loss:\n",
    "    \n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        \n",
    "        return data_loss\n",
    "\n",
    "    \n",
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "    \n",
    "    def forward(self, y_pred, y_target):\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        # clipping the values so we don't end up with inf error\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "\n",
    "        # checking if 1D array so we just need to get that index value\n",
    "        if len(y_target.shape) == 1:\n",
    "            correct = y_pred_clipped[range(samples), y_target]\n",
    "        # checking if one-hot-encoded values, then multiplying and summing up\n",
    "        elif len(y_target.shape) == 2:\n",
    "            correct = np.sum(y_pred_clipped * y_target, axis=1)\n",
    "        \n",
    "        negative_log = -np.log(correct)\n",
    "        \n",
    "        return negative_log\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "        \n",
    "        if len(y_true.shape) ==1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "            \n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples\n",
    "    \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Accuracy:\n",
    "    def get_accuracy(self,y_pred, y_true):\n",
    "        \n",
    "        predictions = np.argmax(y_pred, axis=1)\n",
    "                \n",
    "        if len(y_true.shape) == 2:\n",
    "            actual = np.argmax(y_true, axis=1)\n",
    "        else:\n",
    "            actual = np.argmax(y_true)\n",
    "        \n",
    "        accuracy = np.mean(predictions == y)\n",
    "        \n",
    "        print(\"accuracy : \", accuracy)\n",
    "        \n",
    "\n",
    "#.......................................................................\n",
    "\n",
    "class Activation_Softmax_Loss_CategoricalCrossEntropy():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # initializing the activation function\n",
    "        self.activation = Activation_Softmax()\n",
    "        \n",
    "        # initializing the loss function\n",
    "        self.loss = Loss_CategoricalCrossEntropy()\n",
    "    \n",
    "    # forward pass method, takes the input from \n",
    "    # the dense layer and the actual value\n",
    "    def forward(self, inputs, y_true):\n",
    "        \n",
    "        # forward passing the inputs in the activation function\n",
    "        self.activation.forward(inputs)\n",
    "        \n",
    "        # getting the output from the activation function\n",
    "        self.output = self.activation.output\n",
    "        \n",
    "        # calculating loss with the activation function output and actual value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        # checking if the values are one-hot-encoded\n",
    "        if len(y_true.shape) ==2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "            \n",
    "        \n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "        # calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -=1\n",
    "        \n",
    "        # normalizing gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "    \n",
    "#---------------------------------------------------------------------\n",
    "    \n",
    "class Optimizer_SGD:\n",
    "    \n",
    "    # setting learning rate default as 1.\n",
    "    def __init__(self, learning_rate = 1.0, decay=0., momentum =0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def pre_update_parameters(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "        \n",
    "    def update_params(self, layer):\n",
    "        \n",
    "        # if we use momentum\n",
    "        if self.momentum:\n",
    "            # if the layer does not contain momentum arrays\n",
    "            if not hasattr(layer, \"weight_momentums\"):\n",
    "                # creating weight_momentums and bias_momentums arrays initialized with zero\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            \n",
    "            # getting the weight updates with momentum - take previous factors\n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "            \n",
    "            # getting bias updates\n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "        \n",
    "        else:\n",
    "\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates =  -self.current_learning_rate * layer.dbiases\n",
    "            \n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "        \n",
    "    def post_update_parameters(self):\n",
    "        self.iterations +=1\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "        \n",
    "class Optimizer_AdaGrad:\n",
    "    \n",
    "    # setting learning rate default as 1.\n",
    "    def __init__(self, learning_rate = 1.0, decay=0., epsilon =1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def pre_update_parameters(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "        \n",
    "    def update_params(self, layer):\n",
    "        # checking if layer has attribute weight_cache\n",
    "        if not hasattr(layer, \"weight_cache\"):\n",
    "            # creating weight_cache and bias_cache\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        # getting the square of the gradients\n",
    "        layer.weight_cache += layer.dweights **2\n",
    "        layer.bias_cache += layer.dbiases **2\n",
    "        \n",
    "        # calculating the result and adding it to the weights and biases\n",
    "        layer.weights += - self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += - self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "        \n",
    "    def post_update_parameters(self):\n",
    "        self.iterations +=1\n",
    "                \n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class Optimizer_Adam:\n",
    "    \n",
    "    def __init__(self, learning_rate = 0.001, decay=0. ,\n",
    "                 epsilon=1e-7, beta_1=0.9, beta_2 = 0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        \n",
    "    def pre_update_parameters(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "        \n",
    "    def update_params(self, layer):\n",
    "        # checking if layer has attribute weight_cache\n",
    "        if not hasattr(layer, \"weight_cache\"):\n",
    "            \n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            \n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        \n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1- self.beta_1) * layer.dweights\n",
    "        \n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1- self.beta_1) * layer.dbiases\n",
    "        \n",
    "        \n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        \n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "\n",
    "        \n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights **2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases **2\n",
    "        \n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations +1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations +1))\n",
    "        \n",
    "        \n",
    "        layer.weights += - self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected)+ self.epsilon)\n",
    "        layer.biases += - self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected)+ self.epsilon)\n",
    "\n",
    "        \n",
    "    def post_update_parameters(self):\n",
    "        self.iterations +=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9742a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, acc: 0.36, loss: 1.098594307899475 lr: 1.0 \n",
      "Epoch: 100, acc: 0.49, loss: 1.0180515050888062 lr: 0.9099181073703367 \n",
      "Epoch: 200, acc: 0.5733333333333334, loss: 0.9450024366378784 lr: 0.8340283569641367 \n",
      "Epoch: 300, acc: 0.6166666666666667, loss: 0.8792305588722229 lr: 0.7698229407236336 \n",
      "Epoch: 400, acc: 0.6266666666666667, loss: 0.8319222927093506 lr: 0.7147962830593281 \n",
      "Epoch: 500, acc: 0.6266666666666667, loss: 0.8031525611877441 lr: 0.66711140760507 \n",
      "Epoch: 600, acc: 0.6366666666666667, loss: 0.7787525653839111 lr: 0.6253908692933083 \n",
      "Epoch: 700, acc: 0.63, loss: 0.762219250202179 lr: 0.5885815185403178 \n",
      "Epoch: 800, acc: 0.6233333333333333, loss: 0.7464234232902527 lr: 0.5558643690939411 \n",
      "Epoch: 900, acc: 0.6433333333333333, loss: 0.7227268218994141 lr: 0.526592943654555 \n",
      "Epoch: 1000, acc: 0.6633333333333333, loss: 0.7044738531112671 lr: 0.5002501250625312 \n",
      "Epoch: 1100, acc: 0.6633333333333333, loss: 0.6910290718078613 lr: 0.4764173415912339 \n",
      "Epoch: 1200, acc: 0.6733333333333333, loss: 0.6810699105262756 lr: 0.45475216007276037 \n",
      "Epoch: 1300, acc: 0.6833333333333333, loss: 0.6727485656738281 lr: 0.43497172683775553 \n",
      "Epoch: 1400, acc: 0.69, loss: 0.6653763055801392 lr: 0.4168403501458941 \n",
      "Epoch: 1500, acc: 0.6966666666666667, loss: 0.6591847538948059 lr: 0.4001600640256102 \n",
      "Epoch: 1600, acc: 0.6966666666666667, loss: 0.6536184549331665 lr: 0.3847633705271258 \n",
      "Epoch: 1700, acc: 0.6933333333333334, loss: 0.6479750275611877 lr: 0.3705075954057058 \n",
      "Epoch: 1800, acc: 0.6933333333333334, loss: 0.6422483921051025 lr: 0.35727045373347627 \n",
      "Epoch: 1900, acc: 0.7033333333333334, loss: 0.634223461151123 lr: 0.3449465332873405 \n",
      "Epoch: 2000, acc: 0.71, loss: 0.6256051659584045 lr: 0.33344448149383127 \n",
      "Epoch: 2100, acc: 0.71, loss: 0.6188440918922424 lr: 0.32268473701193934 \n",
      "Epoch: 2200, acc: 0.7233333333333334, loss: 0.611801028251648 lr: 0.31259768677711786 \n",
      "Epoch: 2300, acc: 0.7233333333333334, loss: 0.606736421585083 lr: 0.3031221582297666 \n",
      "Epoch: 2400, acc: 0.7266666666666667, loss: 0.602022111415863 lr: 0.29420417769932333 \n",
      "Epoch: 2500, acc: 0.7333333333333333, loss: 0.5978400111198425 lr: 0.2857959416976279 \n",
      "Epoch: 2600, acc: 0.7433333333333333, loss: 0.5940737724304199 lr: 0.2778549597110308 \n",
      "Epoch: 2700, acc: 0.7366666666666667, loss: 0.5903540849685669 lr: 0.2703433360367667 \n",
      "Epoch: 2800, acc: 0.7366666666666667, loss: 0.5866615772247314 lr: 0.26322716504343247 \n",
      "Epoch: 2900, acc: 0.73, loss: 0.5829418897628784 lr: 0.25647601949217746 \n",
      "Epoch: 3000, acc: 0.7433333333333333, loss: 0.5802040100097656 lr: 0.25006251562890724 \n",
      "Epoch: 3100, acc: 0.74, loss: 0.577214241027832 lr: 0.2439619419370578 \n",
      "Epoch: 3200, acc: 0.74, loss: 0.5745000243186951 lr: 0.23815194093831865 \n",
      "Epoch: 3300, acc: 0.7366666666666667, loss: 0.5718058347702026 lr: 0.23261223540358225 \n",
      "Epoch: 3400, acc: 0.74, loss: 0.5694363117218018 lr: 0.22732439190725165 \n",
      "Epoch: 3500, acc: 0.7466666666666667, loss: 0.5672092437744141 lr: 0.22227161591464767 \n",
      "Epoch: 3600, acc: 0.7466666666666667, loss: 0.565166175365448 lr: 0.21743857360295715 \n",
      "Epoch: 3700, acc: 0.7466666666666667, loss: 0.5632395148277283 lr: 0.21281123643328367 \n",
      "Epoch: 3800, acc: 0.74, loss: 0.5614005327224731 lr: 0.20837674515524068 \n",
      "Epoch: 3900, acc: 0.7366666666666667, loss: 0.5595778226852417 lr: 0.20412329046744235 \n",
      "Epoch: 4000, acc: 0.7333333333333333, loss: 0.557876467704773 lr: 0.2000400080016003 \n",
      "Epoch: 4100, acc: 0.7333333333333333, loss: 0.5563338994979858 lr: 0.19611688566385566 \n",
      "Epoch: 4200, acc: 0.7366666666666667, loss: 0.5548475384712219 lr: 0.19234468166955185 \n",
      "Epoch: 4300, acc: 0.7333333333333333, loss: 0.553454577922821 lr: 0.18871485185884126 \n",
      "Epoch: 4400, acc: 0.7333333333333333, loss: 0.5521315336227417 lr: 0.18521948508983144 \n",
      "Epoch: 4500, acc: 0.73, loss: 0.5508381724357605 lr: 0.18185124568103292 \n",
      "Epoch: 4600, acc: 0.7333333333333333, loss: 0.5496591329574585 lr: 0.1786033220217896 \n",
      "Epoch: 4700, acc: 0.73, loss: 0.5479179620742798 lr: 0.1754693805930865 \n",
      "Epoch: 4800, acc: 0.74, loss: 0.5461475849151611 lr: 0.17244352474564578 \n",
      "Epoch: 4900, acc: 0.7333333333333333, loss: 0.5449320077896118 lr: 0.16952025767079165 \n",
      "Epoch: 5000, acc: 0.7333333333333333, loss: 0.5438476800918579 lr: 0.16669444907484582 \n",
      "Epoch: 5100, acc: 0.73, loss: 0.542849600315094 lr: 0.16396130513198884 \n",
      "Epoch: 5200, acc: 0.73, loss: 0.5419153571128845 lr: 0.16131634134537828 \n",
      "Epoch: 5300, acc: 0.73, loss: 0.5409799218177795 lr: 0.15875535799333226 \n",
      "Epoch: 5400, acc: 0.7333333333333333, loss: 0.5400955080986023 lr: 0.1562744178777934 \n",
      "Epoch: 5500, acc: 0.7333333333333333, loss: 0.5392932891845703 lr: 0.15386982612709646 \n",
      "Epoch: 5600, acc: 0.7333333333333333, loss: 0.5384418964385986 lr: 0.15153811183512653 \n",
      "Epoch: 5700, acc: 0.7366666666666667, loss: 0.5376359224319458 lr: 0.14927601134497687 \n",
      "Epoch: 5800, acc: 0.74, loss: 0.5369068384170532 lr: 0.14708045300779526 \n",
      "Epoch: 5900, acc: 0.7433333333333333, loss: 0.5361872315406799 lr: 0.14494854326714016 \n",
      "Epoch: 6000, acc: 0.75, loss: 0.535481333732605 lr: 0.1428775539362766 \n",
      "Epoch: 6100, acc: 0.7533333333333333, loss: 0.5346987843513489 lr: 0.1408649105507818 \n",
      "Epoch: 6200, acc: 0.7566666666666667, loss: 0.5339822173118591 lr: 0.13890818169190167 \n",
      "Epoch: 6300, acc: 0.7566666666666667, loss: 0.5333234667778015 lr: 0.13700506918755992 \n",
      "Epoch: 6400, acc: 0.7566666666666667, loss: 0.5319584608078003 lr: 0.13515339910798757 \n",
      "Epoch: 6500, acc: 0.75, loss: 0.530062735080719 lr: 0.13335111348179757 \n",
      "Epoch: 6600, acc: 0.76, loss: 0.5287725925445557 lr: 0.13159626266614027 \n",
      "Epoch: 6700, acc: 0.76, loss: 0.5278003811836243 lr: 0.12988699831146902 \n",
      "Epoch: 6800, acc: 0.7633333333333333, loss: 0.5270179510116577 lr: 0.12822156686754713 \n",
      "Epoch: 6900, acc: 0.7566666666666667, loss: 0.5262861847877502 lr: 0.126598303582732 \n",
      "Epoch: 7000, acc: 0.7533333333333333, loss: 0.5256379246711731 lr: 0.12501562695336915 \n",
      "Epoch: 7100, acc: 0.7533333333333333, loss: 0.5249834060668945 lr: 0.12347203358439313 \n",
      "Epoch: 7200, acc: 0.7566666666666667, loss: 0.5243861675262451 lr: 0.12196609342602757 \n",
      "Epoch: 7300, acc: 0.76, loss: 0.5238062739372253 lr: 0.12049644535486204 \n",
      "Epoch: 7400, acc: 0.7633333333333333, loss: 0.5232557654380798 lr: 0.11906179307060363 \n",
      "Epoch: 7500, acc: 0.7633333333333333, loss: 0.5227208733558655 lr: 0.11766090128250381 \n",
      "Epoch: 7600, acc: 0.7666666666666667, loss: 0.5222142338752747 lr: 0.11629259216187929 \n",
      "Epoch: 7700, acc: 0.7666666666666667, loss: 0.5217168927192688 lr: 0.11495574203931487 \n",
      "Epoch: 7800, acc: 0.77, loss: 0.5212453007698059 lr: 0.11364927832708263 \n",
      "Epoch: 7900, acc: 0.77, loss: 0.520774781703949 lr: 0.11237217664906168 \n",
      "Epoch: 8000, acc: 0.77, loss: 0.5203229784965515 lr: 0.11112345816201799 \n",
      "Epoch: 8100, acc: 0.77, loss: 0.5198987722396851 lr: 0.10990218705352237 \n",
      "Epoch: 8200, acc: 0.77, loss: 0.5194816589355469 lr: 0.10870746820306555 \n",
      "Epoch: 8300, acc: 0.77, loss: 0.5190827250480652 lr: 0.1075384449940854 \n",
      "Epoch: 8400, acc: 0.77, loss: 0.518680214881897 lr: 0.10639429726566654 \n",
      "Epoch: 8500, acc: 0.77, loss: 0.5182971954345703 lr: 0.10527423939362038 \n",
      "Epoch: 8600, acc: 0.7666666666666667, loss: 0.5179107785224915 lr: 0.10417751849150952 \n",
      "Epoch: 8700, acc: 0.7666666666666667, loss: 0.5175501704216003 lr: 0.10310341272296113 \n",
      "Epoch: 8800, acc: 0.7666666666666667, loss: 0.5171899795532227 lr: 0.1020512297173181 \n",
      "Epoch: 8900, acc: 0.7666666666666667, loss: 0.5168411731719971 lr: 0.10102030508132134 \n",
      "Epoch: 9000, acc: 0.7666666666666667, loss: 0.5164961218833923 lr: 0.1000100010001 \n",
      "Epoch: 9100, acc: 0.7666666666666667, loss: 0.5161696076393127 lr: 0.09901970492127933 \n",
      "Epoch: 9200, acc: 0.7666666666666667, loss: 0.5157718062400818 lr: 0.09804882831650162 \n",
      "Epoch: 9300, acc: 0.77, loss: 0.5148410201072693 lr: 0.09709680551509856 \n",
      "Epoch: 9400, acc: 0.77, loss: 0.5140573978424072 lr: 0.09616309260505818 \n",
      "Epoch: 9500, acc: 0.77, loss: 0.5133417844772339 lr: 0.09524716639679968 \n",
      "Epoch: 9600, acc: 0.77, loss: 0.5126901268959045 lr: 0.09434852344560807 \n",
      "Epoch: 9700, acc: 0.77, loss: 0.5121338367462158 lr: 0.09346667912889055 \n",
      "Epoch: 9800, acc: 0.7666666666666667, loss: 0.511609673500061 lr: 0.09260116677470137 \n",
      "Epoch: 9900, acc: 0.7666666666666667, loss: 0.5109617710113525 lr: 0.09175153683824203 \n",
      "Epoch: 10000, acc: 0.7666666666666667, loss: 0.5103560090065002 lr: 0.09091735612328393 \n"
     ]
    }
   ],
   "source": [
    "# creating the dataset that has three classes\n",
    "X, y = spiral_data(samples = 100, classes=3)\n",
    "\n",
    "# creating the first layer with 2 inputs and 64 output values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "\n",
    "# initializing the activation function\n",
    "activation1 = Activation_Relu()\n",
    "\n",
    "# creating the second layer with 64 inputs and 3 output values\n",
    "# as we have 64 outputs from the previous layer dense1\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "# initializing the loss activation function\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossEntropy()\n",
    "\n",
    "# initializing the optimizer SGD\n",
    "optimizer = Optimizer_AdaGrad(decay = 1e-3)\n",
    "\n",
    "for epoch in range(10001):\n",
    "    \n",
    "    dense1.forward(X)\n",
    "    \n",
    "    activation1.forward(dense1.output)\n",
    "    \n",
    "    dense2.forward(activation1.output)\n",
    "    \n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "    \n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "\n",
    "    accuracy = np.mean(predictions == y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'Epoch: {epoch}, ' + f'acc: {accuracy}, '+ f'loss: {loss}', f'lr: {optimizer.current_learning_rate} ')\n",
    "    \n",
    "    \n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "\n",
    "    # backward passing from dense2 layer with the gradient\n",
    "    # from loss activation function\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "\n",
    "    # backward passing the activation 1 function with the gradient from dense layer 2\n",
    "    activation1.backward(dense2.dinputs)\n",
    "\n",
    "    # backward passing the gradient from activation1 with the dense layer 1\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # updating the weights and biases of dense layer 1 and 2\n",
    "    optimizer.pre_update_parameters()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266e9331",
   "metadata": {},
   "source": [
    "# Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54cb1c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np \n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "class Layer_Dense:\n",
    "    \n",
    "    # getting the number of inputs and neurons\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        \n",
    "        #initializing random weights in range \n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "    # getting the output with (mx +b)\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    \n",
    "    # backward pass which takes the derivative values from the next function \n",
    "    def backward(self, dvalues):\n",
    "        # getting the partial derivative with the inputs\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims = True)\n",
    "        \n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "        \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Activation_Relu:\n",
    "    \n",
    "    # The method which takes the input\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        self.inputs = inputs\n",
    "        # returns output with the maximum of 0 and the input value\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <=0] = 0\n",
    "        \n",
    "#---------------------------------------------------------------------\n",
    "        \n",
    "# Creating an Activation function class\n",
    "class Activation_Softmax:\n",
    "    \n",
    "    # The method which takes the input\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        \n",
    "        norm_vals = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        \n",
    "        self.output = norm_vals\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        \n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "                \n",
    "                single_output = single_output.reshape(-1,1)\n",
    "                \n",
    "                jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "                \n",
    "                self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "                \n",
    "#---------------------------------------------------------------------\n",
    "                \n",
    "class Loss:\n",
    "    \n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        \n",
    "        return data_loss\n",
    "\n",
    "    \n",
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "    \n",
    "    def forward(self, y_pred, y_target):\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        # clipping the values so we don't end up with inf error\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "\n",
    "        # checking if 1D array so we just need to get that index value\n",
    "        if len(y_target.shape) == 1:\n",
    "            correct = y_pred_clipped[range(samples), y_target]\n",
    "        # checking if one-hot-encoded values, then multiplying and summing up\n",
    "        elif len(y_target.shape) == 2:\n",
    "            correct = np.sum(y_pred_clipped * y_target, axis=1)\n",
    "        \n",
    "        negative_log = -np.log(correct)\n",
    "        \n",
    "        return negative_log\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "        \n",
    "        if len(y_true.shape) ==1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "            \n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples\n",
    "    \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Accuracy:\n",
    "    def get_accuracy(self,y_pred, y_true):\n",
    "        \n",
    "        predictions = np.argmax(y_pred, axis=1)\n",
    "                \n",
    "        if len(y_true.shape) == 2:\n",
    "            actual = np.argmax(y_true, axis=1)\n",
    "        else:\n",
    "            actual = np.argmax(y_true)\n",
    "        \n",
    "        accuracy = np.mean(predictions == y)\n",
    "        \n",
    "        print(\"accuracy : \", accuracy)\n",
    "        \n",
    "\n",
    "#.......................................................................\n",
    "\n",
    "class Activation_Softmax_Loss_CategoricalCrossEntropy():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # initializing the activation function\n",
    "        self.activation = Activation_Softmax()\n",
    "        \n",
    "        # initializing the loss function\n",
    "        self.loss = Loss_CategoricalCrossEntropy()\n",
    "    \n",
    "    # forward pass method, takes the input from \n",
    "    # the dense layer and the actual value\n",
    "    def forward(self, inputs, y_true):\n",
    "        \n",
    "        # forward passing the inputs in the activation function\n",
    "        self.activation.forward(inputs)\n",
    "        \n",
    "        # getting the output from the activation function\n",
    "        self.output = self.activation.output\n",
    "        \n",
    "        # calculating loss with the activation function output and actual value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        # checking if the values are one-hot-encoded\n",
    "        if len(y_true.shape) ==2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "            \n",
    "        \n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "        # calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -=1\n",
    "        \n",
    "        # normalizing gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "    \n",
    "#---------------------------------------------------------------------\n",
    "    \n",
    "class Optimizer_SGD:\n",
    "    \n",
    "    # setting learning rate default as 1.\n",
    "    def __init__(self, learning_rate = 1.0, decay=0., momentum =0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def pre_update_parameters(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "        \n",
    "    def update_params(self, layer):\n",
    "        \n",
    "        # if we use momentum\n",
    "        if self.momentum:\n",
    "            # if the layer does not contain momentum arrays\n",
    "            if not hasattr(layer, \"weight_momentums\"):\n",
    "                # creating weight_momentums and bias_momentums arrays initialized with zero\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            \n",
    "            # getting the weight updates with momentum - take previous factors\n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "            \n",
    "            # getting bias updates\n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "        \n",
    "        else:\n",
    "\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates =  -self.current_learning_rate * layer.dbiases\n",
    "            \n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "        \n",
    "    def post_update_parameters(self):\n",
    "        self.iterations +=1\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "        \n",
    "class Optimizer_AdaGrad:\n",
    "    \n",
    "    # setting learning rate default as 1.\n",
    "    def __init__(self, learning_rate = 1.0, decay=0., epsilon =1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def pre_update_parameters(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "        \n",
    "    def update_params(self, layer):\n",
    "        # checking if layer has attribute weight_cache\n",
    "        if not hasattr(layer, \"weight_cache\"):\n",
    "            # creating weight_cache and bias_cache\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        # getting the square of the gradients\n",
    "        layer.weight_cache += layer.dweights **2\n",
    "        layer.bias_cache += layer.dbiases **2\n",
    "        \n",
    "        # calculating the result and adding it to the weights and biases\n",
    "        layer.weights += - self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += - self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "        \n",
    "    def post_update_parameters(self):\n",
    "        self.iterations +=1\n",
    "                \n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class Optimizer_Adam:\n",
    "    \n",
    "    def __init__(self, learning_rate = 0.001, decay=0. ,\n",
    "                 epsilon=1e-7, beta_1=0.9, beta_2 = 0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        \n",
    "    def pre_update_parameters(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "        \n",
    "    def update_params(self, layer):\n",
    "        # checking if layer has attribute weight_cache\n",
    "        if not hasattr(layer, \"weight_cache\"):\n",
    "            \n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            \n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        \n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1- self.beta_1) * layer.dweights\n",
    "        \n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1- self.beta_1) * layer.dbiases\n",
    "        \n",
    "        \n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        \n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "\n",
    "        \n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights **2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases **2\n",
    "        \n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations +1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations +1))\n",
    "        \n",
    "        \n",
    "        layer.weights += - self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected)+ self.epsilon)\n",
    "        layer.biases += - self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected)+ self.epsilon)\n",
    "\n",
    "        \n",
    "    def post_update_parameters(self):\n",
    "        self.iterations +=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d22d7b78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, acc: 0.36, loss: 1.098594307899475 lr: 0.02 \n",
      "Epoch: 100, acc: 0.6566666666666666, loss: 0.7913389801979065 lr: 0.018198362147406735 \n",
      "Epoch: 200, acc: 0.78, loss: 0.5927379131317139 lr: 0.016680567139282735 \n",
      "Epoch: 300, acc: 0.8266666666666667, loss: 0.495195209980011 lr: 0.015396458814472672 \n",
      "Epoch: 400, acc: 0.85, loss: 0.4285662770271301 lr: 0.014295925661186561 \n",
      "Epoch: 500, acc: 0.8733333333333333, loss: 0.3799552023410797 lr: 0.0133422281521014 \n",
      "Epoch: 600, acc: 0.88, loss: 0.34610792994499207 lr: 0.012507817385866166 \n",
      "Epoch: 700, acc: 0.8866666666666667, loss: 0.32180795073509216 lr: 0.011771630370806356 \n",
      "Epoch: 800, acc: 0.89, loss: 0.3031814992427826 lr: 0.011117287381878822 \n",
      "Epoch: 900, acc: 0.9033333333333333, loss: 0.28818339109420776 lr: 0.010531858873091101 \n",
      "Epoch: 1000, acc: 0.91, loss: 0.26673629879951477 lr: 0.010005002501250623 \n",
      "Epoch: 1100, acc: 0.9066666666666666, loss: 0.2537030577659607 lr: 0.009528346831824679 \n",
      "Epoch: 1200, acc: 0.9133333333333333, loss: 0.2431858330965042 lr: 0.009095043201455207 \n",
      "Epoch: 1300, acc: 0.9166666666666666, loss: 0.23523510992527008 lr: 0.008699434536755112 \n",
      "Epoch: 1400, acc: 0.92, loss: 0.22639745473861694 lr: 0.008336807002917883 \n",
      "Epoch: 1500, acc: 0.92, loss: 0.21796442568302155 lr: 0.008003201280512205 \n",
      "Epoch: 1600, acc: 0.92, loss: 0.21162644028663635 lr: 0.007695267410542516 \n",
      "Epoch: 1700, acc: 0.9266666666666666, loss: 0.2039458304643631 lr: 0.007410151908114116 \n",
      "Epoch: 1800, acc: 0.9266666666666666, loss: 0.1985035240650177 lr: 0.007145409074669526 \n",
      "Epoch: 1900, acc: 0.93, loss: 0.19367490708827972 lr: 0.006898930665746809 \n",
      "Epoch: 2000, acc: 0.93, loss: 0.18942242860794067 lr: 0.006668889629876626 \n",
      "Epoch: 2100, acc: 0.93, loss: 0.18492025136947632 lr: 0.006453694740238787 \n",
      "Epoch: 2200, acc: 0.93, loss: 0.18103039264678955 lr: 0.006251953735542357 \n",
      "Epoch: 2300, acc: 0.9333333333333333, loss: 0.17746831476688385 lr: 0.006062443164595333 \n",
      "Epoch: 2400, acc: 0.9366666666666666, loss: 0.1731051355600357 lr: 0.005884083553986467 \n",
      "Epoch: 2500, acc: 0.94, loss: 0.16896222531795502 lr: 0.0057159188339525584 \n",
      "Epoch: 2600, acc: 0.94, loss: 0.16564731299877167 lr: 0.005557099194220616 \n",
      "Epoch: 2700, acc: 0.9433333333333334, loss: 0.16259892284870148 lr: 0.005406866720735334 \n",
      "Epoch: 2800, acc: 0.9433333333333334, loss: 0.15983189642429352 lr: 0.0052645433008686494 \n",
      "Epoch: 2900, acc: 0.9433333333333334, loss: 0.15738238394260406 lr: 0.005129520389843549 \n",
      "Epoch: 3000, acc: 0.9433333333333334, loss: 0.1548529416322708 lr: 0.005001250312578145 \n",
      "Epoch: 3100, acc: 0.9433333333333334, loss: 0.1526496410369873 lr: 0.0048792388387411565 \n",
      "Epoch: 3200, acc: 0.9466666666666667, loss: 0.15054470300674438 lr: 0.004763038818766373 \n",
      "Epoch: 3300, acc: 0.9433333333333334, loss: 0.14853638410568237 lr: 0.004652244708071645 \n",
      "Epoch: 3400, acc: 0.9466666666666667, loss: 0.14666470885276794 lr: 0.0045464878381450335 \n",
      "Epoch: 3500, acc: 0.9433333333333334, loss: 0.14488562941551208 lr: 0.004445432318292954 \n",
      "Epoch: 3600, acc: 0.9433333333333334, loss: 0.14316925406455994 lr: 0.004348771472059143 \n",
      "Epoch: 3700, acc: 0.9466666666666667, loss: 0.1418001502752304 lr: 0.004256224728665673 \n",
      "Epoch: 3800, acc: 0.9466666666666667, loss: 0.1401185542345047 lr: 0.004167534903104814 \n",
      "Epoch: 3900, acc: 0.9466666666666667, loss: 0.13867565989494324 lr: 0.004082465809348847 \n",
      "Epoch: 4000, acc: 0.9433333333333334, loss: 0.13731731474399567 lr: 0.0040008001600320055 \n",
      "Epoch: 4100, acc: 0.9466666666666667, loss: 0.13607020676136017 lr: 0.003922337713277113 \n",
      "Epoch: 4200, acc: 0.9433333333333334, loss: 0.1348152905702591 lr: 0.003846893633391037 \n",
      "Epoch: 4300, acc: 0.9433333333333334, loss: 0.13365033268928528 lr: 0.0037742970371768252 \n",
      "Epoch: 4400, acc: 0.9433333333333334, loss: 0.1326502561569214 lr: 0.003704389701796629 \n",
      "Epoch: 4500, acc: 0.9433333333333334, loss: 0.13144467771053314 lr: 0.0036370249136206583 \n",
      "Epoch: 4600, acc: 0.9466666666666667, loss: 0.12948375940322876 lr: 0.003572066440435792 \n",
      "Epoch: 4700, acc: 0.9466666666666667, loss: 0.12754379212856293 lr: 0.00350938761186173 \n",
      "Epoch: 4800, acc: 0.9466666666666667, loss: 0.1263108253479004 lr: 0.003448870494912916 \n",
      "Epoch: 4900, acc: 0.95, loss: 0.1251743882894516 lr: 0.003390405153415833 \n",
      "Epoch: 5000, acc: 0.95, loss: 0.12412562221288681 lr: 0.0033338889814969164 \n",
      "Epoch: 5100, acc: 0.9466666666666667, loss: 0.1229780837893486 lr: 0.003279226102639777 \n",
      "Epoch: 5200, acc: 0.95, loss: 0.12169521301984787 lr: 0.0032263268269075657 \n",
      "Epoch: 5300, acc: 0.9533333333333334, loss: 0.12053756415843964 lr: 0.0031751071598666455 \n",
      "Epoch: 5400, acc: 0.9533333333333334, loss: 0.11957938969135284 lr: 0.0031254883575558678 \n",
      "Epoch: 5500, acc: 0.9533333333333334, loss: 0.11871708929538727 lr: 0.0030773965225419295 \n",
      "Epoch: 5600, acc: 0.9566666666666667, loss: 0.11792021244764328 lr: 0.0030307622367025306 \n",
      "Epoch: 5700, acc: 0.9566666666666667, loss: 0.1171271800994873 lr: 0.0029855202268995375 \n",
      "Epoch: 5800, acc: 0.96, loss: 0.11600765585899353 lr: 0.0029416090601559054 \n",
      "Epoch: 5900, acc: 0.9566666666666667, loss: 0.11406813561916351 lr: 0.0028989708653428033 \n",
      "Epoch: 6000, acc: 0.96, loss: 0.11239273101091385 lr: 0.002857551078725532 \n",
      "Epoch: 6100, acc: 0.9566666666666667, loss: 0.11095266789197922 lr: 0.0028172982110156357 \n",
      "Epoch: 6200, acc: 0.9566666666666667, loss: 0.10974735021591187 lr: 0.0027781636338380334 \n",
      "Epoch: 6300, acc: 0.9566666666666667, loss: 0.10866478085517883 lr: 0.0027401013837511983 \n",
      "Epoch: 6400, acc: 0.9533333333333334, loss: 0.10736949741840363 lr: 0.0027030679821597515 \n",
      "Epoch: 6500, acc: 0.9566666666666667, loss: 0.10638031363487244 lr: 0.0026670222696359514 \n",
      "Epoch: 6600, acc: 0.9633333333333334, loss: 0.10538537055253983 lr: 0.0026319252533228057 \n",
      "Epoch: 6700, acc: 0.9633333333333334, loss: 0.10456180572509766 lr: 0.0025977399662293803 \n",
      "Epoch: 6800, acc: 0.9633333333333334, loss: 0.10378273576498032 lr: 0.0025644313373509426 \n",
      "Epoch: 6900, acc: 0.9666666666666667, loss: 0.10309312492609024 lr: 0.00253196607165464 \n",
      "Epoch: 7000, acc: 0.9666666666666667, loss: 0.10236700624227524 lr: 0.002500312539067383 \n",
      "Epoch: 7100, acc: 0.9633333333333334, loss: 0.10170605033636093 lr: 0.0024694406716878627 \n",
      "Epoch: 7200, acc: 0.9666666666666667, loss: 0.10106625407934189 lr: 0.0024393218685205514 \n",
      "Epoch: 7300, acc: 0.9666666666666667, loss: 0.10044004023075104 lr: 0.0024099289070972406 \n",
      "Epoch: 7400, acc: 0.9666666666666667, loss: 0.09976833313703537 lr: 0.0023812358614120725 \n",
      "Epoch: 7500, acc: 0.9666666666666667, loss: 0.09909402579069138 lr: 0.002353218025650076 \n",
      "Epoch: 7600, acc: 0.9666666666666667, loss: 0.0985308438539505 lr: 0.002325851843237586 \n",
      "Epoch: 7700, acc: 0.9666666666666667, loss: 0.09796526283025742 lr: 0.0022991148407862975 \n",
      "Epoch: 7800, acc: 0.9666666666666667, loss: 0.09741141647100449 lr: 0.0022729855665416525 \n",
      "Epoch: 7900, acc: 0.9666666666666667, loss: 0.09688489884138107 lr: 0.002247443532981234 \n",
      "Epoch: 8000, acc: 0.9666666666666667, loss: 0.09636576473712921 lr: 0.00222246916324036 \n",
      "Epoch: 8100, acc: 0.9666666666666667, loss: 0.09587454795837402 lr: 0.0021980437410704474 \n",
      "Epoch: 8200, acc: 0.9666666666666667, loss: 0.09535914659500122 lr: 0.0021741493640613113 \n",
      "Epoch: 8300, acc: 0.9666666666666667, loss: 0.09486854821443558 lr: 0.0021507688998817077 \n",
      "Epoch: 8400, acc: 0.9666666666666667, loss: 0.09395010024309158 lr: 0.002127885945313331 \n",
      "Epoch: 8500, acc: 0.97, loss: 0.09130039811134338 lr: 0.0021054847878724074 \n",
      "Epoch: 8600, acc: 0.9733333333333334, loss: 0.09001565724611282 lr: 0.0020835503698301903 \n",
      "Epoch: 8700, acc: 0.9733333333333334, loss: 0.08908108621835709 lr: 0.0020620682544592226 \n",
      "Epoch: 8800, acc: 0.9733333333333334, loss: 0.08823003619909286 lr: 0.002041024594346362 \n",
      "Epoch: 8900, acc: 0.9733333333333334, loss: 0.08752919733524323 lr: 0.002020406101626427 \n",
      "Epoch: 9000, acc: 0.9733333333333334, loss: 0.0869024470448494 lr: 0.002000200020002 \n",
      "Epoch: 9100, acc: 0.9733333333333334, loss: 0.0863432064652443 lr: 0.0019803940984255866 \n",
      "Epoch: 9200, acc: 0.9733333333333334, loss: 0.08553694188594818 lr: 0.0019609765663300325 \n",
      "Epoch: 9300, acc: 0.9733333333333334, loss: 0.08496864140033722 lr: 0.0019419361103019711 \n",
      "Epoch: 9400, acc: 0.9733333333333334, loss: 0.08446820825338364 lr: 0.0019232618521011636 \n",
      "Epoch: 9500, acc: 0.9733333333333334, loss: 0.08397019654512405 lr: 0.0019049433279359938 \n",
      "Epoch: 9600, acc: 0.9733333333333334, loss: 0.083511121571064 lr: 0.0018869704689121615 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9700, acc: 0.9733333333333334, loss: 0.08293992280960083 lr: 0.001869333582577811 \n",
      "Epoch: 9800, acc: 0.9733333333333334, loss: 0.08239053934812546 lr: 0.0018520233354940275 \n",
      "Epoch: 9900, acc: 0.9733333333333334, loss: 0.08192701637744904 lr: 0.0018350307367648406 \n",
      "Epoch: 10000, acc: 0.9733333333333334, loss: 0.08146724849939346 lr: 0.0018183471224656785 \n"
     ]
    }
   ],
   "source": [
    "# creating the dataset that has three classes\n",
    "X, y = spiral_data(samples = 100, classes=3)\n",
    "\n",
    "# creating the first layer with 2 inputs and 64 output values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "\n",
    "# initializing the activation function\n",
    "activation1 = Activation_Relu()\n",
    "\n",
    "# creating the second layer with 64 inputs and 3 output values\n",
    "# as we have 64 outputs from the previous layer dense1\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "# initializing the loss activation function\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossEntropy()\n",
    "\n",
    "# initializing the optimizer SGD\n",
    "optimizer = Optimizer_Adam(learning_rate=0.02, decay = 1e-3)\n",
    "\n",
    "for epoch in range(10001):\n",
    "    \n",
    "    dense1.forward(X)\n",
    "    \n",
    "    activation1.forward(dense1.output)\n",
    "    \n",
    "    dense2.forward(activation1.output)\n",
    "    \n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "    \n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "\n",
    "    accuracy = np.mean(predictions == y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'Epoch: {epoch}, ' + f'acc: {accuracy}, '+ f'loss: {loss}', f'lr: {optimizer.current_learning_rate} ')\n",
    "    \n",
    "    \n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "\n",
    "    # backward passing from dense2 layer with the gradient\n",
    "    # from loss activation function\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "\n",
    "    # backward passing the activation 1 function with the gradient from dense layer 2\n",
    "    activation1.backward(dense2.dinputs)\n",
    "\n",
    "    # backward passing the gradient from activation1 with the dense layer 1\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # updating the weights and biases of dense layer 1 and 2\n",
    "    optimizer.pre_update_parameters()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5504f19",
   "metadata": {},
   "source": [
    "# Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "002905d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation, acc: 0.8, loss: 0.9266406297683716\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1.forward(X_test)\n",
    "\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "loss = loss_activation.forward(dense2.output, y_test)\n",
    "\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "\n",
    "if len(y_test.shape)==2:\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "    \n",
    "accuracy = np.mean(predictions == y_test)\n",
    "\n",
    "print(f\"validation, acc: {accuracy}, loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea85a37c",
   "metadata": {},
   "source": [
    "# L1 and L2 Regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fae0bdf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lambda_l1w' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m l1w \u001b[38;5;241m=\u001b[39m \u001b[43mlambda_l1w\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mabs\u001b[39m(weights))\n\u001b[0;32m      2\u001b[0m l1b \u001b[38;5;241m=\u001b[39m lambda_l1b \u001b[38;5;241m*\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mabs\u001b[39m(biases))\n\u001b[0;32m      4\u001b[0m l2w \u001b[38;5;241m=\u001b[39m lambda_l2w \u001b[38;5;241m*\u001b[39m \u001b[38;5;28msum\u001b[39m(weights\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lambda_l1w' is not defined"
     ]
    }
   ],
   "source": [
    "l1w = lambda_l1w * sum(abs(weights))\n",
    "l1b = lambda_l1b * sum(abs(biases))\n",
    "\n",
    "l2w = lambda_l2w * sum(weights**2)\n",
    "l2b = lambda_l2b * sum(biases**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9da5616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np \n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "class Layer_Dense:\n",
    "    \n",
    "    # getting the number of inputs and neurons\n",
    "    def __init__(self, n_inputs, n_neurons,\n",
    "                 weight_regularizer_l1=0,\n",
    "                bias_regularizer_l1=0,\n",
    "                weight_regularizer_l2=0,\n",
    "                bias_regularizer_l2=0):\n",
    "        \n",
    "        #initializing random weights in range \n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "        self.weight_regularizer_l1= weight_regularizer_l1\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        \n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "        \n",
    "    # getting the output with (mx +b)\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    \n",
    "    # backward pass which takes the derivative values from the next function \n",
    "    def backward(self, dvalues):\n",
    "        # getting the partial derivative with the inputs\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims = True)\n",
    "        \n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dl1 = np.ones_like(self.weights)\n",
    "            dl1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dl1\n",
    "            \n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "            \n",
    "        if self.bias_regularizer_l1 >0:\n",
    "            dl1 = np.ones_like(self.biases)\n",
    "            dl1[self.biases < 0] = -1\n",
    "            self.biases += self.bias_regularizer_l1 * dl1\n",
    "            \n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
    "        \n",
    "        \n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "        \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Activation_Relu:\n",
    "    \n",
    "    # The method which takes the input\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        self.inputs = inputs\n",
    "        # returns output with the maximum of 0 and the input value\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <=0] = 0\n",
    "        \n",
    "#---------------------------------------------------------------------\n",
    "        \n",
    "# Creating an Activation function class\n",
    "class Activation_Softmax:\n",
    "    \n",
    "    # The method which takes the input\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        \n",
    "        norm_vals = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        \n",
    "        self.output = norm_vals\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        \n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "                \n",
    "                single_output = single_output.reshape(-1,1)\n",
    "                \n",
    "                jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "                \n",
    "                self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "                \n",
    "#---------------------------------------------------------------------\n",
    "                \n",
    "class Loss:\n",
    "    \n",
    "    def regularization_loss(self, layer):\n",
    "        \n",
    "        regularization_loss = 0\n",
    "        \n",
    "        #------------------L1 Regularization------------------------\n",
    "\n",
    "        \n",
    "        if layer.weight_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l1 * np.sum(abs(layer.weights))\n",
    "            \n",
    "        if layer.bias_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l1 * np.sum(abs(layer.biases))\n",
    "        \n",
    "        #------------------L2 Regularization------------------------\n",
    "        \n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
    "            \n",
    "        if layer.bias_regularizer_l2 > 0 :\n",
    "            regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
    "            \n",
    "        return regularization_loss\n",
    "    \n",
    "        \n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        \n",
    "        return data_loss\n",
    "    \n",
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "    \n",
    "    def forward(self, y_pred, y_target):\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        # clipping the values so we don't end up with inf error\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "\n",
    "        # checking if 1D array so we just need to get that index value\n",
    "        if len(y_target.shape) == 1:\n",
    "            correct = y_pred_clipped[range(samples), y_target]\n",
    "        # checking if one-hot-encoded values, then multiplying and summing up\n",
    "        elif len(y_target.shape) == 2:\n",
    "            correct = np.sum(y_pred_clipped * y_target, axis=1)\n",
    "        \n",
    "        negative_log = -np.log(correct)\n",
    "        \n",
    "        return negative_log\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "        \n",
    "        if len(y_true.shape) ==1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "            \n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples\n",
    "    \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Accuracy:\n",
    "    def get_accuracy(self,y_pred, y_true):\n",
    "        \n",
    "        predictions = np.argmax(y_pred, axis=1)\n",
    "                \n",
    "        if len(y_true.shape) == 2:\n",
    "            actual = np.argmax(y_true, axis=1)\n",
    "        else:\n",
    "            actual = np.argmax(y_true)\n",
    "        \n",
    "        accuracy = np.mean(predictions == y)\n",
    "        \n",
    "        print(\"accuracy : \", accuracy)\n",
    "        \n",
    "\n",
    "#.......................................................................\n",
    "\n",
    "class Activation_Softmax_Loss_CategoricalCrossEntropy():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # initializing the activation function\n",
    "        self.activation = Activation_Softmax()\n",
    "        \n",
    "        # initializing the loss function\n",
    "        self.loss = Loss_CategoricalCrossEntropy()\n",
    "    \n",
    "    # forward pass method, takes the input from \n",
    "    # the dense layer and the actual value\n",
    "    def forward(self, inputs, y_true):\n",
    "        \n",
    "        # forward passing the inputs in the activation function\n",
    "        self.activation.forward(inputs)\n",
    "        \n",
    "        # getting the output from the activation function\n",
    "        self.output = self.activation.output\n",
    "        \n",
    "        # calculating loss with the activation function output and actual value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        # checking if the values are one-hot-encoded\n",
    "        if len(y_true.shape) ==2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "            \n",
    "        \n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "        # calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -=1\n",
    "        \n",
    "        # normalizing gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "    \n",
    "#---------------------------------------------------------------------\n",
    "    \n",
    "class Optimizer_SGD:\n",
    "    \n",
    "    # setting learning rate default as 1.\n",
    "    def __init__(self, learning_rate = 1.0, decay=0., momentum =0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def pre_update_parameters(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "        \n",
    "    def update_params(self, layer):\n",
    "        \n",
    "        # if we use momentum\n",
    "        if self.momentum:\n",
    "            # if the layer does not contain momentum arrays\n",
    "            if not hasattr(layer, \"weight_momentums\"):\n",
    "                # creating weight_momentums and bias_momentums arrays initialized with zero\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            \n",
    "            # getting the weight updates with momentum - take previous factors\n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "            \n",
    "            # getting bias updates\n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "        \n",
    "        else:\n",
    "\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates =  -self.current_learning_rate * layer.dbiases\n",
    "            \n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "        \n",
    "    def post_update_parameters(self):\n",
    "        self.iterations +=1\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "        \n",
    "class Optimizer_AdaGrad:\n",
    "    \n",
    "    # setting learning rate default as 1.\n",
    "    def __init__(self, learning_rate = 1.0, decay=0., epsilon =1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def pre_update_parameters(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "        \n",
    "    def update_params(self, layer):\n",
    "        # checking if layer has attribute weight_cache\n",
    "        if not hasattr(layer, \"weight_cache\"):\n",
    "            # creating weight_cache and bias_cache\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        # getting the square of the gradients\n",
    "        layer.weight_cache += layer.dweights **2\n",
    "        layer.bias_cache += layer.dbiases **2\n",
    "        \n",
    "        # calculating the result and adding it to the weights and biases\n",
    "        layer.weights += - self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += - self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "        \n",
    "    def post_update_parameters(self):\n",
    "        self.iterations +=1\n",
    "                \n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class Optimizer_Adam:\n",
    "    \n",
    "    def __init__(self, learning_rate = 0.001, decay=0. ,\n",
    "                 epsilon=1e-7, beta_1=0.9, beta_2 = 0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        \n",
    "    def pre_update_parameters(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "        \n",
    "    def update_params(self, layer):\n",
    "        # checking if layer has attribute weight_cache\n",
    "        if not hasattr(layer, \"weight_cache\"):\n",
    "            \n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            \n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        \n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1- self.beta_1) * layer.dweights\n",
    "        \n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1- self.beta_1) * layer.dbiases\n",
    "        \n",
    "        \n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        \n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "\n",
    "        \n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights **2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases **2\n",
    "        \n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations +1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations +1))\n",
    "        \n",
    "        \n",
    "        layer.weights += - self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected)+ self.epsilon)\n",
    "        layer.biases += - self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected)+ self.epsilon)\n",
    "\n",
    "        \n",
    "    def post_update_parameters(self):\n",
    "        self.iterations +=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d0142c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating the dataset that has three classes\n",
    "X, y = spiral_data(samples = 100, classes=3)\n",
    "\n",
    "# creating the first layer with 2 inputs and 64 output values\n",
    "dense1 = Layer_Dense(2, 64, weight_regularizer_l1=5e-4, weight_regularizer_l2=5e-4)\n",
    "\n",
    "# initializing the activation function\n",
    "activation1 = Activation_Relu()\n",
    "\n",
    "# creating the second layer with 64 inputs and 3 output values\n",
    "# as we have 64 outputs from the previous layer dense1\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "# initializing the loss activation function\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossEntropy()\n",
    "\n",
    "# initializing the optimizer SGD\n",
    "optimizer = Optimizer_Adam(learning_rate=0.02, decay = 1e-3)\n",
    "\n",
    "for epoch in range(10001):\n",
    "    \n",
    "    dense1.forward(X)\n",
    "    \n",
    "    activation1.forward(dense1.output)\n",
    "    \n",
    "    dense2.forward(activation1.output)\n",
    "    \n",
    "    \n",
    "    data_loss = loss_activation.forward(dense2.output, y)\n",
    "    \n",
    "    regularization_loss = loss_activation.loss.regularization_loss(dense1) +loss_activation.loss.regularization_loss(dense2)\n",
    "    \n",
    "    loss = data_loss + regularization_loss\n",
    "    \n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "\n",
    "    accuracy = np.mean(predictions == y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'Epoch: {epoch}, ' + f'acc: {accuracy}, '+ f'loss: {loss}', f'lr: {optimizer.current_learning_rate} ')\n",
    "    \n",
    "    \n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "\n",
    "    # backward passing from dense2 layer with the gradient\n",
    "    # from loss activation function\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "\n",
    "    # backward passing the activation 1 function with the gradient from dense layer 2\n",
    "    activation1.backward(dense2.dinputs)\n",
    "\n",
    "    # backward passing the gradient from activation1 with the dense layer 1\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # updating the weights and biases of dense layer 1 and 2\n",
    "    optimizer.pre_update_parameters()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cb62a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1.forward(X_test)\n",
    "\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "loss = loss_activation.forward(dense2.output, y_test)\n",
    "\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "\n",
    "if len(y_test.shape)==2:\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "    \n",
    "accuracy = np.mean(predictions == y_test)\n",
    "\n",
    "print(f\"validation, acc: {accuracy}, loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a619026a",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd70885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "droput_rate = 0.5\n",
    "\n",
    "example_output = [0.27, -1.03, 0.67, 0.99, 0.05, -0.37, -2.01, 1.13, -0.07, 0.73]\n",
    "\n",
    "while True:\n",
    "    index = random.randint(0, len(example_output) -1)\n",
    "    example_output[index] = 0\n",
    "    \n",
    "    dropped_out = 0\n",
    "    for value in example_output:\n",
    "        if value == 0:\n",
    "            dropped_out +=1\n",
    "            \n",
    "    if dropped_out / len(example_output) >= droput_rate:\n",
    "        break\n",
    "        \n",
    "print(example_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f842c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.binomial(2, 0.5, size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586d5cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "droput_rate = 0.5\n",
    "\n",
    "example_output = np.array([0.27, -1.03, 0.67, 0.99, 0.05, -0.37, -2.01, 1.13, -0.07, 0.73])\n",
    "example_output *= np.random.binomial(1, 1-droput_rate, example_output.shape) / (1 - droput_rate)\n",
    "\n",
    "example_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7636d1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "droput_rate = 0.5\n",
    "\n",
    "example_output = np.array([0.27, -1.03, 0.67, 0.99, 0.05, -0.37, -2.01, 1.13, -0.07, 0.73])\n",
    "\n",
    "print(\"Initial sum\", sum(example_output))\n",
    "\n",
    "sums = []\n",
    "\n",
    "for i in range(100001):\n",
    "    example_output2 = example_output * np.random.binomial(1, 1-droput_rate, example_output.shape)/ (1- droput_rate)\n",
    "    \n",
    "    sums.append(sum(example_output2))\n",
    "    \n",
    "print(\"Mean sum\", np.mean(sums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9024a7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np \n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "class Layer_Dense:\n",
    "    \n",
    "    # getting the number of inputs and neurons\n",
    "    def __init__(self, n_inputs, n_neurons,\n",
    "                 weight_regularizer_l1=0,\n",
    "                bias_regularizer_l1=0,\n",
    "                weight_regularizer_l2=0,\n",
    "                bias_regularizer_l2=0):\n",
    "        \n",
    "        #initializing random weights in range \n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "        self.weight_regularizer_l1= weight_regularizer_l1\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        \n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "        \n",
    "    # getting the output with (mx +b)\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    \n",
    "    # backward pass which takes the derivative values from the next function \n",
    "    def backward(self, dvalues):\n",
    "        # getting the partial derivative with the inputs\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims = True)\n",
    "        \n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dl1 = np.ones_like(self.weights)\n",
    "            dl1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dl1\n",
    "            \n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "            \n",
    "        if self.bias_regularizer_l1 >0:\n",
    "            dl1 = np.ones_like(self.biases)\n",
    "            dl1[self.biases < 0] = -1\n",
    "            self.biases += self.bias_regularizer_l1 * dl1\n",
    "            \n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
    "        \n",
    "        \n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "        \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Activation_Relu:\n",
    "    \n",
    "    # The method which takes the input\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        self.inputs = inputs\n",
    "        # returns output with the maximum of 0 and the input value\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <=0] = 0\n",
    "        \n",
    "#---------------------------------------------------------------------\n",
    "        \n",
    "# Creating an Activation function class\n",
    "class Activation_Softmax:\n",
    "    \n",
    "    # The method which takes the input\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        \n",
    "        norm_vals = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        \n",
    "        self.output = norm_vals\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        \n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "                \n",
    "                single_output = single_output.reshape(-1,1)\n",
    "                \n",
    "                jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "                \n",
    "                self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "                \n",
    "#---------------------------------------------------------------------\n",
    "                \n",
    "class Loss:\n",
    "    \n",
    "    def regularization_loss(self, layer):\n",
    "        \n",
    "        regularization_loss = 0\n",
    "        \n",
    "        #------------------L1 Regularization------------------------\n",
    "\n",
    "        \n",
    "        if layer.weight_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l1 * np.sum(abs(layer.weights))\n",
    "            \n",
    "        if layer.bias_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l1 * np.sum(abs(layer.biases))\n",
    "        \n",
    "        #------------------L2 Regularization------------------------\n",
    "        \n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
    "            \n",
    "        if layer.bias_regularizer_l2 > 0 :\n",
    "            regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
    "            \n",
    "        return regularization_loss\n",
    "    \n",
    "        \n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        \n",
    "        return data_loss\n",
    "    \n",
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "    \n",
    "    def forward(self, y_pred, y_target):\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        # clipping the values so we don't end up with inf error\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "\n",
    "        # checking if 1D array so we just need to get that index value\n",
    "        if len(y_target.shape) == 1:\n",
    "            correct = y_pred_clipped[range(samples), y_target]\n",
    "        # checking if one-hot-encoded values, then multiplying and summing up\n",
    "        elif len(y_target.shape) == 2:\n",
    "            correct = np.sum(y_pred_clipped * y_target, axis=1)\n",
    "        \n",
    "        negative_log = -np.log(correct)\n",
    "        \n",
    "        return negative_log\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "        \n",
    "        if len(y_true.shape) ==1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "            \n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples\n",
    "    \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Accuracy:\n",
    "    def get_accuracy(self,y_pred, y_true):\n",
    "        \n",
    "        predictions = np.argmax(y_pred, axis=1)\n",
    "                \n",
    "        if len(y_true.shape) == 2:\n",
    "            actual = np.argmax(y_true, axis=1)\n",
    "        else:\n",
    "            actual = np.argmax(y_true)\n",
    "        \n",
    "        accuracy = np.mean(predictions == y)\n",
    "        \n",
    "        print(\"accuracy : \", accuracy)\n",
    "        \n",
    "\n",
    "#.......................................................................\n",
    "\n",
    "class Activation_Softmax_Loss_CategoricalCrossEntropy():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # initializing the activation function\n",
    "        self.activation = Activation_Softmax()\n",
    "        \n",
    "        # initializing the loss function\n",
    "        self.loss = Loss_CategoricalCrossEntropy()\n",
    "    \n",
    "    # forward pass method, takes the input from \n",
    "    # the dense layer and the actual value\n",
    "    def forward(self, inputs, y_true):\n",
    "        \n",
    "        # forward passing the inputs in the activation function\n",
    "        self.activation.forward(inputs)\n",
    "        \n",
    "        # getting the output from the activation function\n",
    "        self.output = self.activation.output\n",
    "        \n",
    "        # calculating loss with the activation function output and actual value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        # checking if the values are one-hot-encoded\n",
    "        if len(y_true.shape) ==2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "            \n",
    "        \n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "        # calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -=1\n",
    "        \n",
    "        # normalizing gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "    \n",
    "#---------------------------------------------------------------------\n",
    "    \n",
    "class Optimizer_SGD:\n",
    "    \n",
    "    # setting learning rate default as 1.\n",
    "    def __init__(self, learning_rate = 1.0, decay=0., momentum =0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def pre_update_parameters(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "        \n",
    "    def update_params(self, layer):\n",
    "        \n",
    "        # if we use momentum\n",
    "        if self.momentum:\n",
    "            # if the layer does not contain momentum arrays\n",
    "            if not hasattr(layer, \"weight_momentums\"):\n",
    "                # creating weight_momentums and bias_momentums arrays initialized with zero\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            \n",
    "            # getting the weight updates with momentum - take previous factors\n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "            \n",
    "            # getting bias updates\n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "        \n",
    "        else:\n",
    "\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates =  -self.current_learning_rate * layer.dbiases\n",
    "            \n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "        \n",
    "    def post_update_parameters(self):\n",
    "        self.iterations +=1\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "        \n",
    "class Optimizer_AdaGrad:\n",
    "    \n",
    "    # setting learning rate default as 1.\n",
    "    def __init__(self, learning_rate = 1.0, decay=0., epsilon =1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def pre_update_parameters(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "        \n",
    "    def update_params(self, layer):\n",
    "        # checking if layer has attribute weight_cache\n",
    "        if not hasattr(layer, \"weight_cache\"):\n",
    "            # creating weight_cache and bias_cache\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        # getting the square of the gradients\n",
    "        layer.weight_cache += layer.dweights **2\n",
    "        layer.bias_cache += layer.dbiases **2\n",
    "        \n",
    "        # calculating the result and adding it to the weights and biases\n",
    "        layer.weights += - self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += - self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "        \n",
    "    def post_update_parameters(self):\n",
    "        self.iterations +=1\n",
    "                \n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class Optimizer_Adam:\n",
    "    \n",
    "    def __init__(self, learning_rate = 0.001, decay=0. ,\n",
    "                 epsilon=1e-7, beta_1=0.9, beta_2 = 0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        \n",
    "    def pre_update_parameters(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "        \n",
    "    def update_params(self, layer):\n",
    "        # checking if layer has attribute weight_cache\n",
    "        if not hasattr(layer, \"weight_cache\"):\n",
    "            \n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            \n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        \n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1- self.beta_1) * layer.dweights\n",
    "        \n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1- self.beta_1) * layer.dbiases\n",
    "        \n",
    "        \n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        \n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "\n",
    "        \n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights **2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases **2\n",
    "        \n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations +1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations +1))\n",
    "        \n",
    "        \n",
    "        layer.weights += - self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected)+ self.epsilon)\n",
    "        layer.biases += - self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected)+ self.epsilon)\n",
    "\n",
    "        \n",
    "    def post_update_parameters(self):\n",
    "        self.iterations +=1\n",
    "        \n",
    "        \n",
    "        \n",
    "class Layer_Dropout:\n",
    "    \n",
    "    def __init__(self, rate):\n",
    "        self.rate = 1- rate\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.intputs = inputs\n",
    "        \n",
    "        self.binary_mask = np.random.binomial(1, self.rate, size=inputs.shape) / self.rate\n",
    "        \n",
    "        self.output = inputs * self.binary_mask\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * self.binary_mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d926b30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, acc: 0.38, loss: 1.0985890626907349 lr: 0.02 \n",
      "Epoch: 100, acc: 0.39, loss: 1.0544819831848145 lr: 0.018198362147406735 \n",
      "Epoch: 200, acc: 0.43333333333333335, loss: 1.0319746732711792 lr: 0.016680567139282735 \n",
      "Epoch: 300, acc: 0.44333333333333336, loss: 0.9989341497421265 lr: 0.015396458814472672 \n",
      "Epoch: 400, acc: 0.44, loss: 1.0134342908859253 lr: 0.014295925661186561 \n",
      "Epoch: 500, acc: 0.47, loss: 0.9917550683021545 lr: 0.0133422281521014 \n",
      "Epoch: 600, acc: 0.43, loss: 1.0112937688827515 lr: 0.012507817385866166 \n",
      "Epoch: 700, acc: 0.4266666666666667, loss: 0.9907911419868469 lr: 0.011771630370806356 \n",
      "Epoch: 800, acc: 0.45666666666666667, loss: 0.9738128781318665 lr: 0.011117287381878822 \n",
      "Epoch: 900, acc: 0.4766666666666667, loss: 0.9676095843315125 lr: 0.010531858873091101 \n",
      "Epoch: 1000, acc: 0.4633333333333333, loss: 0.9677438139915466 lr: 0.010005002501250623 \n",
      "Epoch: 1100, acc: 0.44666666666666666, loss: 0.9753645658493042 lr: 0.009528346831824679 \n",
      "Epoch: 1200, acc: 0.3933333333333333, loss: 0.9934157133102417 lr: 0.009095043201455207 \n",
      "Epoch: 1300, acc: 0.47333333333333333, loss: 0.9646670818328857 lr: 0.008699434536755112 \n",
      "Epoch: 1400, acc: 0.42, loss: 0.9676381349563599 lr: 0.008336807002917883 \n",
      "Epoch: 1500, acc: 0.48333333333333334, loss: 0.9476967453956604 lr: 0.008003201280512205 \n",
      "Epoch: 1600, acc: 0.4533333333333333, loss: 0.959132969379425 lr: 0.007695267410542516 \n",
      "Epoch: 1700, acc: 0.48333333333333334, loss: 0.9226891994476318 lr: 0.007410151908114116 \n",
      "Epoch: 1800, acc: 0.4866666666666667, loss: 0.938844621181488 lr: 0.007145409074669526 \n",
      "Epoch: 1900, acc: 0.5066666666666667, loss: 0.9244316816329956 lr: 0.006898930665746809 \n",
      "Epoch: 2000, acc: 0.5133333333333333, loss: 0.9186391234397888 lr: 0.006668889629876626 \n",
      "Epoch: 2100, acc: 0.5166666666666667, loss: 0.9568737745285034 lr: 0.006453694740238787 \n",
      "Epoch: 2200, acc: 0.5366666666666666, loss: 0.9460123777389526 lr: 0.006251953735542357 \n",
      "Epoch: 2300, acc: 0.5533333333333333, loss: 0.9290266036987305 lr: 0.006062443164595333 \n",
      "Epoch: 2400, acc: 0.49666666666666665, loss: 0.9224456548690796 lr: 0.005884083553986467 \n",
      "Epoch: 2500, acc: 0.5266666666666666, loss: 0.9655680060386658 lr: 0.0057159188339525584 \n",
      "Epoch: 2600, acc: 0.5066666666666667, loss: 0.9377939701080322 lr: 0.005557099194220616 \n",
      "Epoch: 2700, acc: 0.5166666666666667, loss: 0.9509673118591309 lr: 0.005406866720735334 \n",
      "Epoch: 2800, acc: 0.5366666666666666, loss: 0.9128796458244324 lr: 0.0052645433008686494 \n",
      "Epoch: 2900, acc: 0.52, loss: 0.939895749092102 lr: 0.005129520389843549 \n",
      "Epoch: 3000, acc: 0.5366666666666666, loss: 0.9246972799301147 lr: 0.005001250312578145 \n",
      "Epoch: 3100, acc: 0.5433333333333333, loss: 0.9809485077857971 lr: 0.0048792388387411565 \n",
      "Epoch: 3200, acc: 0.5, loss: 0.9340061545372009 lr: 0.004763038818766373 \n",
      "Epoch: 3300, acc: 0.5266666666666666, loss: 0.9164406061172485 lr: 0.004652244708071645 \n",
      "Epoch: 3400, acc: 0.54, loss: 0.9387837648391724 lr: 0.0045464878381450335 \n",
      "Epoch: 3500, acc: 0.5133333333333333, loss: 0.9453958868980408 lr: 0.004445432318292954 \n",
      "Epoch: 3600, acc: 0.5333333333333333, loss: 0.9045349359512329 lr: 0.004348771472059143 \n",
      "Epoch: 3700, acc: 0.54, loss: 0.9121471047401428 lr: 0.004256224728665673 \n",
      "Epoch: 3800, acc: 0.49333333333333335, loss: 0.9252984523773193 lr: 0.004167534903104814 \n",
      "Epoch: 3900, acc: 0.4866666666666667, loss: 0.9721301198005676 lr: 0.004082465809348847 \n",
      "Epoch: 4000, acc: 0.5133333333333333, loss: 0.938586950302124 lr: 0.0040008001600320055 \n",
      "Epoch: 4100, acc: 0.5333333333333333, loss: 0.9686017632484436 lr: 0.003922337713277113 \n",
      "Epoch: 4200, acc: 0.5233333333333333, loss: 0.9119618535041809 lr: 0.003846893633391037 \n",
      "Epoch: 4300, acc: 0.5033333333333333, loss: 0.9221022725105286 lr: 0.0037742970371768252 \n",
      "Epoch: 4400, acc: 0.5366666666666666, loss: 0.9490248560905457 lr: 0.003704389701796629 \n",
      "Epoch: 4500, acc: 0.5333333333333333, loss: 0.9293639063835144 lr: 0.0036370249136206583 \n",
      "Epoch: 4600, acc: 0.49, loss: 1.0131597518920898 lr: 0.003572066440435792 \n",
      "Epoch: 4700, acc: 0.4866666666666667, loss: 0.9620745778083801 lr: 0.00350938761186173 \n",
      "Epoch: 4800, acc: 0.5066666666666667, loss: 0.942765474319458 lr: 0.003448870494912916 \n",
      "Epoch: 4900, acc: 0.54, loss: 0.9489192962646484 lr: 0.003390405153415833 \n",
      "Epoch: 5000, acc: 0.52, loss: 0.9097049832344055 lr: 0.0033338889814969164 \n",
      "Epoch: 5100, acc: 0.54, loss: 0.9169812202453613 lr: 0.003279226102639777 \n",
      "Epoch: 5200, acc: 0.53, loss: 0.920606255531311 lr: 0.0032263268269075657 \n",
      "Epoch: 5300, acc: 0.5433333333333333, loss: 0.9230877757072449 lr: 0.0031751071598666455 \n",
      "Epoch: 5400, acc: 0.5233333333333333, loss: 0.894650399684906 lr: 0.0031254883575558678 \n",
      "Epoch: 5500, acc: 0.5233333333333333, loss: 0.9238633513450623 lr: 0.0030773965225419295 \n",
      "Epoch: 5600, acc: 0.48333333333333334, loss: 0.9622082710266113 lr: 0.0030307622367025306 \n",
      "Epoch: 5700, acc: 0.55, loss: 0.8885593414306641 lr: 0.0029855202268995375 \n",
      "Epoch: 5800, acc: 0.54, loss: 0.928910493850708 lr: 0.0029416090601559054 \n",
      "Epoch: 5900, acc: 0.5333333333333333, loss: 0.9210144281387329 lr: 0.0028989708653428033 \n",
      "Epoch: 6000, acc: 0.52, loss: 0.9174175262451172 lr: 0.002857551078725532 \n",
      "Epoch: 6100, acc: 0.53, loss: 0.9210414886474609 lr: 0.0028172982110156357 \n",
      "Epoch: 6200, acc: 0.54, loss: 0.9421951174736023 lr: 0.0027781636338380334 \n",
      "Epoch: 6300, acc: 0.5533333333333333, loss: 0.9059951901435852 lr: 0.0027401013837511983 \n",
      "Epoch: 6400, acc: 0.47, loss: 0.9496831297874451 lr: 0.0027030679821597515 \n",
      "Epoch: 6500, acc: 0.52, loss: 0.912136435508728 lr: 0.0026670222696359514 \n",
      "Epoch: 6600, acc: 0.5166666666666667, loss: 0.9302509427070618 lr: 0.0026319252533228057 \n",
      "Epoch: 6700, acc: 0.53, loss: 0.9680711030960083 lr: 0.0025977399662293803 \n",
      "Epoch: 6800, acc: 0.5533333333333333, loss: 0.8945786356925964 lr: 0.0025644313373509426 \n",
      "Epoch: 6900, acc: 0.5366666666666666, loss: 0.9419999122619629 lr: 0.00253196607165464 \n",
      "Epoch: 7000, acc: 0.5133333333333333, loss: 0.9423335790634155 lr: 0.002500312539067383 \n",
      "Epoch: 7100, acc: 0.53, loss: 0.9278615117073059 lr: 0.0024694406716878627 \n",
      "Epoch: 7200, acc: 0.56, loss: 0.897907018661499 lr: 0.0024393218685205514 \n",
      "Epoch: 7300, acc: 0.47333333333333333, loss: 0.9604164361953735 lr: 0.0024099289070972406 \n",
      "Epoch: 7400, acc: 0.52, loss: 0.9158374071121216 lr: 0.0023812358614120725 \n",
      "Epoch: 7500, acc: 0.51, loss: 0.9124456644058228 lr: 0.002353218025650076 \n",
      "Epoch: 7600, acc: 0.5, loss: 0.9660158157348633 lr: 0.002325851843237586 \n",
      "Epoch: 7700, acc: 0.52, loss: 0.923937201499939 lr: 0.0022991148407862975 \n",
      "Epoch: 7800, acc: 0.5266666666666666, loss: 0.909994900226593 lr: 0.0022729855665416525 \n",
      "Epoch: 7900, acc: 0.5433333333333333, loss: 0.9247831106185913 lr: 0.002247443532981234 \n",
      "Epoch: 8000, acc: 0.5233333333333333, loss: 0.9218495488166809 lr: 0.00222246916324036 \n",
      "Epoch: 8100, acc: 0.5433333333333333, loss: 0.9015046954154968 lr: 0.0021980437410704474 \n",
      "Epoch: 8200, acc: 0.5366666666666666, loss: 0.9113244414329529 lr: 0.0021741493640613113 \n",
      "Epoch: 8300, acc: 0.5066666666666667, loss: 0.9464935064315796 lr: 0.0021507688998817077 \n",
      "Epoch: 8400, acc: 0.5233333333333333, loss: 0.9279142022132874 lr: 0.002127885945313331 \n",
      "Epoch: 8500, acc: 0.5133333333333333, loss: 0.9167863130569458 lr: 0.0021054847878724074 \n",
      "Epoch: 8600, acc: 0.5133333333333333, loss: 0.9728239178657532 lr: 0.0020835503698301903 \n",
      "Epoch: 8700, acc: 0.5233333333333333, loss: 0.9208953976631165 lr: 0.0020620682544592226 \n",
      "Epoch: 8800, acc: 0.49333333333333335, loss: 0.934076726436615 lr: 0.002041024594346362 \n",
      "Epoch: 8900, acc: 0.5133333333333333, loss: 0.9297786355018616 lr: 0.002020406101626427 \n",
      "Epoch: 9000, acc: 0.5233333333333333, loss: 0.9034937620162964 lr: 0.002000200020002 \n",
      "Epoch: 9100, acc: 0.5, loss: 0.922423779964447 lr: 0.0019803940984255866 \n",
      "Epoch: 9200, acc: 0.5666666666666667, loss: 0.8831465840339661 lr: 0.0019609765663300325 \n",
      "Epoch: 9300, acc: 0.5266666666666666, loss: 0.9267739057540894 lr: 0.0019419361103019711 \n",
      "Epoch: 9400, acc: 0.5266666666666666, loss: 0.901197612285614 lr: 0.0019232618521011636 \n",
      "Epoch: 9500, acc: 0.5366666666666666, loss: 0.919665515422821 lr: 0.0019049433279359938 \n",
      "Epoch: 9600, acc: 0.55, loss: 0.9085578322410583 lr: 0.0018869704689121615 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9700, acc: 0.5566666666666666, loss: 0.9112416505813599 lr: 0.001869333582577811 \n",
      "Epoch: 9800, acc: 0.49, loss: 0.9573771357536316 lr: 0.0018520233354940275 \n",
      "Epoch: 9900, acc: 0.5333333333333333, loss: 0.9452655911445618 lr: 0.0018350307367648406 \n",
      "Epoch: 10000, acc: 0.5166666666666667, loss: 0.9295267462730408 lr: 0.0018183471224656785 \n"
     ]
    }
   ],
   "source": [
    "# creating the dataset that has three classes\n",
    "X, y = spiral_data(samples = 100, classes=3)\n",
    "\n",
    "# creating the first layer with 2 inputs and 64 output values\n",
    "dense1 = Layer_Dense(2, 64, weight_regularizer_l1=5e-4, weight_regularizer_l2=5e-4)\n",
    "\n",
    "# initializing the activation function\n",
    "activation1 = Activation_Relu()\n",
    "\n",
    "# adding a dropout layer\n",
    "dropout1 = Layer_Dropout(0.2)\n",
    "\n",
    "# creating the second layer with 64 inputs and 3 output values\n",
    "# as we have 64 outputs from the previous layer dense1\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "# initializing the loss activation function\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossEntropy()\n",
    "\n",
    "# initializing the optimizer SGD\n",
    "optimizer = Optimizer_Adam(learning_rate=0.02, decay = 1e-3)\n",
    "\n",
    "for epoch in range(10001):\n",
    "    \n",
    "    dense1.forward(X)\n",
    "    \n",
    "    activation1.forward(dense1.output)\n",
    "    \n",
    "    dropout1.forward(activation1.output)\n",
    "    \n",
    "    dense2.forward(dropout1.output)\n",
    "        \n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "    \n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "\n",
    "    accuracy = np.mean(predictions == y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'Epoch: {epoch}, ' + f'acc: {accuracy}, '+ f'loss: {loss}', f'lr: {optimizer.current_learning_rate} ')\n",
    "    \n",
    "    \n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "\n",
    "    # backward passing from dense2 layer with the gradient\n",
    "    # from loss activation function\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    \n",
    "    # backwardpassing the dropout layer\n",
    "    dropout1.backward(dense2.dinputs)\n",
    "    \n",
    "    # backward passing the activation 1 function with the gradient from dense layer 2\n",
    "    activation1.backward(dropout1.dinputs)\n",
    "\n",
    "    # backward passing the gradient from activation1 with the dense layer 1\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # updating the weights and biases of dense layer 1 and 2\n",
    "    optimizer.pre_update_parameters()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eeea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1.forward(X_test)\n",
    "\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "loss = loss_activation.forward(dense2.output, y_test)\n",
    "\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "\n",
    "if len(y_test.shape)==2:\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "    \n",
    "accuracy = np.mean(predictions == y_test)\n",
    "\n",
    "print(f\"validation, acc: {accuracy}, loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a98125",
   "metadata": {},
   "source": [
    "# Binary Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2ab75b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np \n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "class Layer_Dense:\n",
    "    \n",
    "    # getting the number of inputs and neurons\n",
    "    def __init__(self, n_inputs, n_neurons,\n",
    "                 weight_regularizer_l1=0,\n",
    "                bias_regularizer_l1=0,\n",
    "                weight_regularizer_l2=0,\n",
    "                bias_regularizer_l2=0):\n",
    "        \n",
    "        #initializing random weights in range \n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "        self.weight_regularizer_l1= weight_regularizer_l1\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        \n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "        \n",
    "    # getting the output with (mx +b)\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    \n",
    "    # backward pass which takes the derivative values from the next function \n",
    "    def backward(self, dvalues):\n",
    "        # getting the partial derivative with the inputs\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims = True)\n",
    "        \n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dl1 = np.ones_like(self.weights)\n",
    "            dl1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dl1\n",
    "            \n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "            \n",
    "        if self.bias_regularizer_l1 >0:\n",
    "            dl1 = np.ones_like(self.biases)\n",
    "            dl1[self.biases < 0] = -1\n",
    "            self.biases += self.bias_regularizer_l1 * dl1\n",
    "            \n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
    "        \n",
    "        \n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "        \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Activation_Relu:\n",
    "    \n",
    "    # The method which takes the input\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        self.inputs = inputs\n",
    "        # returns output with the maximum of 0 and the input value\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <=0] = 0\n",
    "        \n",
    "#---------------------------------------------------------------------\n",
    "        \n",
    "# Creating an Activation function class\n",
    "class Activation_Softmax:\n",
    "    \n",
    "    # The method which takes the input\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        \n",
    "        norm_vals = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        \n",
    "        self.output = norm_vals\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        \n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "                \n",
    "                single_output = single_output.reshape(-1,1)\n",
    "                \n",
    "                jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "                \n",
    "                self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "                \n",
    "#---------------------------------------------------------------------\n",
    "                \n",
    "class Loss:\n",
    "    \n",
    "    def regularization_loss(self, layer):\n",
    "        \n",
    "        regularization_loss = 0\n",
    "        \n",
    "        #------------------L1 Regularization------------------------\n",
    "\n",
    "        \n",
    "        if layer.weight_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l1 * np.sum(abs(layer.weights))\n",
    "            \n",
    "        if layer.bias_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l1 * np.sum(abs(layer.biases))\n",
    "        \n",
    "        #------------------L2 Regularization------------------------\n",
    "        \n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
    "            \n",
    "        if layer.bias_regularizer_l2 > 0 :\n",
    "            regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
    "            \n",
    "        return regularization_loss\n",
    "    \n",
    "        \n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        \n",
    "        return data_loss\n",
    "    \n",
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "    \n",
    "    def forward(self, y_pred, y_target):\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        # clipping the values so we don't end up with inf error\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "\n",
    "        # checking if 1D array so we just need to get that index value\n",
    "        if len(y_target.shape) == 1:\n",
    "            correct = y_pred_clipped[range(samples), y_target]\n",
    "        # checking if one-hot-encoded values, then multiplying and summing up\n",
    "        elif len(y_target.shape) == 2:\n",
    "            correct = np.sum(y_pred_clipped * y_target, axis=1)\n",
    "        \n",
    "        negative_log = -np.log(correct)\n",
    "        \n",
    "        return negative_log\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "        \n",
    "        if len(y_true.shape) ==1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "            \n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples\n",
    "    \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Accuracy:\n",
    "    def get_accuracy(self,y_pred, y_true):\n",
    "        \n",
    "        predictions = np.argmax(y_pred, axis=1)\n",
    "                \n",
    "        if len(y_true.shape) == 2:\n",
    "            actual = np.argmax(y_true, axis=1)\n",
    "        else:\n",
    "            actual = np.argmax(y_true)\n",
    "        \n",
    "        accuracy = np.mean(predictions == y)\n",
    "        \n",
    "        print(\"accuracy : \", accuracy)\n",
    "        \n",
    "\n",
    "#.......................................................................\n",
    "\n",
    "class Activation_Softmax_Loss_CategoricalCrossEntropy():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # initializing the activation function\n",
    "        self.activation = Activation_Softmax()\n",
    "        \n",
    "        # initializing the loss function\n",
    "        self.loss = Loss_CategoricalCrossEntropy()\n",
    "    \n",
    "    # forward pass method, takes the input from \n",
    "    # the dense layer and the actual value\n",
    "    def forward(self, inputs, y_true):\n",
    "        \n",
    "        # forward passing the inputs in the activation function\n",
    "        self.activation.forward(inputs)\n",
    "        \n",
    "        # getting the output from the activation function\n",
    "        self.output = self.activation.output\n",
    "        \n",
    "        # calculating loss with the activation function output and actual value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        # checking if the values are one-hot-encoded\n",
    "        if len(y_true.shape) ==2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "            \n",
    "        \n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "        # calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -=1\n",
    "        \n",
    "        # normalizing gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "    \n",
    "#---------------------------------------------------------------------\n",
    "    \n",
    "class Activation_Sigmoid:\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * (1 - self.output) * self.output\n",
    "        \n",
    "\n",
    "class Loss_BinaryCrossentropy(Loss):\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        sample_losses = -(y_true * np.log(y_pred_clipped) + (1-y_true) * np.log(1- y_pred_clipped))\n",
    "        \n",
    "        sample_losses = np.mean(sample_losses, axis = -1)\n",
    "        \n",
    "        return sample_losses\n",
    "    \n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        self.dinputs = -(y_true / clipped_dvalues - (1-y_true) / (1-clipped_dvalues))/ outputs\n",
    "        \n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "        \n",
    "class Optimizer_SGD:\n",
    "    \n",
    "    # setting learning rate default as 1.\n",
    "    def __init__(self, learning_rate = 1.0, decay=0., momentum =0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def pre_update_parameters(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "        \n",
    "    def update_params(self, layer):\n",
    "        \n",
    "        # if we use momentum\n",
    "        if self.momentum:\n",
    "            # if the layer does not contain momentum arrays\n",
    "            if not hasattr(layer, \"weight_momentums\"):\n",
    "                # creating weight_momentums and bias_momentums arrays initialized with zero\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            \n",
    "            # getting the weight updates with momentum - take previous factors\n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "            \n",
    "            # getting bias updates\n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "        \n",
    "        else:\n",
    "\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates =  -self.current_learning_rate * layer.dbiases\n",
    "            \n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "        \n",
    "    def post_update_parameters(self):\n",
    "        self.iterations +=1\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "        \n",
    "class Optimizer_AdaGrad:\n",
    "    \n",
    "    # setting learning rate default as 1.\n",
    "    def __init__(self, learning_rate = 1.0, decay=0., epsilon =1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def pre_update_parameters(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "        \n",
    "    def update_params(self, layer):\n",
    "        # checking if layer has attribute weight_cache\n",
    "        if not hasattr(layer, \"weight_cache\"):\n",
    "            # creating weight_cache and bias_cache\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        # getting the square of the gradients\n",
    "        layer.weight_cache += layer.dweights **2\n",
    "        layer.bias_cache += layer.dbiases **2\n",
    "        \n",
    "        # calculating the result and adding it to the weights and biases\n",
    "        layer.weights += - self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += - self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "        \n",
    "    def post_update_parameters(self):\n",
    "        self.iterations +=1\n",
    "                \n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class Optimizer_Adam:\n",
    "    \n",
    "    def __init__(self, learning_rate = 0.001, decay=0. ,\n",
    "                 epsilon=1e-7, beta_1=0.9, beta_2 = 0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        \n",
    "    def pre_update_parameters(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "        \n",
    "    def update_params(self, layer):\n",
    "        # checking if layer has attribute weight_cache\n",
    "        if not hasattr(layer, \"weight_cache\"):\n",
    "            \n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            \n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        \n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1- self.beta_1) * layer.dweights\n",
    "        \n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1- self.beta_1) * layer.dbiases\n",
    "        \n",
    "        \n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        \n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "\n",
    "        \n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights **2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases **2\n",
    "        \n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations +1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations +1))\n",
    "        \n",
    "        \n",
    "        layer.weights += - self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected)+ self.epsilon)\n",
    "        layer.biases += - self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected)+ self.epsilon)\n",
    "\n",
    "        \n",
    "    def post_update_parameters(self):\n",
    "        self.iterations +=1\n",
    "        \n",
    "        \n",
    "        \n",
    "class Layer_Dropout:\n",
    "    \n",
    "    def __init__(self, rate):\n",
    "        self.rate = 1- rate\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.intputs = inputs\n",
    "        \n",
    "        self.binary_mask = np.random.binomial(1, self.rate, size=inputs.shape) / self.rate\n",
    "        \n",
    "        self.output = inputs * self.binary_mask\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * self.binary_mask\n",
    "    \n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d665ac6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, acc: 0.555, data_loss: 6.154515780508518e-06 loss: 0.6931260380530729  lr: 0.001 \n",
      "Epoch: 100, acc: 0.59, data_loss: 0.0008019400089979171 loss: 0.6718646479099989  lr: 0.0009999505024501287 \n",
      "Epoch: 200, acc: 0.6, data_loss: 0.001079534888267517 loss: 0.6683897131681442  lr: 0.0009999005098992651 \n",
      "Epoch: 300, acc: 0.605, data_loss: 0.0013125455379486084 loss: 0.6652172720432281  lr: 0.000999850522346909 \n",
      "Epoch: 400, acc: 0.605, data_loss: 0.0019169971942901611 loss: 0.6607432887554169  lr: 0.0009998005397923115 \n",
      "Epoch: 500, acc: 0.625, data_loss: 0.002754528522491455 loss: 0.6559043717384339  lr: 0.0009997505622347225 \n",
      "Epoch: 600, acc: 0.64, data_loss: 0.003877665638923645 loss: 0.6499506493806839  lr: 0.0009997005896733929 \n",
      "Epoch: 700, acc: 0.62, data_loss: 0.004973494708538055 loss: 0.6439607974886894  lr: 0.0009996506221075735 \n",
      "Epoch: 800, acc: 0.64, data_loss: 0.0061616690754890445 loss: 0.636773207962513  lr: 0.000999600659536515 \n",
      "Epoch: 900, acc: 0.66, data_loss: 0.007799117803573608 loss: 0.6288053085803985  lr: 0.0009995507019594694 \n",
      "Epoch: 1000, acc: 0.665, data_loss: 0.009733493328094483 loss: 0.6216359686851501  lr: 0.000999500749375687 \n",
      "Epoch: 1100, acc: 0.675, data_loss: 0.011711524605751037 loss: 0.6126218305826188  lr: 0.0009994508017844195 \n",
      "Epoch: 1200, acc: 0.675, data_loss: 0.014175643563270569 loss: 0.595711876988411  lr: 0.0009994008591849186 \n",
      "Epoch: 1300, acc: 0.76, data_loss: 0.017806607723236084 loss: 0.5778099479675293  lr: 0.0009993509215764362 \n",
      "Epoch: 1400, acc: 0.795, data_loss: 0.022363237380981445 loss: 0.559243730545044  lr: 0.0009993009889582235 \n",
      "Epoch: 1500, acc: 0.815, data_loss: 0.02748764944076538 loss: 0.5405287880897522  lr: 0.0009992510613295335 \n",
      "Epoch: 1600, acc: 0.82, data_loss: 0.032785767078399655 loss: 0.5229372022151947  lr: 0.0009992011386896176 \n",
      "Epoch: 1700, acc: 0.815, data_loss: 0.03793215799331665 loss: 0.5067968671321869  lr: 0.0009991512210377285 \n",
      "Epoch: 1800, acc: 0.81, data_loss: 0.04254430866241455 loss: 0.4941100809574127  lr: 0.0009991013083731183 \n",
      "Epoch: 1900, acc: 0.815, data_loss: 0.046297867298126225 loss: 0.4825148177146912  lr: 0.0009990514006950402 \n",
      "Epoch: 2000, acc: 0.82, data_loss: 0.04962339115142823 loss: 0.47196387362480163  lr: 0.0009990014980027463 \n",
      "Epoch: 2100, acc: 0.835, data_loss: 0.052542001247406 loss: 0.4626499714851379  lr: 0.0009989516002954898 \n",
      "Epoch: 2200, acc: 0.84, data_loss: 0.05504381084442139 loss: 0.4544018747806549  lr: 0.000998901707572524 \n",
      "Epoch: 2300, acc: 0.845, data_loss: 0.057213665008544926 loss: 0.44719672298431395  lr: 0.0009988518198331018 \n",
      "Epoch: 2400, acc: 0.84, data_loss: 0.05911721038818359 loss: 0.44072191166877744  lr: 0.0009988019370764769 \n",
      "Epoch: 2500, acc: 0.855, data_loss: 0.0607258415222168 loss: 0.4348914957046509  lr: 0.0009987520593019025 \n",
      "Epoch: 2600, acc: 0.865, data_loss: 0.06179716682434082 loss: 0.42629274225234987  lr: 0.000998702186508632 \n",
      "Epoch: 2700, acc: 0.85, data_loss: 0.06294624900817872 loss: 0.41824292039871214  lr: 0.00099865231869592 \n",
      "Epoch: 2800, acc: 0.865, data_loss: 0.06443295001983643 loss: 0.41123792648315427  lr: 0.0009986024558630198 \n",
      "Epoch: 2900, acc: 0.875, data_loss: 0.06555960750579834 loss: 0.40566454505920413  lr: 0.0009985525980091856 \n",
      "Epoch: 3000, acc: 0.88, data_loss: 0.06642753887176514 loss: 0.40020493674278257  lr: 0.000998502745133672 \n",
      "Epoch: 3100, acc: 0.885, data_loss: 0.06722492599487305 loss: 0.3952531876564026  lr: 0.0009984528972357331 \n",
      "Epoch: 3200, acc: 0.885, data_loss: 0.06786170291900635 loss: 0.3906450538635254  lr: 0.0009984030543146237 \n",
      "Epoch: 3300, acc: 0.885, data_loss: 0.06836921882629395 loss: 0.3862907977104187  lr: 0.0009983532163695982 \n",
      "Epoch: 3400, acc: 0.885, data_loss: 0.0687692985534668 loss: 0.3821637258529663  lr: 0.000998303383399912 \n",
      "Epoch: 3500, acc: 0.89, data_loss: 0.06888471794128417 loss: 0.3735153551101684  lr: 0.0009982535554048193 \n",
      "Epoch: 3600, acc: 0.89, data_loss: 0.0693319206237793 loss: 0.3634247844219208  lr: 0.000998203732383576 \n",
      "Epoch: 3700, acc: 0.895, data_loss: 0.0699669017791748 loss: 0.3562758460044861  lr: 0.0009981539143354365 \n",
      "Epoch: 3800, acc: 0.9, data_loss: 0.07054084014892578 loss: 0.35036155056953433  lr: 0.0009981041012596574 \n",
      "Epoch: 3900, acc: 0.9, data_loss: 0.07105669498443604 loss: 0.34486752867698667  lr: 0.0009980542931554933 \n",
      "Epoch: 4000, acc: 0.91, data_loss: 0.07158867931365967 loss: 0.33971784925460813  lr: 0.0009980044900222008 \n",
      "Epoch: 4100, acc: 0.915, data_loss: 0.07222032070159912 loss: 0.33432697772979736  lr: 0.0009979546918590348 \n",
      "Epoch: 4200, acc: 0.91, data_loss: 0.07288017082214356 loss: 0.32966347742080687  lr: 0.0009979048986652524 \n",
      "Epoch: 4300, acc: 0.915, data_loss: 0.07346360206604004 loss: 0.32531182289123534  lr: 0.000997855110440109 \n",
      "Epoch: 4400, acc: 0.915, data_loss: 0.07399599075317383 loss: 0.32119138360023497  lr: 0.0009978053271828614 \n",
      "Epoch: 4500, acc: 0.93, data_loss: 0.07449487495422363 loss: 0.3172981505393982  lr: 0.0009977555488927658 \n",
      "Epoch: 4600, acc: 0.93, data_loss: 0.07490628433227539 loss: 0.3136247493028641  lr: 0.000997705775569079 \n",
      "Epoch: 4700, acc: 0.93, data_loss: 0.0752682638168335 loss: 0.3101405155658722  lr: 0.0009976560072110577 \n",
      "Epoch: 4800, acc: 0.93, data_loss: 0.07560120010375976 loss: 0.3068148889541626  lr: 0.0009976062438179587 \n",
      "Epoch: 4900, acc: 0.93, data_loss: 0.07591110801696778 loss: 0.30362946128845214  lr: 0.0009975564853890394 \n",
      "Epoch: 5000, acc: 0.925, data_loss: 0.07618326950073243 loss: 0.3005784701108932  lr: 0.000997506731923557 \n",
      "Epoch: 5100, acc: 0.925, data_loss: 0.07640974044799806 loss: 0.2976563107967377  lr: 0.0009974569834207687 \n",
      "Epoch: 5200, acc: 0.925, data_loss: 0.07659546852111816 loss: 0.29485520541667937  lr: 0.0009974072398799322 \n",
      "Epoch: 5300, acc: 0.925, data_loss: 0.07674655437469483 loss: 0.2921737742424011  lr: 0.0009973575013003048 \n",
      "Epoch: 5400, acc: 0.93, data_loss: 0.07685132026672363 loss: 0.2895831733942032  lr: 0.0009973077676811448 \n",
      "Epoch: 5500, acc: 0.93, data_loss: 0.07692023658752442 loss: 0.28709719622135166  lr: 0.00099725803902171 \n",
      "Epoch: 5600, acc: 0.935, data_loss: 0.07694292449951172 loss: 0.28469294691085817  lr: 0.0009972083153212581 \n",
      "Epoch: 5700, acc: 0.935, data_loss: 0.07693562507629395 loss: 0.2823623329401016  lr: 0.000997158596579048 \n",
      "Epoch: 5800, acc: 0.935, data_loss: 0.07691286087036132 loss: 0.2800954985618591  lr: 0.0009971088827943377 \n",
      "Epoch: 5900, acc: 0.935, data_loss: 0.07684877204895019 loss: 0.2778309254646301  lr: 0.0009970591739663862 \n",
      "Epoch: 6000, acc: 0.935, data_loss: 0.07683793067932129 loss: 0.27559495329856876  lr: 0.0009970094700944517 \n",
      "Epoch: 6100, acc: 0.935, data_loss: 0.07680366325378418 loss: 0.2735099731683731  lr: 0.0009969597711777935 \n",
      "Epoch: 6200, acc: 0.935, data_loss: 0.07668769645690918 loss: 0.27150839853286746  lr: 0.00099691007721567 \n",
      "Epoch: 6300, acc: 0.935, data_loss: 0.07651461982727051 loss: 0.2695517870187759  lr: 0.000996860388207341 \n",
      "Epoch: 6400, acc: 0.935, data_loss: 0.07631770324707031 loss: 0.26765390288829805  lr: 0.0009968107041520655 \n",
      "Epoch: 6500, acc: 0.935, data_loss: 0.07609755325317383 loss: 0.26579779911041257  lr: 0.000996761025049103 \n",
      "Epoch: 6600, acc: 0.94, data_loss: 0.07586614036560058 loss: 0.26398650550842284  lr: 0.000996711350897713 \n",
      "Epoch: 6700, acc: 0.94, data_loss: 0.07561894607543945 loss: 0.2622240717411041  lr: 0.0009966616816971556 \n",
      "Epoch: 6800, acc: 0.94, data_loss: 0.07535342597961427 loss: 0.2605002942085266  lr: 0.00099661201744669 \n",
      "Epoch: 6900, acc: 0.94, data_loss: 0.07508108997344971 loss: 0.25881859147548675  lr: 0.0009965623581455767 \n",
      "Epoch: 7000, acc: 0.945, data_loss: 0.07479955863952636 loss: 0.25717190515995025  lr: 0.000996512703793076 \n",
      "Epoch: 7100, acc: 0.94, data_loss: 0.07450311756134033 loss: 0.25556477284431456  lr: 0.0009964630543884481 \n",
      "Epoch: 7200, acc: 0.945, data_loss: 0.07419596481323243 loss: 0.25397792327404023  lr: 0.0009964134099309536 \n",
      "Epoch: 7300, acc: 0.945, data_loss: 0.07388725471496582 loss: 0.2524332512617111  lr: 0.0009963637704198528 \n",
      "Epoch: 7400, acc: 0.945, data_loss: 0.0735700855255127 loss: 0.2509200105667114  lr: 0.0009963141358544066 \n",
      "Epoch: 7500, acc: 0.945, data_loss: 0.07324689102172852 loss: 0.24941892397403717  lr: 0.000996264506233876 \n",
      "Epoch: 7600, acc: 0.945, data_loss: 0.07294090461730958 loss: 0.24794155728816986  lr: 0.0009962148815575223 \n",
      "Epoch: 7700, acc: 0.945, data_loss: 0.07262370014190674 loss: 0.246502503991127  lr: 0.000996165261824606 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7800, acc: 0.945, data_loss: 0.07229034423828125 loss: 0.2451041918992996  lr: 0.0009961156470343895 \n",
      "Epoch: 7900, acc: 0.945, data_loss: 0.07195491886138916 loss: 0.2437125722169876  lr: 0.0009960660371861334 \n",
      "Epoch: 8000, acc: 0.945, data_loss: 0.07161364364624023 loss: 0.24235937404632568  lr: 0.0009960164322790998 \n",
      "Epoch: 8100, acc: 0.945, data_loss: 0.07127527618408204 loss: 0.2410322847366333  lr: 0.0009959668323125503 \n",
      "Epoch: 8200, acc: 0.945, data_loss: 0.07093473052978516 loss: 0.23972916877269745  lr: 0.000995917237285747 \n",
      "Epoch: 8300, acc: 0.945, data_loss: 0.0705975399017334 loss: 0.23843735015392303  lr: 0.000995867647197952 \n",
      "Epoch: 8400, acc: 0.945, data_loss: 0.07026161003112794 loss: 0.23716826486587525  lr: 0.0009958180620484277 \n",
      "Epoch: 8500, acc: 0.945, data_loss: 0.0699356689453125 loss: 0.23591947996616364  lr: 0.0009957684818364362 \n",
      "Epoch: 8600, acc: 0.945, data_loss: 0.06963124370574951 loss: 0.2346979776620865  lr: 0.0009957189065612402 \n",
      "Epoch: 8700, acc: 0.945, data_loss: 0.069309006690979 loss: 0.2334969744682312  lr: 0.000995669336222102 \n",
      "Epoch: 8800, acc: 0.945, data_loss: 0.06898722076416015 loss: 0.23231630051136015  lr: 0.000995619770818285 \n",
      "Epoch: 8900, acc: 0.945, data_loss: 0.06866118240356446 loss: 0.23115347254276275  lr: 0.0009955702103490519 \n",
      "Epoch: 9000, acc: 0.945, data_loss: 0.06834481239318849 loss: 0.23000518560409547  lr: 0.000995520654813666 \n",
      "Epoch: 9100, acc: 0.945, data_loss: 0.06802472496032716 loss: 0.2288869025707245  lr: 0.0009954711042113903 \n",
      "Epoch: 9200, acc: 0.945, data_loss: 0.06771405410766601 loss: 0.22776657712459564  lr: 0.0009954215585414883 \n",
      "Epoch: 9300, acc: 0.945, data_loss: 0.06741640090942383 loss: 0.2266654235124588  lr: 0.000995372017803224 \n",
      "Epoch: 9400, acc: 0.95, data_loss: 0.06711833572387696 loss: 0.22559279596805573  lr: 0.0009953224819958604 \n",
      "Epoch: 9500, acc: 0.95, data_loss: 0.06680927848815918 loss: 0.22453027641773224  lr: 0.000995272951118662 \n",
      "Epoch: 9600, acc: 0.95, data_loss: 0.06649695205688477 loss: 0.22348263728618623  lr: 0.0009952234251708924 \n",
      "Epoch: 9700, acc: 0.945, data_loss: 0.06618538284301757 loss: 0.22245445632934568  lr: 0.000995173904151816 \n",
      "Epoch: 9800, acc: 0.945, data_loss: 0.06587498092651367 loss: 0.22143967652320862  lr: 0.0009951243880606966 \n",
      "Epoch: 9900, acc: 0.95, data_loss: 0.06557100296020507 loss: 0.22044342041015624  lr: 0.0009950748768967994 \n",
      "Epoch: 10000, acc: 0.95, data_loss: 0.06527061367034911 loss: 0.21946926021575927  lr: 0.0009950253706593885 \n"
     ]
    }
   ],
   "source": [
    "X,y = spiral_data(samples=100, classes=2)\n",
    "\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "dense1 = Layer_Dense(2, 64, weight_regularizer_l2=5e-4, bias_regularizer_l2=5e-4)\n",
    "\n",
    "activation1 = Activation_Relu()\n",
    "\n",
    "dense2 = Layer_Dense(64, 1)\n",
    "\n",
    "activation2 = Activation_Sigmoid()\n",
    "\n",
    "loss_function = Loss_BinaryCrossentropy()\n",
    "\n",
    "\n",
    "optimizer = Optimizer_Adam(decay=5e-7)\n",
    "\n",
    "for epoch in range(10001):\n",
    "    \n",
    "    # performing forward pass in the dense layer 1 with the data X\n",
    "    dense1.forward(X)\n",
    "    \n",
    "    # performing forward pass in the activation1 with the output from dense layer1 \n",
    "    activation1.forward(dense1.output)\n",
    "    \n",
    "    # performing forward pass in dense layer 2 with output from activation1\n",
    "    dense2.forward(activation1.output)\n",
    "    \n",
    "    # performing forward pass in activation2 with output from dense2\n",
    "    activation2.forward(dense2.output)\n",
    "    \n",
    "    # calculate the loss\n",
    "    data_loss = loss_function.calculate(activation2.output, y)\n",
    "    \n",
    "    regularization_loss = loss_function.regularization_loss(dense1) + loss_function.regularization_loss(dense2)\n",
    "    \n",
    "    loss = data_loss + regularization_loss\n",
    "    \n",
    "    predictions = (activation2.output > 0.5) * 1\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f'Epoch: {epoch}, ' + f'acc: {accuracy}, '+ f\"data_loss: {regularization_loss} \" + f'loss: {loss} ', f'lr: {optimizer.current_learning_rate} ')\n",
    "    \n",
    "    #----------------------Backwardpass-------------------------------------\n",
    "    \n",
    "    loss_function.backward(activation2.output, y)\n",
    "    \n",
    "    activation2.backward(loss_function.dinputs)\n",
    "    \n",
    "    dense2.backward(activation2.dinputs)\n",
    "    \n",
    "    activation1.backward(dense2.dinputs)\n",
    "    \n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # updating weights and biases\n",
    "    \n",
    "    optimizer.pre_update_parameters()\n",
    "    \n",
    "    optimizer.update_params(dense1)\n",
    "    \n",
    "    optimizer.update_params(dense2)\n",
    "    \n",
    "    optimizer.post_update_parameters()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b39931",
   "metadata": {},
   "source": [
    "# Validating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79861d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation, acc: 0.895, loss: 0.2775419354438782\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = spiral_data(samples=100, classes=2)\n",
    "\n",
    "y_test = y_test.reshape(-1,1)\n",
    "\n",
    "dense1.forward(X_test)\n",
    "\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "loss = loss_function.calculate(activation2.output, y_test)\n",
    "\n",
    "predictions = (activation2.output > 0.5) * 1\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "\n",
    "print(f\"validation, acc: {accuracy}, loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c01141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b1058b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8938e5b2",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7059a418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nnfs\n",
    "from nnfs.datasets import sine_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "X,y = sine_data()\n",
    "\n",
    "plt.plot(X,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660c8d3a",
   "metadata": {},
   "source": [
    "# Linear Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94176cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Linear:\n",
    "    \n",
    "    # forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = inputs\n",
    "        \n",
    "    # backward pass\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b51d719",
   "metadata": {},
   "source": [
    "# MSE & MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10223e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import matplotlib.pyplot as plt\n",
    "from nnfs.datasets import sine_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "class Layer_Dense:\n",
    "    \n",
    "    # getting the number of inputs and neurons\n",
    "    def __init__(self, n_inputs, n_neurons,\n",
    "                 weight_regularizer_l1=0,\n",
    "                bias_regularizer_l1=0,\n",
    "                weight_regularizer_l2=0,\n",
    "                bias_regularizer_l2=0):\n",
    "        \n",
    "        #initializing random weights in range \n",
    "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "        self.weight_regularizer_l1= weight_regularizer_l1\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        \n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "        \n",
    "    # getting the output with (mx +b)\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    \n",
    "    # backward pass which takes the derivative values from the next function \n",
    "    def backward(self, dvalues):\n",
    "        # getting the partial derivative with the inputs\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims = True)\n",
    "        \n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dl1 = np.ones_like(self.weights)\n",
    "            dl1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dl1\n",
    "            \n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "            \n",
    "        if self.bias_regularizer_l1 >0:\n",
    "            dl1 = np.ones_like(self.biases)\n",
    "            dl1[self.biases < 0] = -1\n",
    "            self.biases += self.bias_regularizer_l1 * dl1\n",
    "            \n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
    "        \n",
    "        \n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "        \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Activation_Relu:\n",
    "    \n",
    "    # The method which takes the input\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        self.inputs = inputs\n",
    "        # returns output with the maximum of 0 and the input value\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <=0] = 0\n",
    "        \n",
    "#---------------------------------------------------------------------\n",
    "        \n",
    "# Creating an Activation function class\n",
    "class Activation_Softmax:\n",
    "    \n",
    "    # The method which takes the input\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        \n",
    "        norm_vals = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        \n",
    "        self.output = norm_vals\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        \n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "                \n",
    "                single_output = single_output.reshape(-1,1)\n",
    "                \n",
    "                jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "                \n",
    "                self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "                \n",
    "#---------------------------------------------------------------------\n",
    "                \n",
    "class Activation_Linear:\n",
    "    \n",
    "    # forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = inputs\n",
    "        \n",
    "    # backward pass\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "#---------------------------------------------------------------------\n",
    "                \n",
    "class Loss:\n",
    "    \n",
    "    def regularization_loss(self, layer):\n",
    "        \n",
    "        regularization_loss = 0\n",
    "        \n",
    "        #------------------L1 Regularization------------------------\n",
    "\n",
    "        \n",
    "        if layer.weight_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l1 * np.sum(abs(layer.weights))\n",
    "            \n",
    "        if layer.bias_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l1 * np.sum(abs(layer.biases))\n",
    "        \n",
    "        #------------------L2 Regularization------------------------\n",
    "        \n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
    "            \n",
    "        if layer.bias_regularizer_l2 > 0 :\n",
    "            regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
    "            \n",
    "        return regularization_loss\n",
    "    \n",
    "        \n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        \n",
    "        return data_loss\n",
    "    \n",
    "#---------------------------------------------------------------------\n",
    "    \n",
    "class Loss_MeanSquaredError(Loss):\n",
    "    \n",
    "    # forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        sample_losses = np.mean((y_true - y_pred)**2, axis=-1)\n",
    "        \n",
    "        return sample_losses\n",
    "    \n",
    "    # backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        # gradient on values\n",
    "        self.dinputs = -2 * (y_true - dvalues) / outputs\n",
    "        \n",
    "        # normalizing gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "        \n",
    "class Loss_MeanAbsoluteError(Loss):\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        sample_losses = np.mean(np.abs(y_true - y_pred), axis=-1)\n",
    "        \n",
    "        return sample_losses\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        self.dinputs = np.sign(y_true - dvalues)/ outputs\n",
    "        \n",
    "        self.dinputs = self.dinputs / samples\n",
    "         \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "    \n",
    "    def forward(self, y_pred, y_target):\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        # clipping the values so we don't end up with inf error\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "\n",
    "        # checking if 1D array so we just need to get that index value\n",
    "        if len(y_target.shape) == 1:\n",
    "            correct = y_pred_clipped[range(samples), y_target]\n",
    "        # checking if one-hot-encoded values, then multiplying and summing up\n",
    "        elif len(y_target.shape) == 2:\n",
    "            correct = np.sum(y_pred_clipped * y_target, axis=1)\n",
    "        \n",
    "        negative_log = -np.log(correct)\n",
    "        \n",
    "        return negative_log\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "        \n",
    "        if len(y_true.shape) ==1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "            \n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples\n",
    "    \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Accuracy:\n",
    "    def get_accuracy(self,y_pred, y_true):\n",
    "        \n",
    "        predictions = np.argmax(y_pred, axis=1)\n",
    "                \n",
    "        if len(y_true.shape) == 2:\n",
    "            actual = np.argmax(y_true, axis=1)\n",
    "        else:\n",
    "            actual = np.argmax(y_true)\n",
    "        \n",
    "        accuracy = np.mean(predictions == y)\n",
    "        \n",
    "        print(\"accuracy : \", accuracy)\n",
    "\n",
    "    \n",
    "        \n",
    "class Accuracy_Categorical(Accuracy):\n",
    "    \n",
    "    def __init__(self, y):\n",
    "        pass\n",
    "    \n",
    "    def compare(self, predictions, y):\n",
    "        if len(y.shape) == 2:\n",
    "            y = np.argmax(y, axis=1)\n",
    "            \n",
    "        return predictions == y\n",
    "\n",
    "#.......................................................................\n",
    "\n",
    "class Activation_Softmax_Loss_CategoricalCrossEntropy():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # initializing the activation function\n",
    "        self.activation = Activation_Softmax()\n",
    "        \n",
    "        # initializing the loss function\n",
    "        self.loss = Loss_CategoricalCrossEntropy()\n",
    "    \n",
    "    # forward pass method, takes the input from \n",
    "    # the dense layer and the actual value\n",
    "    def forward(self, inputs, y_true):\n",
    "        \n",
    "        # forward passing the inputs in the activation function\n",
    "        self.activation.forward(inputs)\n",
    "        \n",
    "        # getting the output from the activation function\n",
    "        self.output = self.activation.output\n",
    "        \n",
    "        # calculating loss with the activation function output and actual value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        # checking if the values are one-hot-encoded\n",
    "        if len(y_true.shape) ==2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "            \n",
    "        \n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "        # calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -=1\n",
    "        \n",
    "        # normalizing gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "    \n",
    "#---------------------------------------------------------------------\n",
    "    \n",
    "class Activation_Sigmoid:\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * (1 - self.output) * self.output\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Loss_BinaryCrossentropy(Loss):\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        sample_losses = -(y_true * np.log(y_pred_clipped) + (1-y_true) * np.log(1- y_pred_clipped))\n",
    "        \n",
    "        sample_losses = np.mean(sample_losses, axis = -1)\n",
    "        \n",
    "        return sample_losses\n",
    "    \n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        self.dinputs = -(y_true / clipped_dvalues - (1-y_true) / (1-clipped_dvalues))/ outputs\n",
    "        \n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "        \n",
    "class Optimizer_SGD:\n",
    "    \n",
    "    # setting learning rate default as 1.\n",
    "    def __init__(self, learning_rate = 1.0, decay=0., momentum =0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def pre_update_parameters(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "        \n",
    "    def update_params(self, layer):\n",
    "        \n",
    "        # if we use momentum\n",
    "        if self.momentum:\n",
    "            # if the layer does not contain momentum arrays\n",
    "            if not hasattr(layer, \"weight_momentums\"):\n",
    "                # creating weight_momentums and bias_momentums arrays initialized with zero\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            \n",
    "            # getting the weight updates with momentum - take previous factors\n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "            \n",
    "            # getting bias updates\n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "        \n",
    "        else:\n",
    "\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates =  -self.current_learning_rate * layer.dbiases\n",
    "            \n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "        \n",
    "    def post_update_parameters(self):\n",
    "        self.iterations +=1\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "        \n",
    "class Optimizer_AdaGrad:\n",
    "    \n",
    "    # setting learning rate default as 1.\n",
    "    def __init__(self, learning_rate = 1.0, decay=0., epsilon =1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def pre_update_parameters(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "        \n",
    "    def update_params(self, layer):\n",
    "        # checking if layer has attribute weight_cache\n",
    "        if not hasattr(layer, \"weight_cache\"):\n",
    "            # creating weight_cache and bias_cache\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        # getting the square of the gradients\n",
    "        layer.weight_cache += layer.dweights **2\n",
    "        layer.bias_cache += layer.dbiases **2\n",
    "        \n",
    "        # calculating the result and adding it to the weights and biases\n",
    "        layer.weights += - self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += - self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "        \n",
    "    def post_update_parameters(self):\n",
    "        self.iterations +=1\n",
    "                \n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class Optimizer_Adam:\n",
    "    \n",
    "    def __init__(self, learning_rate = 0.001, decay=0. ,\n",
    "                 epsilon=1e-7, beta_1=0.9, beta_2 = 0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        \n",
    "    def pre_update_parameters(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "        \n",
    "    def update_params(self, layer):\n",
    "        # checking if layer has attribute weight_cache\n",
    "        if not hasattr(layer, \"weight_cache\"):\n",
    "            \n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            \n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        \n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1- self.beta_1) * layer.dweights\n",
    "        \n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1- self.beta_1) * layer.dbiases\n",
    "        \n",
    "        \n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        \n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "\n",
    "        \n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights **2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases **2\n",
    "        \n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations +1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations +1))\n",
    "        \n",
    "        \n",
    "        layer.weights += - self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected)+ self.epsilon)\n",
    "        layer.biases += - self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected)+ self.epsilon)\n",
    "\n",
    "        \n",
    "    def post_update_parameters(self):\n",
    "        self.iterations +=1\n",
    "        \n",
    "        \n",
    "        \n",
    "class Layer_Dropout:\n",
    "    \n",
    "    def __init__(self, rate):\n",
    "        self.rate = 1- rate\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.intputs = inputs\n",
    "        \n",
    "        self.binary_mask = np.random.binomial(1, self.rate, size=inputs.shape) / self.rate\n",
    "        \n",
    "        self.output = inputs * self.binary_mask\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * self.binary_mask\n",
    "    \n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2203a6cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, acc: 0.003, loss: 0.49692495421339256 lr: 0.005 \n",
      "Epoch: 100, acc: 0.008, loss: 0.05490867255256923 lr: 0.004549590536851684 \n",
      "Epoch: 200, acc: 0.368, loss: 0.0013410961386431449 lr: 0.004170141784820684 \n",
      "Epoch: 300, acc: 0.561, loss: 0.0002489867241516661 lr: 0.003849114703618168 \n",
      "Epoch: 400, acc: 0.022, loss: 0.00025897337474862494 lr: 0.0035739814152966403 \n",
      "Epoch: 500, acc: 0.767, loss: 4.882750568873712e-05 lr: 0.00333555703802535 \n",
      "Epoch: 600, acc: 0.036, loss: 0.00012482269734581265 lr: 0.0031269543464665416 \n",
      "Epoch: 700, acc: 0.817, loss: 1.946893968806166e-05 lr: 0.002942907592701589 \n",
      "Epoch: 800, acc: 0.831, loss: 1.3297329875599596e-05 lr: 0.0027793218454697055 \n",
      "Epoch: 900, acc: 0.047, loss: 8.137744639664829e-05 lr: 0.0026329647182727752 \n",
      "Epoch: 1000, acc: 0.852, loss: 7.709966585565008e-06 lr: 0.002501250625312656 \n",
      "Epoch: 1100, acc: 0.863, loss: 5.989764906374923e-06 lr: 0.0023820867079561697 \n",
      "Epoch: 1200, acc: 0.86, loss: 6.028861971693765e-06 lr: 0.002273760800363802 \n",
      "Epoch: 1300, acc: 0.881, loss: 4.168962715282533e-06 lr: 0.002174858634188778 \n",
      "Epoch: 1400, acc: 0.655, loss: 8.964639139870233e-06 lr: 0.0020842017507294707 \n",
      "Epoch: 1500, acc: 0.893, loss: 3.187105052720835e-06 lr: 0.0020008003201280513 \n",
      "Epoch: 1600, acc: 0.904, loss: 2.6904894107204306e-06 lr: 0.001923816852635629 \n",
      "Epoch: 1700, acc: 0.87, loss: 4.513355726602892e-06 lr: 0.001852537977028529 \n",
      "Epoch: 1800, acc: 0.918, loss: 2.1918488238218457e-06 lr: 0.0017863522686673815 \n",
      "Epoch: 1900, acc: 0.065, loss: 5.5571917152539645e-05 lr: 0.0017247326664367024 \n",
      "Epoch: 2000, acc: 0.937, loss: 1.850813242222877e-06 lr: 0.0016672224074691564 \n",
      "Epoch: 2100, acc: 0.949, loss: 1.6646337767776968e-06 lr: 0.0016134236850596968 \n",
      "Epoch: 2200, acc: 0.944, loss: 2.0561977120123163e-06 lr: 0.0015629884338855893 \n",
      "Epoch: 2300, acc: 0.958, loss: 1.4167437944356131e-06 lr: 0.0015156107911488332 \n",
      "Epoch: 2400, acc: 0.016, loss: 0.00012294073132681233 lr: 0.0014710208884966167 \n",
      "Epoch: 2500, acc: 0.967, loss: 1.276430386490622e-06 lr: 0.0014289797084881396 \n",
      "Epoch: 2600, acc: 0.97, loss: 1.1831912812491052e-06 lr: 0.001389274798555154 \n",
      "Epoch: 2700, acc: 0.305, loss: 1.4869494180930233e-05 lr: 0.0013517166801838335 \n",
      "Epoch: 2800, acc: 0.972, loss: 1.0746565377246056e-06 lr: 0.0013161358252171624 \n",
      "Epoch: 2900, acc: 0.967, loss: 1.213427557940583e-06 lr: 0.0012823800974608873 \n",
      "Epoch: 3000, acc: 0.975, loss: 1.022526255437429e-06 lr: 0.0012503125781445363 \n",
      "Epoch: 3100, acc: 0.974, loss: 9.402009095136366e-07 lr: 0.0012198097096852891 \n",
      "Epoch: 3200, acc: 0.885, loss: 4.410626450586727e-06 lr: 0.0011907597046915933 \n",
      "Epoch: 3300, acc: 0.976, loss: 8.792243948480543e-07 lr: 0.0011630611770179114 \n",
      "Epoch: 3400, acc: 0.977, loss: 8.37188440595197e-07 lr: 0.0011366219595362584 \n",
      "Epoch: 3500, acc: 0.57, loss: 8.81859025775301e-06 lr: 0.0011113580795732384 \n",
      "Epoch: 3600, acc: 0.979, loss: 7.935454008298983e-07 lr: 0.0010871928680147858 \n",
      "Epoch: 3700, acc: 0.98, loss: 7.623837003267461e-07 lr: 0.0010640561821664183 \n",
      "Epoch: 3800, acc: 0.979, loss: 7.630719756324281e-07 lr: 0.0010418837257762034 \n",
      "Epoch: 3900, acc: 0.981, loss: 7.271452434247051e-07 lr: 0.0010206164523372118 \n",
      "Epoch: 4000, acc: 0.536, loss: 8.497331726868268e-06 lr: 0.0010002000400080014 \n",
      "Epoch: 4100, acc: 0.983, loss: 6.970911942647518e-07 lr: 0.0009805844283192783 \n",
      "Epoch: 4200, acc: 0.98, loss: 1.6893276043241945e-06 lr: 0.0009617234083477593 \n",
      "Epoch: 4300, acc: 0.987, loss: 6.721939083934961e-07 lr: 0.0009435742592942063 \n",
      "Epoch: 4400, acc: 0.988, loss: 6.53695051998007e-07 lr: 0.0009260974254491572 \n",
      "Epoch: 4500, acc: 0.978, loss: 9.733248150372906e-07 lr: 0.0009092562284051646 \n",
      "Epoch: 4600, acc: 0.987, loss: 6.318527027825477e-07 lr: 0.000893016610108948 \n",
      "Epoch: 4700, acc: 0.994, loss: 1.1176805627945776e-06 lr: 0.0008773469029654326 \n",
      "Epoch: 4800, acc: 0.989, loss: 6.137520319966985e-07 lr: 0.000862217623728229 \n",
      "Epoch: 4900, acc: 0.032, loss: 5.3215035073623975e-05 lr: 0.0008476012883539582 \n",
      "Epoch: 5000, acc: 0.989, loss: 5.988380833499299e-07 lr: 0.0008334722453742291 \n",
      "Epoch: 5100, acc: 0.99, loss: 5.860242240979312e-07 lr: 0.0008198065256599442 \n",
      "Epoch: 5200, acc: 0.996, loss: 8.350901682620867e-07 lr: 0.0008065817067268914 \n",
      "Epoch: 5300, acc: 0.99, loss: 5.734828976072036e-07 lr: 0.0007937767899666614 \n",
      "Epoch: 5400, acc: 0.052, loss: 5.563413461497059e-05 lr: 0.0007813720893889669 \n",
      "Epoch: 5500, acc: 0.99, loss: 5.638717373669085e-07 lr: 0.0007693491306354824 \n",
      "Epoch: 5600, acc: 0.991, loss: 5.538338719659286e-07 lr: 0.0007576905591756327 \n",
      "Epoch: 5700, acc: 0.992, loss: 5.618145996556973e-07 lr: 0.0007463800567248844 \n",
      "Epoch: 5800, acc: 0.991, loss: 5.443552644579269e-07 lr: 0.0007354022650389764 \n",
      "Epoch: 5900, acc: 0.988, loss: 5.79884806748933e-07 lr: 0.0007247427163357008 \n",
      "Epoch: 6000, acc: 0.992, loss: 5.372242848853468e-07 lr: 0.000714387769681383 \n",
      "Epoch: 6100, acc: 0.992, loss: 5.279095538917195e-07 lr: 0.0007043245527539089 \n",
      "Epoch: 6200, acc: 0.121, loss: 2.5565864357212103e-05 lr: 0.0006945409084595084 \n",
      "Epoch: 6300, acc: 0.993, loss: 5.209222709679279e-07 lr: 0.0006850253459377996 \n",
      "Epoch: 6400, acc: 0.993, loss: 5.135530295386853e-07 lr: 0.0006757669955399379 \n",
      "Epoch: 6500, acc: 0.98, loss: 1.0685394749958849e-06 lr: 0.0006667555674089878 \n",
      "Epoch: 6600, acc: 0.992, loss: 5.062459815326753e-07 lr: 0.0006579813133307014 \n",
      "Epoch: 6700, acc: 0.02, loss: 8.421638440720355e-05 lr: 0.0006494349915573451 \n",
      "Epoch: 6800, acc: 0.994, loss: 5.023624527834159e-07 lr: 0.0006411078343377356 \n",
      "Epoch: 6900, acc: 0.992, loss: 4.947029260555613e-07 lr: 0.00063299151791366 \n",
      "Epoch: 7000, acc: 0.998, loss: 7.560003411460581e-07 lr: 0.0006250781347668457 \n",
      "Epoch: 7100, acc: 0.993, loss: 4.882473219673056e-07 lr: 0.0006173601679219657 \n",
      "Epoch: 7200, acc: 0.986, loss: 8.100461835187625e-07 lr: 0.0006098304671301379 \n",
      "Epoch: 7300, acc: 0.995, loss: 4.932325255500772e-07 lr: 0.0006024822267743102 \n",
      "Epoch: 7400, acc: 0.995, loss: 4.778367838766372e-07 lr: 0.0005953089653530181 \n",
      "Epoch: 7500, acc: 0.995, loss: 4.836382674452505e-07 lr: 0.000588304506412519 \n",
      "Epoch: 7600, acc: 0.995, loss: 4.727374871651525e-07 lr: 0.0005814629608093965 \n",
      "Epoch: 7700, acc: 0.691, loss: 6.217441512318687e-06 lr: 0.0005747787101965744 \n",
      "Epoch: 7800, acc: 0.995, loss: 4.680894642538246e-07 lr: 0.0005682463916354131 \n",
      "Epoch: 7900, acc: 0.995, loss: 4.6460067191392464e-07 lr: 0.0005618608832453085 \n",
      "Epoch: 8000, acc: 0.998, loss: 5.508394387816038e-07 lr: 0.00055561729081009 \n",
      "Epoch: 8100, acc: 0.995, loss: 4.5933373201239305e-07 lr: 0.0005495109352676119 \n",
      "Epoch: 8200, acc: 0.412, loss: 1.145808347920282e-05 lr: 0.0005435373410153278 \n",
      "Epoch: 8300, acc: 0.995, loss: 4.559247754591868e-07 lr: 0.0005376922249704269 \n",
      "Epoch: 8400, acc: 0.995, loss: 4.522282009465871e-07 lr: 0.0005319714863283328 \n",
      "Epoch: 8500, acc: 0.966, loss: 2.9562536842920634e-06 lr: 0.0005263711969681019 \n",
      "Epoch: 8600, acc: 0.995, loss: 4.479900043319095e-07 lr: 0.0005208875924575476 \n",
      "Epoch: 8700, acc: 1.0, loss: 1.0134195157149835e-06 lr: 0.0005155170636148056 \n",
      "Epoch: 8800, acc: 0.995, loss: 4.441545054387052e-07 lr: 0.0005102561485865905 \n",
      "Epoch: 8900, acc: 0.865, loss: 4.213757145170771e-06 lr: 0.0005051015254066068 \n",
      "Epoch: 9000, acc: 0.995, loss: 4.435584279448827e-07 lr: 0.0005000500050005 \n",
      "Epoch: 9100, acc: 0.995, loss: 4.368714114168923e-07 lr: 0.0004950985246063966 \n",
      "Epoch: 9200, acc: 0.992, loss: 5.053095849946317e-07 lr: 0.0004902441415825081 \n",
      "Epoch: 9300, acc: 0.995, loss: 4.338035172134343e-07 lr: 0.0004854840275754928 \n",
      "Epoch: 9400, acc: 0.995, loss: 4.3802325258840355e-07 lr: 0.0004808154630252909 \n",
      "Epoch: 9500, acc: 0.994, loss: 4.353633905494823e-07 lr: 0.00047623583198399844 \n",
      "Epoch: 9600, acc: 0.994, loss: 4.279448435432333e-07 lr: 0.00047174261722804036 \n",
      "Epoch: 9700, acc: 0.979, loss: 1.8019496414142426e-06 lr: 0.00046733339564445275 \n",
      "Epoch: 9800, acc: 0.994, loss: 4.253183642527087e-07 lr: 0.00046300583387350687 \n",
      "Epoch: 9900, acc: 0.994, loss: 4.2289127244677854e-07 lr: 0.00045875768419121016 \n",
      "Epoch: 10000, acc: 0.999, loss: 1.2327593996717573e-06 lr: 0.00045458678061641964 \n"
     ]
    }
   ],
   "source": [
    "X, y = sine_data()\n",
    "\n",
    "dense1 = Layer_Dense(1, 64)\n",
    "\n",
    "activation1 = Activation_Relu()\n",
    "\n",
    "dense2 = Layer_Dense(64, 64)\n",
    "\n",
    "activation2 = Activation_Relu()\n",
    "\n",
    "dense3 = Layer_Dense(64, 1)\n",
    "\n",
    "activation3 = Activation_Linear()\n",
    "\n",
    "loss_function = Loss_MeanSquaredError()\n",
    "\n",
    "optimizer = Optimizer_Adam(learning_rate=0.005, decay=1e-3)\n",
    "\n",
    "accuracy_precision = np.std(y) / 250\n",
    "\n",
    "\n",
    "for epoch in range(10001):\n",
    "    \n",
    "    # Forward pass\n",
    "    \n",
    "    # forwarding input to the dense1 layer\n",
    "    dense1.forward(X)\n",
    "    \n",
    "    # forwarding the dense1 layer output to the activation1 function\n",
    "    activation1.forward(dense1.output)\n",
    "    \n",
    "    # forwarding the activation1 output to the dense2 layer\n",
    "    dense2.forward(activation1.output)\n",
    "    \n",
    "    # forwarding the dense2 output to the activation2 funciton\n",
    "    activation2.forward(dense2.output)\n",
    "    \n",
    "    # forwarding the activation2 output to the dense3 layer\n",
    "    dense3.forward(activation2.output)\n",
    "    \n",
    "    # forwarding the dense3 layer output to the activation3 function\n",
    "    activation3.forward(dense3.output)\n",
    "    \n",
    "    # getting the data loss\n",
    "    data_loss = loss_function.calculate(activation3.output, y)\n",
    "    \n",
    "    regularization_loss = loss_function.regularization_loss(dense1) + loss_function.regularization_loss(dense2) + loss_function.regularization_loss(dense3)\n",
    "    \n",
    "    loss = data_loss + regularization_loss\n",
    "    \n",
    "    predictions = activation3.output\n",
    "    \n",
    "    accuracy = np.mean(np.absolute(predictions - y) < accuracy_precision)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f'Epoch: {epoch}, ' + f'acc: {accuracy}, '+ f'loss: {loss}', f'lr: {optimizer.current_learning_rate} ')\n",
    "    \n",
    "    # Backward Pass\n",
    "    \n",
    "    # getting the gradient from the loss function\n",
    "    loss_function.backward(activation3.output, y)\n",
    "    \n",
    "    # passing the gradients from loss function to the activation3 backward function\n",
    "    activation3.backward(loss_function.dinputs)\n",
    "    \n",
    "    # passing the activation3 gradient to the dense3 backward function\n",
    "    dense3.backward(activation3.dinputs)\n",
    "    \n",
    "    # passing the gradient from the dense3 layer to from activation2 function\n",
    "    activation2.backward(dense3.dinputs)\n",
    "    \n",
    "    # passing the gradient of activation2 function and generating the gradient from dense2 layer\n",
    "    dense2.backward(activation2.dinputs)\n",
    "    \n",
    "    # passing the gradient of dense2 layer to the activation1 function and generating the gradient \n",
    "    activation1.backward(dense2.dinputs)\n",
    "    \n",
    "    # passing the gradient from activation1 and getting gradient from dense1 layer\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    \n",
    "    # Update weights and biases with optimizer\n",
    "    optimizer.pre_update_parameters()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.update_params(dense3)\n",
    "    optimizer.post_update_parameters()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dce510bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_test, y_test = sine_data()\n",
    "\n",
    "dense1.forward(X_test)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "dense3.forward(activation2.output)\n",
    "activation3.forward(dense3.output)\n",
    "\n",
    "plt.plot(X_test, y_test)\n",
    "plt.plot(X_test, activation3.output)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486cae9e",
   "metadata": {},
   "source": [
    "# Model Object Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "306d01db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import matplotlib.pyplot as plt\n",
    "from nnfs.datasets import sine_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "class Layer_Dense:\n",
    "    \n",
    "    # getting the number of inputs and neurons\n",
    "    def __init__(self, n_inputs, n_neurons,\n",
    "                 weight_regularizer_l1=0,\n",
    "                bias_regularizer_l1=0,\n",
    "                weight_regularizer_l2=0,\n",
    "                bias_regularizer_l2=0):\n",
    "        \n",
    "        #initializing random weights in range \n",
    "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "        self.weight_regularizer_l1= weight_regularizer_l1\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        \n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "        \n",
    "    # getting the output with (mx +b)\n",
    "    def forward(self, inputs, training):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    \n",
    "    # backward pass which takes the derivative values from the next function \n",
    "    def backward(self, dvalues):\n",
    "        # getting the partial derivative with the inputs\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims = True)\n",
    "        \n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dl1 = np.ones_like(self.weights)\n",
    "            dl1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dl1\n",
    "            \n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "            \n",
    "        if self.bias_regularizer_l1 >0:\n",
    "            dl1 = np.ones_like(self.biases)\n",
    "            dl1[self.biases < 0] = -1\n",
    "            self.biases += self.bias_regularizer_l1 * dl1\n",
    "            \n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
    "        \n",
    "        \n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "        \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Activation_Relu:\n",
    "    \n",
    "    # The method which takes the input\n",
    "    def forward(self, inputs, training):\n",
    "        \n",
    "        self.inputs = inputs\n",
    "        # returns output with the maximum of 0 and the input value\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <=0] = 0\n",
    "        \n",
    "    def predictions(self, outputs):\n",
    "        return outputs\n",
    "        \n",
    "#---------------------------------------------------------------------\n",
    "        \n",
    "# Creating an Activation function class\n",
    "class Activation_Softmax:\n",
    "    \n",
    "    # The method which takes the input\n",
    "    def forward(self, inputs, training):\n",
    "        self.inputs = inputs\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        \n",
    "        norm_vals = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        \n",
    "        self.output = norm_vals\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        \n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "                \n",
    "                single_output = single_output.reshape(-1,1)\n",
    "                \n",
    "                jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "                \n",
    "                self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "                \n",
    "    def predictions(self, outputs):\n",
    "        return np.argmax(outputs, axis=1)\n",
    "                \n",
    "#---------------------------------------------------------------------\n",
    "                \n",
    "class Activation_Linear:\n",
    "    \n",
    "    # forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        self.inputs = inputs\n",
    "        self.output = inputs\n",
    "        \n",
    "    # backward pass\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "    def predictions(self, outputs):\n",
    "        return outputs\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "    \n",
    "class Activation_Sigmoid:\n",
    "    \n",
    "    def forward(self, inputs, training):\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * (1 - self.output) * self.output\n",
    "        \n",
    "    def predictions(self, outputs):\n",
    "        return (outputs > 0.5) * 1\n",
    "    \n",
    "#---------------------------------------------------------------------\n",
    "                \n",
    "class Loss:\n",
    "    \n",
    "    def regularization_loss(self):\n",
    "        \n",
    "        regularization_loss = 0\n",
    "        \n",
    "        #------------------L1 Regularization------------------------\n",
    "\n",
    "        \n",
    "        for layer in self.trainable_layers:\n",
    "            \n",
    "            if layer.weight_regularizer_l1 > 0:\n",
    "                regularization_loss += layer.weight_regularizer_l1 * np.sum(abs(layer.weights))\n",
    "\n",
    "            if layer.bias_regularizer_l1 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l1 * np.sum(abs(layer.biases))\n",
    "\n",
    "            #------------------L2 Regularization------------------------\n",
    "\n",
    "            if layer.weight_regularizer_l2 > 0:\n",
    "                regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
    "\n",
    "            if layer.bias_regularizer_l2 > 0 :\n",
    "                regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
    "\n",
    "        return regularization_loss\n",
    "    \n",
    "    # set / remember trainable layers\n",
    "    def remember_trainable_layers(self, trainable_layers):\n",
    "        self.trainable_layers = trainable_layers\n",
    "        \n",
    "        \n",
    "    def calculate(self, output, y, *, include_regularization= False):\n",
    "        \n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        \n",
    "        if not include_regularization:\n",
    "            return data_loss\n",
    "        \n",
    "        return data_loss, self.regularization_loss()\n",
    "    \n",
    "#---------------------------------------------------------------------\n",
    "    \n",
    "class Loss_MeanSquaredError(Loss):\n",
    "    \n",
    "    # forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        sample_losses = np.mean((y_true - y_pred)**2, axis=-1)\n",
    "        \n",
    "        return sample_losses\n",
    "    \n",
    "    # backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        # gradient on values\n",
    "        self.dinputs = -2 * (y_true - dvalues) / outputs\n",
    "        \n",
    "        # normalizing gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "        \n",
    "class Loss_MeanAbsoluteError(Loss):\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        sample_losses = np.mean(np.abs(y_true - y_pred), axis=-1)\n",
    "        \n",
    "        return sample_losses\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        self.dinputs = np.sign(y_true - dvalues)/ outputs\n",
    "        \n",
    "        self.dinputs = self.dinputs / samples\n",
    "         \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "    \n",
    "    def forward(self, y_pred, y_target):\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        # clipping the values so we don't end up with inf error\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "\n",
    "        # checking if 1D array so we just need to get that index value\n",
    "        if len(y_target.shape) == 1:\n",
    "            correct = y_pred_clipped[range(samples), y_target]\n",
    "        # checking if one-hot-encoded values, then multiplying and summing up\n",
    "        elif len(y_target.shape) == 2:\n",
    "            correct = np.sum(y_pred_clipped * y_target, axis=1)\n",
    "        \n",
    "        negative_log = -np.log(correct)\n",
    "        \n",
    "        return negative_log\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "        \n",
    "        if len(y_true.shape) ==1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "            \n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples\n",
    "    \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Accuracy:\n",
    "    def calculate(self, predictions, y):\n",
    "        comparisons = self.compare(predictions, y)\n",
    "        \n",
    "        accuracy = np.mean(comparisons)\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Accuracy_Regression(Accuracy):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.precision = None\n",
    "        \n",
    "    def init(self, y, reinit= False):\n",
    "        if self.precision is None or reinit:\n",
    "            self.precision = np.std(y) / 250\n",
    "            \n",
    "    def compare(self, predictions, y):\n",
    "        return np.absolute(predictions - y) < self.precision\n",
    "        \n",
    "\n",
    "        \n",
    "class Accuracy_Categorical(Accuracy):\n",
    "    \n",
    "    def init(self, y):\n",
    "        pass\n",
    "    \n",
    "    def compare(self, predictions, y):\n",
    "        if len(y.shape) ==2:\n",
    "            y = np.argmax(y, axis=1)\n",
    "            \n",
    "            \n",
    "        return predictions == y\n",
    "        \n",
    "\n",
    "#.......................................................................\n",
    "\n",
    "class Activation_Softmax_Loss_CategoricalCrossEntropy():\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        samples = len(dvalues)\n",
    "        \n",
    "#         # checking if the values are one-hot-encoded\n",
    "#         if len(y_true.shape) ==2:\n",
    "#             y_true = np.argmax(y_true, axis=1)\n",
    "            \n",
    "        \n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "        # calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -=1\n",
    "        \n",
    "        # normalizing gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "    \n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Loss_BinaryCrossentropy(Loss):\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        sample_losses = -(y_true * np.log(y_pred_clipped) + (1-y_true) * np.log(1- y_pred_clipped))\n",
    "        \n",
    "        sample_losses = np.mean(sample_losses, axis = -1)\n",
    "        \n",
    "        return sample_losses\n",
    "    \n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        self.dinputs = -(y_true / clipped_dvalues - (1-y_true) / (1-clipped_dvalues))/ outputs\n",
    "        \n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "        \n",
    "class Optimizer_SGD:\n",
    "    \n",
    "    # setting learning rate default as 1.\n",
    "    def __init__(self, learning_rate = 1.0, decay=0., momentum =0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def pre_update_parameters(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "        \n",
    "    def update_params(self, layer):\n",
    "        \n",
    "        # if we use momentum\n",
    "        if self.momentum:\n",
    "            # if the layer does not contain momentum arrays\n",
    "            if not hasattr(layer, \"weight_momentums\"):\n",
    "                # creating weight_momentums and bias_momentums arrays initialized with zero\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            \n",
    "            # getting the weight updates with momentum - take previous factors\n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "            \n",
    "            # getting bias updates\n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "        \n",
    "        else:\n",
    "\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates =  -self.current_learning_rate * layer.dbiases\n",
    "            \n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "        \n",
    "    def post_update_parameters(self):\n",
    "        self.iterations +=1\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "        \n",
    "class Optimizer_AdaGrad:\n",
    "    \n",
    "    # setting learning rate default as 1.\n",
    "    def __init__(self, learning_rate = 1.0, decay=0., epsilon =1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def pre_update_parameters(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "        \n",
    "    def update_params(self, layer):\n",
    "        # checking if layer has attribute weight_cache\n",
    "        if not hasattr(layer, \"weight_cache\"):\n",
    "            # creating weight_cache and bias_cache\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        # getting the square of the gradients\n",
    "        layer.weight_cache += layer.dweights **2\n",
    "        layer.bias_cache += layer.dbiases **2\n",
    "        \n",
    "        # calculating the result and adding it to the weights and biases\n",
    "        layer.weights += - self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += - self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "        \n",
    "    def post_update_parameters(self):\n",
    "        self.iterations +=1\n",
    "                \n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class Optimizer_Adam:\n",
    "    \n",
    "    def __init__(self, learning_rate = 0.001, decay=0. ,\n",
    "                 epsilon=1e-7, beta_1=0.9, beta_2 = 0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        \n",
    "    def pre_update_parameters(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "        \n",
    "    def update_params(self, layer):\n",
    "        # checking if layer has attribute weight_cache\n",
    "        if not hasattr(layer, \"weight_cache\"):\n",
    "            \n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            \n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        \n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1- self.beta_1) * layer.dweights\n",
    "        \n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1- self.beta_1) * layer.dbiases\n",
    "        \n",
    "        \n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        \n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "\n",
    "        \n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights **2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases **2\n",
    "        \n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations +1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations +1))\n",
    "        \n",
    "        \n",
    "        layer.weights += - self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected)+ self.epsilon)\n",
    "        layer.biases += - self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected)+ self.epsilon)\n",
    "\n",
    "        \n",
    "    def post_update_parameters(self):\n",
    "        self.iterations +=1\n",
    "        \n",
    "        \n",
    "        \n",
    "class Layer_Dropout:\n",
    "    \n",
    "    def __init__(self, rate):\n",
    "        self.rate = 1- rate\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs, training):\n",
    "        self.intputs = inputs\n",
    "        \n",
    "        if not training:\n",
    "            self.output = inputs.copy()\n",
    "            return\n",
    "        \n",
    "        self.binary_mask = np.random.binomial(1, self.rate, size=inputs.shape) / self.rate\n",
    "        \n",
    "        self.output = inputs * self.binary_mask\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * self.binary_mask\n",
    "        \n",
    "class Layer_Input:\n",
    "    \n",
    "    # forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        self.output = inputs\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658aca52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, acc: 0.20766666666666667, loss: 35.72122472572327 lr: 0.04999752512250644 \n",
      "Epoch: 200, acc: 0.15766666666666668, loss: 35.71379767417908 lr: 0.04999502549496326 \n",
      "Epoch: 300, acc: 0.24166666666666667, loss: 35.70944302630424 lr: 0.049992526117345455 \n",
      "Epoch: 400, acc: 0.30933333333333335, loss: 35.706693580150606 lr: 0.04999002698961558 \n",
      "Epoch: 500, acc: 0.3253333333333333, loss: 35.70508813250065 lr: 0.049987528111736124 \n",
      "Epoch: 600, acc: 0.15333333333333332, loss: 35.70322092071176 lr: 0.049985029483669646 \n",
      "Epoch: 700, acc: 0.7023333333333334, loss: 35.70212423917651 lr: 0.049982531105378675 \n",
      "Epoch: 800, acc: 0.10066666666666667, loss: 35.70539029940963 lr: 0.04998003297682575 \n",
      "Epoch: 900, acc: 0.357, loss: 35.70083937984705 lr: 0.049977535097973466 \n",
      "Epoch: 1000, acc: 0.3453333333333333, loss: 35.700016566742214 lr: 0.049975037468784345 \n",
      "Epoch: 1100, acc: 0.01633333333333333, loss: 35.70057478415966 lr: 0.049972540089220974 \n",
      "Epoch: 1200, acc: 0.029333333333333333, loss: 35.705648092746735 lr: 0.04997004295924593 \n",
      "Epoch: 1300, acc: 0.04766666666666667, loss: 35.70050464087725 lr: 0.04996754607882181 \n",
      "Epoch: 1400, acc: 0.094, loss: 35.70324807071686 lr: 0.049965049447911185 \n",
      "Epoch: 1500, acc: 0.04933333333333333, loss: 35.703191526412965 lr: 0.04996255306647668 \n",
      "Epoch: 1600, acc: 0.0013333333333333333, loss: 35.70046671956778 lr: 0.049960056934480884 \n",
      "Epoch: 1700, acc: 0.8343333333333334, loss: 35.699205489985644 lr: 0.04995756105188642 \n",
      "Epoch: 1800, acc: 0.042333333333333334, loss: 35.71611547446251 lr: 0.049955065418655915 \n",
      "Epoch: 1900, acc: 0.08133333333333333, loss: 35.703709831237795 lr: 0.04995257003475201 \n",
      "Epoch: 2000, acc: 0.08733333333333333, loss: 35.719374393463134 lr: 0.04995007490013731 \n",
      "Epoch: 2100, acc: 0.02033333333333333, loss: 36.03659429359436 lr: 0.0499475800147745 \n",
      "Epoch: 2200, acc: 0.12266666666666666, loss: 35.71518507575989 lr: 0.0499450853786262 \n",
      "Epoch: 2300, acc: 0.10933333333333334, loss: 35.707581708312034 lr: 0.0499425909916551 \n",
      "Epoch: 2400, acc: 0.0013333333333333333, loss: 35.70618558830023 lr: 0.04994009685382384 \n",
      "Epoch: 2500, acc: 0.0, loss: 35.7030596306324 lr: 0.04993760296509512 \n",
      "Epoch: 2600, acc: 0.4716666666666667, loss: 35.7049028699398 lr: 0.049935109325431604 \n",
      "Epoch: 2700, acc: 0.07366666666666667, loss: 35.71191547894478 lr: 0.049932615934796004 \n",
      "Epoch: 2800, acc: 0.0, loss: 35.70410738551617 lr: 0.04993012279315098 \n",
      "Epoch: 2900, acc: 0.026333333333333334, loss: 35.70916577100754 lr: 0.049927629900459285 \n",
      "Epoch: 3000, acc: 0.011666666666666667, loss: 35.72578129386902 lr: 0.049925137256683606 \n",
      "Epoch: 3100, acc: 0.0, loss: 35.712418094158174 lr: 0.04992264486178666 \n",
      "Epoch: 3200, acc: 0.0, loss: 35.709776943087576 lr: 0.04992015271573119 \n",
      "Epoch: 3300, acc: 0.8866666666666667, loss: 35.84022968292236 lr: 0.04991766081847992 \n",
      "Epoch: 3400, acc: 0.06066666666666667, loss: 35.7386881904602 lr: 0.049915169169995596 \n",
      "Epoch: 3500, acc: 0.708, loss: 35.72663889312744 lr: 0.049912677770240964 \n",
      "Epoch: 3600, acc: 0.999, loss: 35.71436210823059 lr: 0.049910186619178794 \n",
      "Epoch: 3700, acc: 0.0016666666666666668, loss: 35.709757124185565 lr: 0.04990769571677183 \n",
      "Epoch: 3800, acc: 0.033, loss: 35.70736532354355 lr: 0.04990520506298287 \n",
      "Epoch: 3900, acc: 1.0, loss: 35.70547312319279 lr: 0.04990271465777467 \n",
      "Epoch: 4000, acc: 0.027, loss: 35.70662524008751 lr: 0.049900224501110035 \n",
      "Epoch: 4100, acc: 0.663, loss: 35.71793045854569 lr: 0.04989773459295174 \n",
      "Epoch: 4200, acc: 0.07433333333333333, loss: 35.70599848848581 lr: 0.04989524493326262 \n",
      "Epoch: 4300, acc: 0.04133333333333333, loss: 35.710669752359394 lr: 0.04989275552200545 \n",
      "Epoch: 4400, acc: 0.407, loss: 35.72020962142944 lr: 0.04989026635914307 \n",
      "Epoch: 4500, acc: 0.09666666666666666, loss: 35.71056119823456 lr: 0.04988777744463829 \n",
      "Epoch: 4600, acc: 0.9023333333333333, loss: 35.70817272567749 lr: 0.049885288778453954 \n",
      "Epoch: 4700, acc: 0.15166666666666667, loss: 35.70397635805607 lr: 0.049882800360552884 \n",
      "Epoch: 4800, acc: 0.797, loss: 35.71111983299255 lr: 0.04988031219089794 \n",
      "Epoch: 4900, acc: 0.10966666666666666, loss: 35.70970258402824 lr: 0.049877824269451976 \n",
      "Epoch: 5000, acc: 0.8986666666666666, loss: 35.71684113264084 lr: 0.04987533659617785 \n",
      "Epoch: 5100, acc: 0.9983333333333333, loss: 35.72010581970215 lr: 0.04987284917103844 \n",
      "Epoch: 5200, acc: 0.97, loss: 35.71266076683998 lr: 0.04987036199399661 \n",
      "Epoch: 5300, acc: 0.952, loss: 35.75945906639099 lr: 0.04986787506501525 \n",
      "Epoch: 5400, acc: 0.015333333333333332, loss: 35.734754055023195 lr: 0.04986538838405724 \n",
      "Epoch: 5500, acc: 0.867, loss: 35.72211591720581 lr: 0.049862901951085496 \n",
      "Epoch: 5600, acc: 0.017666666666666667, loss: 35.71849226951599 lr: 0.049860415766062906 \n",
      "Epoch: 5700, acc: 0.983, loss: 35.71810293006897 lr: 0.0498579298289524 \n",
      "Epoch: 5800, acc: 0.10666666666666667, loss: 35.70956393420696 lr: 0.04985544413971689 \n",
      "Epoch: 5900, acc: 0.028, loss: 35.72347031211853 lr: 0.049852958698319315 \n",
      "Epoch: 6000, acc: 0.9666666666666667, loss: 35.725500914096834 lr: 0.04985047350472258 \n",
      "Epoch: 6100, acc: 0.5063333333333333, loss: 35.73148556041718 lr: 0.04984798855888967 \n",
      "Epoch: 6200, acc: 0.8693333333333333, loss: 35.716385787963866 lr: 0.049845503860783506 \n",
      "Epoch: 6300, acc: 0.194, loss: 35.7234309296608 lr: 0.049843019410367055 \n",
      "Epoch: 6400, acc: 0.8763333333333333, loss: 35.714972218990326 lr: 0.04984053520760327 \n",
      "Epoch: 6500, acc: 0.03866666666666667, loss: 35.731219291210174 lr: 0.049838051252455155 \n",
      "Epoch: 6600, acc: 0.10033333333333333, loss: 35.717932700157164 lr: 0.049835567544885655 \n",
      "Epoch: 6700, acc: 0.12933333333333333, loss: 35.73204277610779 lr: 0.04983308408485778 \n",
      "Epoch: 6800, acc: 0.837, loss: 35.72570913410187 lr: 0.0498306008723345 \n",
      "Epoch: 6900, acc: 0.30133333333333334, loss: 35.76924226093292 lr: 0.04982811790727884 \n"
     ]
    }
   ],
   "source": [
    "class Model:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # creating a list of network objects\n",
    "        self.layers = []\n",
    "        self.softmax_classifier_output = None\n",
    "        \n",
    "    # adding layers to the model\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    # set loss and optimizer\n",
    "    def set(self, * , loss, optimizer, accuracy):\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.accuracy = accuracy\n",
    "        \n",
    "    # Train the model\n",
    "    def train(self, X, y, *, epochs=1, print_every=1, validation_data = None):\n",
    "        \n",
    "        self.accuracy.init(y)\n",
    "        \n",
    "        for epoch in range(1, epochs+1):\n",
    "            \n",
    "            output = self.forward(X, training=True)\n",
    "            \n",
    "            data_loss, regularization_loss = self.loss.calculate(output, y, include_regularization=True)\n",
    "            \n",
    "            loss = data_loss + regularization_loss\n",
    "            \n",
    "            predictions = self.output_layer_activation.predictions(output)\n",
    "            \n",
    "            accuracy = self.accuracy.calculate(predictions, y)\n",
    "            \n",
    "            self.backward(output, y)\n",
    "            \n",
    "            self.optimizer.pre_update_parameters()\n",
    "            \n",
    "            for layer in self.trainable_layers:\n",
    "                self.optimizer.update_params(layer)\n",
    "                \n",
    "            self.optimizer.post_update_parameters()\n",
    "            \n",
    "            if not epoch % print_every:\n",
    "                print(f'Epoch: {epoch}, ' + f'acc: {accuracy}, '+ f'loss: {loss}', f'lr: {self.optimizer.current_learning_rate} ')\n",
    "        \n",
    "        \n",
    "        if validation_data is not None:\n",
    "            \n",
    "            X_val, y_val = validation_data\n",
    "            \n",
    "            output = self.forward(X_val,training= False)\n",
    "            \n",
    "            loss = self.loss.calculate(output, y_val)\n",
    "            \n",
    "            predictions = self.output_layer_activation.predictions(output)\n",
    "            \n",
    "            accuracy = self.accuracy.calculate(predictions, y_val)\n",
    "            \n",
    "            print(f\"validation || acc : {accuracy} , loss : {loss}\")\n",
    "        \n",
    "\n",
    "    # finalize the model\n",
    "    def finalize(self):\n",
    "        \n",
    "        #create and set the input layer\n",
    "        self.input_layer = Layer_Input()\n",
    "        \n",
    "        # count all objects\n",
    "        layer_count = len(self.layers)\n",
    "        \n",
    "        # initializing a list to store trainable layers\n",
    "        self.trainable_layers = []\n",
    "        \n",
    "        for i in range(layer_count):\n",
    "            \n",
    "            if i == 0:\n",
    "                self.layers[i].prev = self.input_layer\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "                \n",
    "            elif i < layer_count -1:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "                \n",
    "            else:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.loss\n",
    "                self.output_layer_activation = self.layers[i]\n",
    "                \n",
    "            # checking if the layer has an attribute weights\n",
    "            # then adding it to the trainable layers list\n",
    "            if hasattr(self.layers[i], \"weights\"):\n",
    "                self.trainable_layers.append(self.layers[i])\n",
    "                \n",
    "                \n",
    "            self.loss.remember_trainable_layers(self.trainable_layers)\n",
    "            \n",
    "        if isinstance(self.layers[-1], Activation_Softmax) and isinstance(self.loss, Loss_CategoricalCrossEntropy):\n",
    "                self.softmax_classifier_output =  Activation_Softmax_Loss_CategoricalCrossEntropy()\n",
    "            \n",
    "                \n",
    "                \n",
    "    # forward method || performs forward pass\n",
    "    def forward(self, X, training):\n",
    "        \n",
    "        # calling the forward method on the input layer with X data\n",
    "        # this will ensure the next layer or technically the first layer\n",
    "        # gets the data it needs from the previous layer\n",
    "        self.input_layer.forward(X, training)\n",
    "        \n",
    "        # looping over the layers and passing the previous layer output to each layer\n",
    "        for layer in self.layers:\n",
    "            layer.forward(layer.prev.output, training)\n",
    "            \n",
    "        # returning the latest layer that is the last activation function output\n",
    "        return layer.output\n",
    "    \n",
    "    \n",
    "    # performs backward pass\n",
    "    def backward(self, output, y):\n",
    "        \n",
    "        self.loss.backward(output, y)\n",
    "        \n",
    "        for layer in reversed(self.layers):\n",
    "            layer.backward(layer.next.dinputs)\n",
    "\n",
    "        \n",
    "\n",
    "from nnfs.datasets import sine_data\n",
    "\n",
    "X,y = spiral_data(samples=1000, classes=3)\n",
    "X_test, y_test = spiral_data(samples=100, classes=3)\n",
    "\n",
    "y = y.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "        \n",
    "model = Model()\n",
    "\n",
    "# adding layers\n",
    "\n",
    "model.add(Layer_Dense(2, 512, weight_regularizer_l2=5e-4, bias_regularizer_l2=5e-4))\n",
    "model.add(Activation_Relu())\n",
    "model.add(Layer_Dropout(0.1))\n",
    "model.add(Layer_Dense(512, 3))\n",
    "model.add(Activation_Softmax())\n",
    "\n",
    "\n",
    "model.set(loss=Loss_BinaryCrossentropy(),\n",
    "          optimizer=Optimizer_Adam(learning_rate=0.05, decay=5e-7), accuracy = Accuracy_Categorical())\n",
    "\n",
    "model.finalize()\n",
    "\n",
    "model.train(X, y, epochs=10000, print_every=100, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c8d1a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ee33f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a92f9652",
   "metadata": {},
   "source": [
    "# Working with MNIST Fashion Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ebbc04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://nnfs.io/datasets/fashion_mnist_images.zip\"\n",
    "FILE = \"fashion_mnist_images.zip\"\n",
    "FOLDER = \"D:\\Machine Learning Projects\\datasets\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7073916b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://nnfs.io/datasets/fashion_mnist_images.zip and saving as fashion_mnist_images.zip...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib\n",
    "import urllib.request\n",
    "\n",
    "if not os.path.isfile(FILE):\n",
    "    print(f\"Downloading {URL} and saving as {FILE}...\")\n",
    "    urllib.request.urlretrieve(URL, FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb78c458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipping images..\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnzipping images..\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ZipFile(FILE) \u001b[38;5;28;01mas\u001b[39;00m zip_images:\n\u001b[1;32m----> 6\u001b[0m     \u001b[43mzip_images\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextractall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFOLDER\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\zipfile.py:1645\u001b[0m, in \u001b[0;36mZipFile.extractall\u001b[1;34m(self, path, members, pwd)\u001b[0m\n\u001b[0;32m   1642\u001b[0m     path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(path)\n\u001b[0;32m   1644\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m zipinfo \u001b[38;5;129;01min\u001b[39;00m members:\n\u001b[1;32m-> 1645\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_member\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzipinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpwd\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\zipfile.py:1700\u001b[0m, in \u001b[0;36mZipFile._extract_member\u001b[1;34m(self, member, targetpath, pwd)\u001b[0m\n\u001b[0;32m   1696\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m targetpath\n\u001b[0;32m   1698\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen(member, pwd\u001b[38;5;241m=\u001b[39mpwd) \u001b[38;5;28;01mas\u001b[39;00m source, \\\n\u001b[0;32m   1699\u001b[0m      \u001b[38;5;28mopen\u001b[39m(targetpath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m target:\n\u001b[1;32m-> 1700\u001b[0m     \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopyfileobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1702\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m targetpath\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\shutil.py:187\u001b[0m, in \u001b[0;36mcopyfileobj\u001b[1;34m(fsrc, fdst, length)\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m                 fdst_write(mv)\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcopyfileobj\u001b[39m(fsrc, fdst, length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;124;03m\"\"\"copy data from file-like object fsrc to file-like object fdst\"\"\"\u001b[39;00m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;66;03m# Localize variable access to minimize overhead.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "print(\"Unzipping images..\")\n",
    "\n",
    "with ZipFile(FILE) as zip_images:\n",
    "    zip_images.extractall(FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02dad936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n"
     ]
    }
   ],
   "source": [
    "labels = os.listdir(\"fashion_mnist_images/train\")\n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8b0017d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m files \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfashion_mnist_images/train/0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(files[:\u001b[38;5;241m10\u001b[39m])\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(files))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "files = os.listdir(\"fashion_mnist_images/train/0\")\n",
    "\n",
    "print(files[:10])\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57ce5baf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0  49\n",
      "  135 182 150  59   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  78 255\n",
      "  220 212 219 255 246 191 155  87   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  57 206 215\n",
      "  203 191 203 212 216 217 220 211  15   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   1   0   0   0  58 231 220 210\n",
      "  199 209 218 218 217 208 200 215  56   0]\n",
      " [  0   0   0   0   1   2   0   0   4   0   0   0   0 145 213 207 199 187\n",
      "  203 210 216 217 215 215 206 215 130   0]\n",
      " [  0   0   0   0   1   2   4   0   0   0   3 105 225 205 190 201 210 214\n",
      "  213 215 215 212 211 208 205 207 218   0]\n",
      " [  1   5   7   0   0   0   0   0  52 162 217 189 174 157 187 198 202 217\n",
      "  220 223 224 222 217 211 217 201 247  65]\n",
      " [  0   0   0   0   0   0  21  72 185 189 171 171 185 203 200 207 208 209\n",
      "  214 219 222 222 224 215 218 211 212 148]\n",
      " [  0  70 114 129 145 159 179 196 172 176 185 196 199 206 201 210 212 213\n",
      "  216 218 219 217 212 207 208 200 198 173]\n",
      " [  0 122 158 184 194 192 193 196 203 209 211 211 215 218 221 222 226 227\n",
      "  227 226 226 223 222 216 211 208 216 185]\n",
      " [ 21   0   0  12  48  82 123 152 170 184 195 211 225 232 233 237 242 242\n",
      "  240 240 238 236 222 209 200 193 185 106]\n",
      " [ 26  47  54  18   5   0   0   0   0   0   0   0   0   0   2   4   6   9\n",
      "    9   8   9   6   6   4   2   0   0   0]\n",
      " [  0  10  27  45  55  59  57  50  44  51  58  62  65  56  54  57  59  61\n",
      "   60  63  68  67  66  73  77  74  65  39]\n",
      " [  0   0   0   0   4   9  18  23  26  25  23  25  29  37  38  37  39  36\n",
      "   29  31  33  34  28  24  20  14   7   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "image_data = cv2.imread(\"fashion_mnist_images/train/7/0002.png\", cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "print(image_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adce75eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(image_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78b58ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(cv2.imread(\"fashion_mnist_images/train/4/0002.png\", cv2.IMREAD_UNCHANGED))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27d1c125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(cv2.imread(\"fashion_mnist_images/train/4/0002.png\", cv2.IMREAD_UNCHANGED), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936f6b96",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1625d90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to load the datasets for test and train\n",
    "def mnist_data(dataset):\n",
    "    \n",
    "    # getting the labels from the dataset folder\n",
    "    labels = os.listdir(f\"fashion_mnist_images/{dataset}\")\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    # iterating over the labels \n",
    "    for label in labels:\n",
    "        # iterating over the images and loading the images \n",
    "        for file in os.listdir(os.path.join(\"fashion_mnist_images\", dataset, label)):\n",
    "            image = cv2.imread(os.path.join(f\"fashion_mnist_images/{dataset}\", label, file), cv2.IMREAD_UNCHANGED)\n",
    "            \n",
    "            # storing the images and the labels\n",
    "            X.append(image)\n",
    "            y.append(label)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# creating a function to call the mnist_data function to get the train and test data at once\n",
    "def create_data_mnist():\n",
    "    \n",
    "    X, y = mnist_data(\"train\")\n",
    "    X_test, y_test = mnist_data(\"train\")\n",
    "    \n",
    "    return X, y, X_test, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2072cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, X_test, y_test = create_data_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cb0531",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78fe5a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# changing the values to numpy array\n",
    "X = np.array(X)\n",
    "y = np.array(y).astype('uint8')\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test).astype('uint8')\n",
    "\n",
    "# changing the datatype to float32 and normalizing the 0-255 range values to 0,1 or -1,1\n",
    "X = (X.astype(np.float32) - 127.5) / 127.5\n",
    "X_test = (X_test.astype(np.float32) - 127.5) / 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0bb3fe8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0 1.0\n",
      "-1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# checking the range of the images matrix values\n",
    "print(X.min(), X.max())\n",
    "print(X_test.min(), X_test.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61262766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98fd98e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n",
      "[1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "example = np.array([[1,2], [3,4]])\n",
    "flattened = example.reshape(-1)\n",
    "\n",
    "print(example)\n",
    "print(flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7beddd65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (60000, 784))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# changing the shapes of the 28, 28 matrix\n",
    "X = X.reshape(X.shape[0], -1)\n",
    "X_test = X_test.reshape(X.shape[0], -1)\n",
    "\n",
    "X.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e7b04e",
   "metadata": {},
   "source": [
    "## Data Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55a8ce4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     1     2 ... 59997 59998 59999]\n"
     ]
    }
   ],
   "source": [
    "keys = np.array(range(X.shape[0]))\n",
    "print(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "38c6959a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3048 19563 58303 ... 42613 43567  2732]\n"
     ]
    }
   ],
   "source": [
    "np.random.shuffle(keys)\n",
    "print(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9be1b5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[keys]\n",
    "y = y[keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2174fc29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3, 9, 1, 6, 5, 3, 9, 0, 4], dtype=uint8)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "babfd04e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAR40lEQVR4nO3dfYxc5XUG8OfZ2dkPrx1sY+IY4gIlNglUlJDFpIISKtoIaCtIhVBQFVGB6qgKEpEStYioCX/kD5Q2iVKpjeoEFKdNSaMmFCqhJtShpfkoYBMH21AwUBtsbK/Bxmt7vR8ze/rHXqIF9p53mDt37uDz/CRrd+fs3Xl37Md3Zs5935dmBhE5+fVVPQAR6Q6FXSQIhV0kCIVdJAiFXSSI/m7e2QAHbQgj3bzLEOyURbm1Zp3usfUj0/4P9w8HZmfd8vSKodxabcr/0X2HjyfuXN5sEscxbVML/q0VCjvJqwB8DUANwDfN7C7v+4cwgkt4ZZG7lAVMXr4ut3bs9Jp77HsefNH/4fTTbidOuPXdN5+bWzvlBf8/iiX//D9uXd7qUduUW2v7aTzJGoC/BXA1gPMA3EjyvHZ/noiUq8hr9nUAnjOzF8xsGsB3AVzbmWGJSKcVCfsZAF6a9/We7LY3ILme5GaSm2eQeJEmIqUp/d14M9tgZqNmNlrHYNl3JyI5ioR9L4DV875+b3abiPSgImF/HMAakmeTHADwcQAPdGZYItJpbbfezKxB8lYAP8Rc6+0eM9vRsZF1W6LFhAKzA2vn57efAODZm5e59Qsuft6tf2Tpf+XWXp5a6h77Z3f8p1s/f2DYrf/Bs1e79YfP+avc2obDH3KPfeXPF7v1F4/7j9u2n78vt7b2b3a7xzb2vuzW34kK9dnN7EEAD3ZoLCJSIl0uKxKEwi4ShMIuEoTCLhKEwi4ShMIuEgS7ubrsu7jcKpvimuqjM/H/3mwzt/Ts31/sHvrbFzzj1l+bzp+PDgDjU/lzwgHgyIn8+qz5v/eKxf6c8TNGXnPrj+850617liyadOtD/Q23Xuvzp8ieNnwst/biuN+jX/7Hr7r15uHDbr0qj9omjNuhBf/SdWYXCUJhFwlCYRcJQmEXCUJhFwlCYRcJoqtLSVcq1WK0/NYaADSvuCi39oG1/podW15e7dZTXcG+RItpqJ7fokq1p/YfWeLWDx7zl/4ecO4b8Ft7EzN199iZWf9cdGxqwK2/4ox9zakH3WN3fPb9bv2sz/3crfcindlFglDYRYJQ2EWCUNhFglDYRYJQ2EWCUNhFgojTZy/oyGfzp0sOJKag9tf8XvfiIX9brHFnCisAnJjO71d7PXgAWDLs3zdZbAq010u3xPTbCef3AoD+xDUEXo//wIR/fcHlv/ukW3/xL/3dcb0p0VXRmV0kCIVdJAiFXSQIhV0kCIVdJAiFXSQIhV0kCPXZM8du+LBbXz68J7d28Lg/53t4YMatH50cLHS816+ebvj94IHEv4BG0z8f1Pv9fvLUTP4dpK4vOP1d4259Ztb/3aYa+fc90/SPPTDp9+FfvfkCt37qN3tvvnuhsJPcBeAogCaAhpmNdmJQItJ5nTiz/46ZvdKBnyMiJdJrdpEgiobdAPyI5BaS6xf6BpLrSW4muXkG/ms0ESlP0afxl5nZXpLvBvAQyf81s0fmf4OZbQCwAZjb663g/YlImwqd2c1sb/ZxDMB9ANZ1YlAi0nlth53kCMklr38O4KMAtndqYCLSWUWexq8EcB/nFj3vB/BPZvbvHRlVBQ7+0YRbbzpz1gcSvealQyfcujcfHQCOT/rro3tGhqbd+kyiD5+SmpPumUis+576yY3EuvLD9fzrE/oS8/THji92680/TGzZ/E2/XIW2w25mLwD4zQ6ORURKpNabSBAKu0gQCrtIEAq7SBAKu0gQYaa4Tv3+xW79otXPu/Vt+1fl1lLLMU81/Yc5NdXzyMSwW6/X8lt/Ryf8ZagXJ8ae0pz1G2SDiaWsPcmlpBNLdJ8yMJlbOzDht9aOnvCnHZ972phbH7v+Erc+8i+PuvUy6MwuEoTCLhKEwi4ShMIuEoTCLhKEwi4ShMIuEkSYPvui/37GrW/ddL5bf/cl+3Nr7xnxlzx+6ehSt95ILGtsifV9vH7z8KA/xTW1VHSql53iTYFNbQedmj67ZNC/RsDrpa9e8pp77MSwP/32F0+d7dY/8PBOt17Fhs46s4sEobCLBKGwiwShsIsEobCLBKGwiwShsIsEEabP3hz3e+Fnft7fYrd2/rm5tem/8x/GC0592a3vPrbcrU9MLXXrk862yP19fp881euuUmpsh44vcuu3nfvj3NpPj6xxj33182e59bU/fsytV9FHT9GZXSQIhV0kCIVdJAiFXSQIhV0kCIVdJAiFXSSIMH32pD5/TnlzR/58+BMf8X/0llt+y62//5an3fpY3V/jfLqR/9fYTMwJZ6Je6yvWMS7Sx59yrh8AgOvft9Wtf+kfr8+trf7iz9xj+7HFrb8TJc/sJO8hOUZy+7zblpN8iOTO7OOycocpIkW18jT+WwCuetNttwPYZGZrAGzKvhaRHpYMu5k9AuDQm26+FsDG7PONAK7r7LBEpNPafc2+0sz2ZZ/vB7Ay7xtJrgewHgCG4F/LLCLlKfxuvJkZgNx3Ycxsg5mNmtloHf5meSJSnnbDfoDkKgDIPvpbWopI5doN+wMAbso+vwnA/Z0ZjoiUJfmaneS9AK4AsILkHgBfAHAXgO+RvAXAbgA3lDnIrrDE+uh0+tGJhd1PvdufK//TD61z61dc5PfhH9v7a7m1ZmJd+CL7p7fCW/u939lXHvAfcgBYO5S/lj8AnPbLAr9b4roLzPbijHVfMuxmdmNO6coOj0VESqTLZUWCUNhFglDYRYJQ2EWCUNhFgtAU19el9kUuUX35pFvf+dppbr3Itsi1vnJ/7+as13rzjx0emHHrPzzkb7N9bFX+HQz5d/2ObK2l6MwuEoTCLhKEwi4ShMIuEoTCLhKEwi4ShMIuEoT67CcBr5fel+ije33wVng9fsAfW+rYomNjYtayf3Diviu8LqNdOrOLBKGwiwShsIsEobCLBKGwiwShsIsEobCLBKE+e6sKLCWdUq/7c6dT2y57vfT+viLN5nQvvMzjU8fOmn+umtW/7jfQmV0kCIVdJAiFXSQIhV0kCIVdJAiFXSQIhV0kCHUie0Ct5vfCi/a6y/zZswWOL3JsK6xW4Oe/A+erpyTP7CTvITlGcvu82+4kuZfk1uzPNeUOU0SKauVp/LcAXLXA7V81swuzPw92dlgi0mnJsJvZIwAOdWEsIlKiIm/Q3Uryyexp/rK8byK5nuRmkptnMFXg7kSkiHbD/nUA5wC4EMA+AF/O+0Yz22Bmo2Y2Wsdgm3cnIkW1FXYzO2BmTTObBfANAOs6OywR6bS2wk5y1bwvPwZge973ikhvSPbZSd4L4AoAK0juAfAFAFeQvBCAAdgF4JPlDfHkV0vsoZ7qR88666s3E+ufpzrRA/2JufaJtd0bs+2/LZQ6dqJRd+uzfjmcZNjN7MYFbr67hLGISIl0uaxIEAq7SBAKu0gQCrtIEAq7SBCa4toDBusNt15kKmjqyL5E2y+lltgSulFsJWtXainp5C8fjM7sIkEo7CJBKOwiQSjsIkEo7CJBKOwiQSjsIkGoz94qOv8vmj8NNGVkYNqtH54Ybvtn1xJbNpe5THVKqsefqjdSffaTbzXoQnRmFwlCYRcJQmEXCUJhFwlCYRcJQmEXCUJhFwlCffYeMFJP9NlRpM+e6FU3/T57aqnoIlLz9JN99tQy1eqzv4HO7CJBKOwiQSjsIkEo7CJBKOwiQSjsIkEo7CJBqM/eA4ZqM249Nefc25WZiV51qt5fKzYfvlniuvHJNe+1bvwbJM/sJFeTfJjkUyR3kLwtu305yYdI7sw+Lit/uCLSrlaexjcAfMbMzgPwYQCfInkegNsBbDKzNQA2ZV+LSI9Kht3M9pnZE9nnRwE8DeAMANcC2Jh920YA15U0RhHpgLf1mp3kWQA+COBRACvNbF9W2g9gZc4x6wGsB4AhLGp7oCJSTMvvxpNcDOD7AD5tZuPza2ZmyJl2YGYbzGzUzEbrGCw0WBFpX0thJ1nHXNC/Y2Y/yG4+QHJVVl8FYKycIYpIJySfxpMkgLsBPG1mX5lXegDATQDuyj7eX8oIAxjp96e4ptpjfc5y0anWWGqaaKr1VmQK7Eyj5tZTY0u1LAtNce3zx4bZxPLhXj8UAKz7829bec1+KYBPANhGcmt22x2YC/n3SN4CYDeAG0oZoYh0RDLsZvYT5F+ecGVnhyMiZdHlsiJBKOwiQSjsIkEo7CJBKOwiQWiKaw94dWrErReZZpqc4upW04YH/F530+mVezUg3WefbNbdem2qwrWkK+ijp+jMLhKEwi4ShMIuEoTCLhKEwi4ShMIuEoTCLhKE+uytsvLWRD444ffZG832+9H9Bceduu+aM5ce8Pv8qbnw/YmfnaqzyK9e4t93VXRmFwlCYRcJQmEXCUJhFwlCYRcJQmEXCUJhFwlCffZW0fl/0RJriCccPupvi7V08Qm37s0LH+hvuMf21/yxJ7dFTqg5xw8OTbnHjk/6OwhNNxPrzo8UmK2fmo+eWhe+6M8vgc7sIkEo7CJBKOwiQSjsIkEo7CJBKOwiQSjsIkG0sj/7agDfBrAScztebzCzr5G8E8CfAjiYfesdZvZgWQOtXInzm6fG/X7yocS875mJgdza0T6/h4++RL831Q5O7P9e6L4Tv/fiQX9f+1P+z7/GwMN+Pxo2mxp7sWsvytDKRTUNAJ8xsydILgGwheRDWe2rZvbX5Q1PRDqllf3Z9wHYl31+lOTTAM4oe2Ai0llv6zU7ybMAfBDAo9lNt5J8kuQ9JJflHLOe5GaSm2fgXx4pIuVpOewkFwP4PoBPm9k4gK8DOAfAhZg78395oePMbIOZjZrZaB3+a1MRKU9LYSdZx1zQv2NmPwAAMztgZk0zmwXwDQDryhumiBSVDDtJArgbwNNm9pV5t6+a920fA7C988MTkU5p5d34SwF8AsA2kluz2+4AcCPJCzHXnNkF4JMljK93lDglcXhXfusMAAYvPubWT1k+nls7cmLIPXYoseXyxJQ/toF+v8XkTaFtJKaoXrrqBbd+/+MXufW1//qYW/dYs/daZ0W18m78T7DwNt4nb09d5CSkK+hEglDYRYJQ2EWCUNhFglDYRYJQ2EWC0FLSPWD1F3/m1mvnn+vWZ1YsOC0BADB0un+J8swifxpp3a0CTLWjp/KvT6j5bXb84shSt77239rvoyeXgq5gqeey6cwuEoTCLhKEwi4ShMIuEoTCLhKEwi4ShMIuEgSti/1EkgcB7J530woAr3RtAG9Pr46tV8cFaGzt6uTYzjSz0xYqdDXsb7lzcrOZjVY2AEevjq1XxwVobO3q1tj0NF4kCIVdJIiqw76h4vv39OrYenVcgMbWrq6MrdLX7CLSPVWf2UWkSxR2kSAqCTvJq0g+Q/I5krdXMYY8JHeR3EZyK8nNFY/lHpJjJLfPu205yYdI7sw+5k9m7/7Y7iS5N3vstpK8pqKxrSb5MMmnSO4geVt2e6WPnTOurjxuXX/NTrIG4FkAvwdgD4DHAdxoZk91dSA5SO4CMGpmlV+AQfJyAMcAfNvMfiO77UsADpnZXdl/lMvM7C96ZGx3AjhW9Tbe2W5Fq+ZvMw7gOgB/ggofO2dcN6ALj1sVZ/Z1AJ4zsxfMbBrAdwFcW8E4ep6ZPQLg0JtuvhbAxuzzjZj7x9J1OWPrCWa2z8yeyD4/CuD1bcYrfeyccXVFFWE/A8BL877eg97a790A/IjkFpLrqx7MAlaa2b7s8/0AVlY5mAUkt/HupjdtM94zj107258XpTfo3uoyM7sIwNUAPpU9Xe1JNvcarJd6py1t490tC2wz/itVPnbtbn9eVBVh3wtg9byv35vd1hPMbG/2cQzAfei9ragPvL6DbvZxrOLx/EovbeO90Dbj6IHHrsrtz6sI++MA1pA8m+QAgI8DeKCCcbwFyZHsjROQHAHwUfTeVtQPALgp+/wmAPdXOJY36JVtvPO2GUfFj13l25+bWdf/ALgGc+/IPw/gc1WMIWdcvw7gl9mfHVWPDcC9mHtaN4O59zZuAXAqgE0AdgL4DwDLe2hs/wBgG4AnMResVRWN7TLMPUV/EsDW7M81VT92zri68rjpclmRIPQGnUgQCrtIEAq7SBAKu0gQCrtIEAq7SBAKu0gQ/w8c4oftDEuFXgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label : 3\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(X[1].reshape(28,28))\n",
    "plt.show()\n",
    "\n",
    "print(\"label :\", y[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4738856",
   "metadata": {},
   "source": [
    "## Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "acf3d56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import matplotlib.pyplot as plt\n",
    "from nnfs.datasets import sine_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "class Layer_Dense:\n",
    "    \n",
    "    # getting the number of inputs and neurons\n",
    "    def __init__(self, n_inputs, n_neurons,\n",
    "                 weight_regularizer_l1=0,\n",
    "                bias_regularizer_l1=0,\n",
    "                weight_regularizer_l2=0,\n",
    "                bias_regularizer_l2=0):\n",
    "        \n",
    "        #initializing random weights in range \n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "        self.weight_regularizer_l1= weight_regularizer_l1\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        \n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "        \n",
    "    # getting the output with (mx +b)\n",
    "    def forward(self, inputs, training):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    \n",
    "    # backward pass which takes the derivative values from the next function \n",
    "    def backward(self, dvalues):\n",
    "        # getting the partial derivative with the inputs\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims = True)\n",
    "        \n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dl1 = np.ones_like(self.weights)\n",
    "            dl1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dl1\n",
    "            \n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "            \n",
    "        if self.bias_regularizer_l1 >0:\n",
    "            dl1 = np.ones_like(self.biases)\n",
    "            dl1[self.biases < 0] = -1\n",
    "            self.biases += self.bias_regularizer_l1 * dl1\n",
    "            \n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
    "        \n",
    "        \n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "        \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Layer_Dropout:\n",
    "    \n",
    "    def __init__(self, rate):\n",
    "        self.rate = 1- rate\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs, training):\n",
    "        self.intputs = inputs\n",
    "        \n",
    "        if not training:\n",
    "            self.output = inputs.copy()\n",
    "            return\n",
    "        \n",
    "        self.binary_mask = np.random.binomial(1, self.rate, size=inputs.shape) / self.rate\n",
    "        \n",
    "        self.output = inputs * self.binary_mask\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * self.binary_mask\n",
    "        \n",
    "class Layer_Input:\n",
    "    \n",
    "    # forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        self.output = inputs\n",
    "        \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Activation_Relu:\n",
    "    \n",
    "    # The method which takes the input\n",
    "    def forward(self, inputs, training):\n",
    "        \n",
    "        self.inputs = inputs\n",
    "        # returns output with the maximum of 0 and the input value\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <=0] = 0\n",
    "        \n",
    "    def predictions(self, outputs):\n",
    "        return outputs\n",
    "        \n",
    "#---------------------------------------------------------------------\n",
    "        \n",
    "# Creating an Activation function class\n",
    "class Activation_Softmax:\n",
    "    \n",
    "    # The method which takes the input\n",
    "    def forward(self, inputs, training):\n",
    "        self.inputs = inputs\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        \n",
    "        norm_vals = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        \n",
    "        self.output = norm_vals\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        \n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "                \n",
    "                single_output = single_output.reshape(-1,1)\n",
    "                \n",
    "                jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "                \n",
    "                self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "                \n",
    "    def predictions(self, outputs):\n",
    "        return np.argmax(outputs, axis=1)\n",
    "                \n",
    "#---------------------------------------------------------------------\n",
    "                \n",
    "class Activation_Linear:\n",
    "    \n",
    "    # forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        self.inputs = inputs\n",
    "        self.output = inputs\n",
    "        \n",
    "    # backward pass\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "    def predictions(self, outputs):\n",
    "        return outputs\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "    \n",
    "class Activation_Sigmoid:\n",
    "    \n",
    "    def forward(self, inputs, training):\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * (1 - self.output) * self.output\n",
    "        \n",
    "    def predictions(self, outputs):\n",
    "        return (outputs > 0.5) * 1\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "        \n",
    "class Optimizer_SGD:\n",
    "    \n",
    "    # setting learning rate default as 1.\n",
    "    def __init__(self, learning_rate = 1.0, decay=0., momentum =0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def pre_update_parameters(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "        \n",
    "    def update_params(self, layer):\n",
    "        \n",
    "        # if we use momentum\n",
    "        if self.momentum:\n",
    "            # if the layer does not contain momentum arrays\n",
    "            if not hasattr(layer, \"weight_momentums\"):\n",
    "                # creating weight_momentums and bias_momentums arrays initialized with zero\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            \n",
    "            # getting the weight updates with momentum - take previous factors\n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "            \n",
    "            # getting bias updates\n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "        \n",
    "        else:\n",
    "\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates =  -self.current_learning_rate * layer.dbiases\n",
    "            \n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "        \n",
    "    def post_update_parameters(self):\n",
    "        self.iterations +=1\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "        \n",
    "class Optimizer_AdaGrad:\n",
    "    \n",
    "    # setting learning rate default as 1.\n",
    "    def __init__(self, learning_rate = 1.0, decay=0., epsilon =1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def pre_update_parameters(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "        \n",
    "    def update_params(self, layer):\n",
    "        # checking if layer has attribute weight_cache\n",
    "        if not hasattr(layer, \"weight_cache\"):\n",
    "            # creating weight_cache and bias_cache\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        # getting the square of the gradients\n",
    "        layer.weight_cache += layer.dweights **2\n",
    "        layer.bias_cache += layer.dbiases **2\n",
    "        \n",
    "        # calculating the result and adding it to the weights and biases\n",
    "        layer.weights += - self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += - self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "        \n",
    "    def post_update_parameters(self):\n",
    "        self.iterations +=1\n",
    "                \n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class Optimizer_Adam:\n",
    "    \n",
    "    def __init__(self, learning_rate = 0.001, decay=0. ,\n",
    "                 epsilon=1e-7, beta_1=0.9, beta_2 = 0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        \n",
    "    def pre_update_parameters(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "        \n",
    "    def update_params(self, layer):\n",
    "        # checking if layer has attribute weight_cache\n",
    "        if not hasattr(layer, \"weight_cache\"):\n",
    "            \n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            \n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        \n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1- self.beta_1) * layer.dweights\n",
    "        \n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1- self.beta_1) * layer.dbiases\n",
    "        \n",
    "        \n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        \n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "\n",
    "        \n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights **2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases **2\n",
    "        \n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations +1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations +1))\n",
    "        \n",
    "        \n",
    "        layer.weights += - self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected)+ self.epsilon)\n",
    "        layer.biases += - self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected)+ self.epsilon)\n",
    "\n",
    "        \n",
    "    def post_update_parameters(self):\n",
    "        self.iterations +=1\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "                \n",
    "class Loss:\n",
    "    \n",
    "    def regularization_loss(self):\n",
    "        \n",
    "        regularization_loss = 0\n",
    "        \n",
    "        #------------------L1 Regularization------------------------\n",
    "\n",
    "        \n",
    "        for layer in self.trainable_layers:\n",
    "            \n",
    "            if layer.weight_regularizer_l1 > 0:\n",
    "                regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "\n",
    "            if layer.bias_regularizer_l1 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
    "\n",
    "            #------------------L2 Regularization------------------------\n",
    "\n",
    "            if layer.weight_regularizer_l2 > 0:\n",
    "                regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
    "\n",
    "            if layer.bias_regularizer_l2 > 0 :\n",
    "                regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
    "\n",
    "        return regularization_loss\n",
    "    \n",
    "    # set / remember trainable layers\n",
    "    def remember_trainable_layers(self, trainable_layers):\n",
    "        self.trainable_layers = trainable_layers\n",
    "        \n",
    "        \n",
    "    def calculate(self, output, y, *, include_regularization= False):\n",
    "        \n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        \n",
    "        self.accumulated_sum += np.sum(sample_losses)\n",
    "        self.accumulated_count += len(sample_losses)\n",
    "        \n",
    "        \n",
    "        if not include_regularization:\n",
    "            return data_loss\n",
    "        \n",
    "        return data_loss, self.regularization_loss()\n",
    "    \n",
    "    def calculate_accumulated(self, *, include_regularization=False):\n",
    "        \n",
    "        data_loss = self.accumulated_sum / self.accumulated_count\n",
    "        \n",
    "        if not include_regularization:\n",
    "            return data_loss\n",
    "        \n",
    "        return data_loss, self.regularization_loss()\n",
    "    \n",
    "    def new_pass(self):\n",
    "        self.accumulated_sum = 0\n",
    "        self.accumulated_count = 0\n",
    "    \n",
    "    \n",
    "#---------------------------------------------------------------------\n",
    "    \n",
    "class Loss_MeanSquaredError(Loss):\n",
    "    \n",
    "    # forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        sample_losses = np.mean((y_true - y_pred)**2, axis=-1)\n",
    "        \n",
    "        return sample_losses\n",
    "    \n",
    "    # backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        # gradient on values\n",
    "        self.dinputs = -2 * (y_true - dvalues) / outputs\n",
    "        \n",
    "        # normalizing gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "        \n",
    "class Loss_MeanAbsoluteError(Loss):\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        sample_losses = np.mean(np.abs(y_true - y_pred), axis=-1)\n",
    "        \n",
    "        return sample_losses\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        self.dinputs = np.sign(y_true - dvalues)/ outputs\n",
    "        \n",
    "        self.dinputs = self.dinputs / samples\n",
    "         \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "    \n",
    "    def forward(self, y_pred, y_target):\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        # clipping the values so we don't end up with inf error\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "\n",
    "        # checking if 1D array so we just need to get that index value\n",
    "        if len(y_target.shape) == 1:\n",
    "            correct = y_pred_clipped[range(samples), y_target]\n",
    "            \n",
    "        # checking if one-hot-encoded values, then multiplying and summing up\n",
    "        elif len(y_target.shape) == 2:\n",
    "            correct = np.sum(y_pred_clipped * y_target, axis=1)\n",
    "        \n",
    "        negative_log = -np.log(correct)\n",
    "        \n",
    "        return negative_log\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "        \n",
    "        if len(y_true.shape) ==1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "            \n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples\n",
    "    \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Accuracy:\n",
    "    \n",
    "    def calculate(self, predictions, y):\n",
    "        \n",
    "        comparisons = self.compare(predictions, y)\n",
    "        \n",
    "        accuracy = np.mean(comparisons)\n",
    "        \n",
    "        self.accumulated_sum += np.sum(comparisons)\n",
    "        self.accumulated_count += len(comparisons)\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def calculate_accumulated(self):\n",
    "        \n",
    "        accuracy = self.accumulated_sum / self.accumulated_count\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def new_pass(self):\n",
    "        self.accumulated_sum = 0\n",
    "        self.accumulated_count = 0\n",
    "    \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Accuracy_Regression(Accuracy):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.precision = None\n",
    "        \n",
    "    def init(self, y, reinit= False):\n",
    "        if self.precision is None or reinit:\n",
    "            self.precision = np.std(y) / 250\n",
    "            \n",
    "    def compare(self, predictions, y):\n",
    "        return np.absolute(predictions - y) < self.precision\n",
    "        \n",
    "\n",
    "        \n",
    "class Accuracy_Categorical(Accuracy):\n",
    "    \n",
    "    def init(self, y):\n",
    "        pass\n",
    "    \n",
    "    def compare(self, predictions, y):\n",
    "        if len(y.shape) ==2:\n",
    "            y = np.argmax(y, axis=1)\n",
    "            \n",
    "            \n",
    "        return predictions == y\n",
    "        \n",
    "\n",
    "#.......................................................................\n",
    "\n",
    "class Activation_Softmax_Loss_CategoricalCrossEntropy():\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        samples = len(dvalues)\n",
    "        \n",
    "#         # checking if the values are one-hot-encoded\n",
    "#         if len(y_true.shape) ==2:\n",
    "#             y_true = np.argmax(y_true, axis=1)\n",
    "            \n",
    "        \n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "        # calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -=1\n",
    "        \n",
    "        # normalizing gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "    \n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Loss_BinaryCrossentropy(Loss):\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        sample_losses = -(y_true * np.log(y_pred_clipped) + (1-y_true) * np.log(1- y_pred_clipped))\n",
    "        \n",
    "        sample_losses = np.mean(sample_losses, axis = -1)\n",
    "        \n",
    "        return sample_losses\n",
    "    \n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        self.dinputs = -(y_true / clipped_dvalues - (1-y_true) / (1-clipped_dvalues))/ outputs\n",
    "        \n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9287f8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # creating a list of network objects\n",
    "        self.layers = []\n",
    "        self.softmax_classifier_output = None\n",
    "        \n",
    "    # adding layers to the model\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    # set loss and optimizer\n",
    "    def set(self, * , loss, optimizer, accuracy):\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.accuracy = accuracy\n",
    "        \n",
    "    # Train the model\n",
    "    def train(self, X, y, *, epochs=1, batch_size=None, print_every=1, validation_data = None):\n",
    "        \n",
    "        self.accuracy.init(y)\n",
    "        \n",
    "        train_steps = 1\n",
    "        \n",
    "        if validation_data :\n",
    "            validation_steps =1\n",
    "            \n",
    "            X_val, y_val = validation_data\n",
    "            \n",
    "        \n",
    "        if batch_size is not None:\n",
    "            \n",
    "            train_steps = len(X) // batch_size\n",
    "            \n",
    "            if train_steps * batch_size < len(X):\n",
    "                train_steps +=1\n",
    "                \n",
    "            if validation_data is not None:\n",
    "                validation_steps = len(X_val) // batch_size\n",
    "                \n",
    "                if validation_steps * batch_size < len(X_val):\n",
    "                    validation_steps +=1\n",
    "        \n",
    "        \n",
    "        for epoch in range(1, epochs+1):\n",
    "            \n",
    "            print(\"Epoch :\" , epoch)\n",
    "            \n",
    "            # reset accumulated values in loss and accuracy objects\n",
    "            self.loss.new_pass()\n",
    "            self.accuracy.new_pass()\n",
    "            \n",
    "            for step in range(train_steps):\n",
    "                \n",
    "                # if batch is not set using the full dataset \n",
    "                if batch_size is None:\n",
    "                    batch_X = X\n",
    "                    batch_y = y\n",
    "                \n",
    "                # otherwise slice the dataset into batches\n",
    "                else:\n",
    "                    batch_X = X[step * batch_size:(step+1) * batch_size]\n",
    "                    batch_y = y[step * batch_size:(step+1) * batch_size]\n",
    "            \n",
    "                output = self.forward(batch_X, training=True)\n",
    "\n",
    "                data_loss, regularization_loss = self.loss.calculate(output, batch_y, include_regularization=True)\n",
    "\n",
    "                loss = data_loss + regularization_loss\n",
    "\n",
    "                predictions = self.output_layer_activation.predictions(output)\n",
    "\n",
    "                accuracy = self.accuracy.calculate(predictions, batch_y)\n",
    "\n",
    "                self.backward(output, batch_y)\n",
    "\n",
    "                self.optimizer.pre_update_parameters()\n",
    "\n",
    "                for layer in self.trainable_layers:\n",
    "                    self.optimizer.update_params(layer)\n",
    "\n",
    "                self.optimizer.post_update_parameters()\n",
    "\n",
    "                if not step % print_every or step == train_steps - 1:\n",
    "                    print(f\"step: {step}, accuracy: {accuracy}, loss: {loss}, data_loss: {data_loss}, lr: {self.optimizer.current_learning_rate}\")\n",
    "                    \n",
    "                    \n",
    "        epoch_data_loss, epoch_regularization_loss = self.loss.calculate_accumulated(include_regularization = True)\n",
    "        \n",
    "        epoch_loss = epoch_data_loss + epoch_regularization_loss\n",
    "        epoch_accuracy = self.accuracy.calculate_accumulated()\n",
    "        \n",
    "        \n",
    "        print(f\"Training, acc: {epoch_accuracy}, loss: {epoch_loss}, data_loss: {epoch_data_loss}, lr: {self.optimizer.current_learning_rate} \")\n",
    "        \n",
    "        if validation_data is not None:\n",
    "            \n",
    "            self.evaluate(*validation_data, batch_size = batch_size)\n",
    "\n",
    "    # finalize the model\n",
    "    def finalize(self):\n",
    "        \n",
    "        #create and set the input layer\n",
    "        self.input_layer = Layer_Input()\n",
    "        \n",
    "        # count all objects\n",
    "        layer_count = len(self.layers)\n",
    "        \n",
    "        # initializing a list to store trainable layers\n",
    "        self.trainable_layers = []\n",
    "        \n",
    "        for i in range(layer_count):\n",
    "            \n",
    "            if i == 0:\n",
    "                self.layers[i].prev = self.input_layer\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "                \n",
    "            elif i < layer_count -1:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "                \n",
    "            else:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.loss\n",
    "                self.output_layer_activation = self.layers[i]\n",
    "                \n",
    "            # checking if the layer has an attribute weights\n",
    "            # then adding it to the trainable layers list\n",
    "            if hasattr(self.layers[i], \"weights\"):\n",
    "                self.trainable_layers.append(self.layers[i])\n",
    "                \n",
    "                \n",
    "            self.loss.remember_trainable_layers(self.trainable_layers)\n",
    "            \n",
    "        if isinstance(self.layers[-1], Activation_Softmax) and isinstance(self.loss, Loss_CategoricalCrossEntropy):\n",
    "                self.softmax_classifier_output =  Activation_Softmax_Loss_CategoricalCrossEntropy()\n",
    "            \n",
    "                \n",
    "                \n",
    "    # forward method || performs forward pass\n",
    "    def forward(self, X, training):\n",
    "        \n",
    "        # calling the forward method on the input layer with X data\n",
    "        # this will ensure the next layer or technically the first layer\n",
    "        # gets the data it needs from the previous layer\n",
    "        self.input_layer.forward(X, training)\n",
    "        \n",
    "        # looping over the layers and passing the previous layer output to each layer\n",
    "        for layer in self.layers:\n",
    "            layer.forward(layer.prev.output, training)\n",
    "            \n",
    "        # returning the latest layer that is the last activation function output\n",
    "        return layer.output\n",
    "    \n",
    "    \n",
    "    # performs backward pass\n",
    "    def backward(self, output, y):\n",
    "        \n",
    "        self.loss.backward(output, y)\n",
    "        \n",
    "        for layer in reversed(self.layers):\n",
    "            layer.backward(layer.next.dinputs)\n",
    "            \n",
    "            \n",
    "    # evaluate the model using passed in dataset\n",
    "    def evaluate(self, X_val, y_val, *, batch_size=None):\n",
    "        \n",
    "        validation_steps = 1\n",
    "        \n",
    "        if batch_size:\n",
    "            \n",
    "            validation_steps = len(X_val) // batch_size\n",
    "            \n",
    "            if validation_steps * batch_size < len(X_val):\n",
    "                validation_steps +=1\n",
    "        \n",
    "        self.loss.new_pass()\n",
    "        self.accuracy.new_pass()\n",
    "\n",
    "        for step in range(validation_steps):\n",
    "\n",
    "            if not batch_size:\n",
    "                batch_X = X_val\n",
    "                batch_y = y_val\n",
    "\n",
    "            else:\n",
    "                batch_X = X_val[step * batch_size:(step+1)* batch_size]\n",
    "                batch_y = y_val[step * batch_size:(step+1)* batch_size]\n",
    "\n",
    "        output = self.forward(batch_X,training= False)\n",
    "\n",
    "        loss = self.loss.calculate(output, batch_y)\n",
    "\n",
    "        predictions = self.output_layer_activation.predictions(output)\n",
    "\n",
    "        self.accuracy.calculate(predictions, batch_y)\n",
    "\n",
    "        validation_loss = self.loss.calculate_accumulated()\n",
    "        validation_accuracy = self.accuracy.calculate_accumulated()\n",
    "\n",
    "        print(f\"validation || acc : {validation_accuracy} , loss : {validation_loss}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c3c9ab75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1\n",
      "step: 0, accuracy: 0.109375, loss: 2.302527904510498, data_loss: 2.302527904510498, lr: 0.001\n",
      "step: 100, accuracy: 0.7421875, loss: 0.6272779107093811, data_loss: 0.6272779107093811, lr: 0.0009900990099009901\n",
      "step: 200, accuracy: 0.8125, loss: 0.45901572704315186, data_loss: 0.45901572704315186, lr: 0.000980392156862745\n",
      "step: 300, accuracy: 0.8203125, loss: 0.5406885147094727, data_loss: 0.5406885147094727, lr: 0.0009708737864077671\n",
      "step: 400, accuracy: 0.828125, loss: 0.44902029633522034, data_loss: 0.44902029633522034, lr: 0.0009615384615384615\n",
      "step: 468, accuracy: 0.875, loss: 0.32379546761512756, data_loss: 0.32379546761512756, lr: 0.0009552923194497518\n",
      "Epoch : 2\n",
      "step: 0, accuracy: 0.8515625, loss: 0.397887647151947, data_loss: 0.397887647151947, lr: 0.0009552010698251983\n",
      "step: 100, accuracy: 0.8203125, loss: 0.47011977434158325, data_loss: 0.47011977434158325, lr: 0.0009461633077869241\n",
      "step: 200, accuracy: 0.8828125, loss: 0.3234570026397705, data_loss: 0.3234570026397705, lr: 0.0009372949667260287\n",
      "step: 300, accuracy: 0.890625, loss: 0.4341307282447815, data_loss: 0.4341307282447815, lr: 0.0009285913269570063\n",
      "step: 400, accuracy: 0.8203125, loss: 0.4082510471343994, data_loss: 0.4082510471343994, lr: 0.0009200478424878093\n",
      "step: 468, accuracy: 0.90625, loss: 0.2587301433086395, data_loss: 0.2587301433086395, lr: 0.0009143275121148394\n",
      "Epoch : 3\n",
      "step: 0, accuracy: 0.8359375, loss: 0.3761402666568756, data_loss: 0.3761402666568756, lr: 0.0009142439202779302\n",
      "step: 100, accuracy: 0.8203125, loss: 0.42529982328414917, data_loss: 0.42529982328414917, lr: 0.0009059612248595759\n",
      "step: 200, accuracy: 0.890625, loss: 0.26650387048721313, data_loss: 0.26650387048721313, lr: 0.0008978272580355541\n",
      "step: 300, accuracy: 0.875, loss: 0.4010982811450958, data_loss: 0.4010982811450958, lr: 0.0008898380494749957\n",
      "step: 400, accuracy: 0.8359375, loss: 0.3863770067691803, data_loss: 0.3863770067691803, lr: 0.0008819897689186807\n",
      "step: 468, accuracy: 0.9166666666666666, loss: 0.2359752655029297, data_loss: 0.2359752655029297, lr: 0.000876731544800982\n",
      "Epoch : 4\n",
      "step: 0, accuracy: 0.8515625, loss: 0.34351056814193726, data_loss: 0.34351056814193726, lr: 0.0008766546857192952\n",
      "step: 100, accuracy: 0.84375, loss: 0.3902054727077484, data_loss: 0.3902054727077484, lr: 0.0008690362388111585\n",
      "step: 200, accuracy: 0.90625, loss: 0.2496989369392395, data_loss: 0.2496989369392395, lr: 0.0008615490652192642\n",
      "step: 300, accuracy: 0.8828125, loss: 0.3762345612049103, data_loss: 0.3762345612049103, lr: 0.0008541898009737763\n",
      "step: 400, accuracy: 0.8515625, loss: 0.3570482134819031, data_loss: 0.3570482134819031, lr: 0.0008469551960701278\n",
      "step: 468, accuracy: 0.9166666666666666, loss: 0.20981557667255402, data_loss: 0.20981557667255402, lr: 0.0008421052631578947\n",
      "Epoch : 5\n",
      "step: 0, accuracy: 0.859375, loss: 0.32037270069122314, data_loss: 0.32037270069122314, lr: 0.0008420343550016842\n",
      "step: 100, accuracy: 0.859375, loss: 0.3509727120399475, data_loss: 0.3509727120399475, lr: 0.0008350033400133601\n",
      "step: 200, accuracy: 0.8984375, loss: 0.23775506019592285, data_loss: 0.23775506019592285, lr: 0.0008280887711162637\n",
      "step: 300, accuracy: 0.8828125, loss: 0.34421053528785706, data_loss: 0.34421053528785706, lr: 0.0008212877792378449\n",
      "step: 400, accuracy: 0.875, loss: 0.3366873264312744, data_loss: 0.3366873264312744, lr: 0.0008145975887911372\n",
      "step: 468, accuracy: 0.90625, loss: 0.19733212888240814, data_loss: 0.19733212888240814, lr: 0.0008101101749837978\n",
      "Epoch : 6\n",
      "step: 0, accuracy: 0.875, loss: 0.30723994970321655, data_loss: 0.30723994970321655, lr: 0.0008100445524503849\n",
      "step: 100, accuracy: 0.8671875, loss: 0.3223574459552765, data_loss: 0.3223574459552765, lr: 0.0008035355564483729\n",
      "step: 200, accuracy: 0.9140625, loss: 0.2260357290506363, data_loss: 0.2260357290506363, lr: 0.0007971303308090873\n",
      "step: 300, accuracy: 0.890625, loss: 0.3085644543170929, data_loss: 0.3085644543170929, lr: 0.0007908264136022144\n",
      "step: 400, accuracy: 0.875, loss: 0.31448161602020264, data_loss: 0.31448161602020264, lr: 0.0007846214201647706\n",
      "step: 468, accuracy: 0.9270833333333334, loss: 0.18526236712932587, data_loss: 0.18526236712932587, lr: 0.0007804573480059316\n",
      "Epoch : 7\n",
      "step: 0, accuracy: 0.890625, loss: 0.28625500202178955, data_loss: 0.28625500202178955, lr: 0.0007803964413922272\n",
      "step: 100, accuracy: 0.875, loss: 0.30146849155426025, data_loss: 0.30146849155426025, lr: 0.0007743534148985598\n",
      "step: 200, accuracy: 0.921875, loss: 0.22488293051719666, data_loss: 0.22488293051719666, lr: 0.0007684032580298141\n",
      "step: 300, accuracy: 0.8984375, loss: 0.2814120054244995, data_loss: 0.2814120054244995, lr: 0.0007625438462711606\n",
      "step: 400, accuracy: 0.8984375, loss: 0.302335262298584, data_loss: 0.302335262298584, lr: 0.0007567731194187983\n",
      "step: 468, accuracy: 0.9166666666666666, loss: 0.17994220554828644, data_loss: 0.17994220554828644, lr: 0.0007528986598403856\n",
      "Epoch : 8\n",
      "step: 0, accuracy: 0.8828125, loss: 0.2639221251010895, data_loss: 0.2639221251010895, lr: 0.0007528419784687194\n",
      "step: 100, accuracy: 0.9140625, loss: 0.28235313296318054, data_loss: 0.28235313296318054, lr: 0.0007472166180975864\n",
      "step: 200, accuracy: 0.90625, loss: 0.22811952233314514, data_loss: 0.22811952233314514, lr: 0.0007416747014759327\n",
      "step: 300, accuracy: 0.90625, loss: 0.2597932815551758, data_loss: 0.2597932815551758, lr: 0.0007362143856290952\n",
      "step: 400, accuracy: 0.8828125, loss: 0.2935224771499634, data_loss: 0.2935224771499634, lr: 0.0007308338814587444\n",
      "step: 468, accuracy: 0.9375, loss: 0.16506247222423553, data_loss: 0.16506247222423553, lr: 0.0007272198385571959\n",
      "Epoch : 9\n",
      "step: 0, accuracy: 0.8984375, loss: 0.2491224706172943, data_loss: 0.2491224706172943, lr: 0.0007271669575334498\n",
      "step: 100, accuracy: 0.9140625, loss: 0.2657575011253357, data_loss: 0.2657575011253357, lr: 0.000721917412647993\n",
      "step: 200, accuracy: 0.921875, loss: 0.22015322744846344, data_loss: 0.22015322744846344, lr: 0.0007167431192660551\n",
      "step: 300, accuracy: 0.90625, loss: 0.24962887167930603, data_loss: 0.24962887167930603, lr: 0.0007116424708226587\n",
      "step: 400, accuracy: 0.8828125, loss: 0.28935033082962036, data_loss: 0.28935033082962036, lr: 0.0007066139061616733\n",
      "step: 468, accuracy: 0.9375, loss: 0.16101382672786713, data_loss: 0.16101382672786713, lr: 0.0007032348804500702\n",
      "Epoch : 10\n",
      "step: 0, accuracy: 0.921875, loss: 0.23195196688175201, data_loss: 0.23195196688175201, lr: 0.0007031854299978904\n",
      "step: 100, accuracy: 0.9140625, loss: 0.2414509654045105, data_loss: 0.2414509654045105, lr: 0.0006982752601075343\n",
      "step: 200, accuracy: 0.9375, loss: 0.21141070127487183, data_loss: 0.21141070127487183, lr: 0.000693433187712364\n",
      "step: 300, accuracy: 0.9140625, loss: 0.23387494683265686, data_loss: 0.23387494683265686, lr: 0.0006886578059362303\n",
      "step: 400, accuracy: 0.8984375, loss: 0.2715880572795868, data_loss: 0.2715880572795868, lr: 0.0006839477463921757\n",
      "step: 468, accuracy: 0.9270833333333334, loss: 0.15913617610931396, data_loss: 0.15913617610931396, lr: 0.000680781537204711\n",
      "Training, acc: 0.908, loss: 0.25083287329673765, data_loss: 0.25083287329673765, lr: 0.000680781537204711 \n",
      "validation || acc : 0.96875 , loss : 0.10304403305053711\n",
      "validation || acc : 0.9050166666666667 , loss : 0.256163671875\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "\n",
    "model.add(Layer_Dense(X.shape[1], 128))\n",
    "model.add(Activation_Relu())\n",
    "model.add(Layer_Dense(128,128))\n",
    "model.add(Activation_Relu())\n",
    "model.add(Layer_Dense(128, 10))\n",
    "model.add(Activation_Softmax())\n",
    "\n",
    "model.set(loss=Loss_CategoricalCrossEntropy(), optimizer= Optimizer_Adam(decay=1e-4), accuracy= Accuracy_Categorical())\n",
    "\n",
    "model.finalize()\n",
    "\n",
    "model.train(X,y, validation_data=(X_test, y_test), epochs=10,  batch_size=128, print_every=100)\n",
    "\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f1a757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3cae4b7",
   "metadata": {},
   "source": [
    "# Saving and Loading Models and their Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47a2ddd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import matplotlib.pyplot as plt\n",
    "from nnfs.datasets import sine_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "class Layer_Dense:\n",
    "    \n",
    "    # getting the number of inputs and neurons\n",
    "    def __init__(self, n_inputs, n_neurons,\n",
    "                 weight_regularizer_l1=0,\n",
    "                bias_regularizer_l1=0,\n",
    "                weight_regularizer_l2=0,\n",
    "                bias_regularizer_l2=0):\n",
    "        \n",
    "        #initializing random weights in range \n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "        self.weight_regularizer_l1= weight_regularizer_l1\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        \n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "        \n",
    "    # getting the output with (mx +b)\n",
    "    def forward(self, inputs, training):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    \n",
    "    # backward pass which takes the derivative values from the next function \n",
    "    def backward(self, dvalues):\n",
    "        # getting the partial derivative with the inputs\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims = True)\n",
    "        \n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dl1 = np.ones_like(self.weights)\n",
    "            dl1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dl1\n",
    "            \n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "            \n",
    "        if self.bias_regularizer_l1 >0:\n",
    "            dl1 = np.ones_like(self.biases)\n",
    "            dl1[self.biases < 0] = -1\n",
    "            self.biases += self.bias_regularizer_l1 * dl1\n",
    "            \n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
    "        \n",
    "        \n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "        \n",
    "    def get_parameters(self):\n",
    "        return self.weights, self.biases\n",
    "    \n",
    "    def set_parameters(self, weights, biases):\n",
    "        self.weights = weights\n",
    "        self.biases = biases\n",
    "        \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Layer_Dropout:\n",
    "    \n",
    "    def __init__(self, rate):\n",
    "        self.rate = 1- rate\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs, training):\n",
    "        self.intputs = inputs\n",
    "        \n",
    "        if not training:\n",
    "            self.output = inputs.copy()\n",
    "            return\n",
    "        \n",
    "        self.binary_mask = np.random.binomial(1, self.rate, size=inputs.shape) / self.rate\n",
    "        \n",
    "        self.output = inputs * self.binary_mask\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * self.binary_mask\n",
    "        \n",
    "class Layer_Input:\n",
    "    \n",
    "    # forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        self.output = inputs\n",
    "        \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Activation_Relu:\n",
    "    \n",
    "    # The method which takes the input\n",
    "    def forward(self, inputs, training):\n",
    "        \n",
    "        self.inputs = inputs\n",
    "        # returns output with the maximum of 0 and the input value\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <=0] = 0\n",
    "        \n",
    "    def predictions(self, outputs):\n",
    "        return outputs\n",
    "        \n",
    "#---------------------------------------------------------------------\n",
    "        \n",
    "# Creating an Activation function class\n",
    "class Activation_Softmax:\n",
    "    \n",
    "    # The method which takes the input\n",
    "    def forward(self, inputs, training):\n",
    "        self.inputs = inputs\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        \n",
    "        norm_vals = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        \n",
    "        self.output = norm_vals\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        \n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "                \n",
    "                single_output = single_output.reshape(-1,1)\n",
    "                \n",
    "                jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "                \n",
    "                self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "                \n",
    "    def predictions(self, outputs):\n",
    "        return np.argmax(outputs, axis=1)\n",
    "                \n",
    "#---------------------------------------------------------------------\n",
    "                \n",
    "class Activation_Linear:\n",
    "    \n",
    "    # forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        self.inputs = inputs\n",
    "        self.output = inputs\n",
    "        \n",
    "    # backward pass\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "    def predictions(self, outputs):\n",
    "        return outputs\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "    \n",
    "class Activation_Sigmoid:\n",
    "    \n",
    "    def forward(self, inputs, training):\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * (1 - self.output) * self.output\n",
    "        \n",
    "    def predictions(self, outputs):\n",
    "        return (outputs > 0.5) * 1\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "        \n",
    "class Optimizer_SGD:\n",
    "    \n",
    "    # setting learning rate default as 1.\n",
    "    def __init__(self, learning_rate = 1.0, decay=0., momentum =0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def pre_update_parameters(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "        \n",
    "    def update_params(self, layer):\n",
    "        \n",
    "        # if we use momentum\n",
    "        if self.momentum:\n",
    "            # if the layer does not contain momentum arrays\n",
    "            if not hasattr(layer, \"weight_momentums\"):\n",
    "                # creating weight_momentums and bias_momentums arrays initialized with zero\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            \n",
    "            # getting the weight updates with momentum - take previous factors\n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "            \n",
    "            # getting bias updates\n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "        \n",
    "        else:\n",
    "\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates =  -self.current_learning_rate * layer.dbiases\n",
    "            \n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "        \n",
    "    def post_update_parameters(self):\n",
    "        self.iterations +=1\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "        \n",
    "class Optimizer_AdaGrad:\n",
    "    \n",
    "    # setting learning rate default as 1.\n",
    "    def __init__(self, learning_rate = 1.0, decay=0., epsilon =1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def pre_update_parameters(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "        \n",
    "    def update_params(self, layer):\n",
    "        # checking if layer has attribute weight_cache\n",
    "        if not hasattr(layer, \"weight_cache\"):\n",
    "            # creating weight_cache and bias_cache\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        # getting the square of the gradients\n",
    "        layer.weight_cache += layer.dweights **2\n",
    "        layer.bias_cache += layer.dbiases **2\n",
    "        \n",
    "        # calculating the result and adding it to the weights and biases\n",
    "        layer.weights += - self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += - self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "        \n",
    "    def post_update_parameters(self):\n",
    "        self.iterations +=1\n",
    "                \n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class Optimizer_Adam:\n",
    "    \n",
    "    def __init__(self, learning_rate = 0.001, decay=0. ,\n",
    "                 epsilon=1e-7, beta_1=0.9, beta_2 = 0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        \n",
    "    def pre_update_parameters(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "        \n",
    "    def update_params(self, layer):\n",
    "        # checking if layer has attribute weight_cache\n",
    "        if not hasattr(layer, \"weight_cache\"):\n",
    "            \n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            \n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        \n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1- self.beta_1) * layer.dweights\n",
    "        \n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1- self.beta_1) * layer.dbiases\n",
    "        \n",
    "        \n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        \n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "\n",
    "        \n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights **2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases **2\n",
    "        \n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations +1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations +1))\n",
    "        \n",
    "        \n",
    "        layer.weights += - self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected)+ self.epsilon)\n",
    "        layer.biases += - self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected)+ self.epsilon)\n",
    "\n",
    "        \n",
    "    def post_update_parameters(self):\n",
    "        self.iterations +=1\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "                \n",
    "class Loss:\n",
    "    \n",
    "    def regularization_loss(self):\n",
    "        \n",
    "        regularization_loss = 0\n",
    "        \n",
    "        #------------------L1 Regularization------------------------\n",
    "\n",
    "        \n",
    "        for layer in self.trainable_layers:\n",
    "            \n",
    "            if layer.weight_regularizer_l1 > 0:\n",
    "                regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "\n",
    "            if layer.bias_regularizer_l1 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
    "\n",
    "            #------------------L2 Regularization------------------------\n",
    "\n",
    "            if layer.weight_regularizer_l2 > 0:\n",
    "                regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
    "\n",
    "            if layer.bias_regularizer_l2 > 0 :\n",
    "                regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
    "\n",
    "        return regularization_loss\n",
    "    \n",
    "    # set / remember trainable layers\n",
    "    def remember_trainable_layers(self, trainable_layers):\n",
    "        self.trainable_layers = trainable_layers\n",
    "        \n",
    "        \n",
    "    def calculate(self, output, y, *, include_regularization= False):\n",
    "        \n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        \n",
    "        self.accumulated_sum += np.sum(sample_losses)\n",
    "        self.accumulated_count += len(sample_losses)\n",
    "        \n",
    "        \n",
    "        if not include_regularization:\n",
    "            return data_loss\n",
    "        \n",
    "        return data_loss, self.regularization_loss()\n",
    "    \n",
    "    def calculate_accumulated(self, *, include_regularization=False):\n",
    "        \n",
    "        data_loss = self.accumulated_sum / self.accumulated_count\n",
    "        \n",
    "        if not include_regularization:\n",
    "            return data_loss\n",
    "        \n",
    "        return data_loss, self.regularization_loss()\n",
    "    \n",
    "    def new_pass(self):\n",
    "        self.accumulated_sum = 0\n",
    "        self.accumulated_count = 0\n",
    "    \n",
    "    \n",
    "#---------------------------------------------------------------------\n",
    "    \n",
    "class Loss_MeanSquaredError(Loss):\n",
    "    \n",
    "    # forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        sample_losses = np.mean((y_true - y_pred)**2, axis=-1)\n",
    "        \n",
    "        return sample_losses\n",
    "    \n",
    "    # backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        # gradient on values\n",
    "        self.dinputs = -2 * (y_true - dvalues) / outputs\n",
    "        \n",
    "        # normalizing gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "        \n",
    "class Loss_MeanAbsoluteError(Loss):\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        sample_losses = np.mean(np.abs(y_true - y_pred), axis=-1)\n",
    "        \n",
    "        return sample_losses\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        self.dinputs = np.sign(y_true - dvalues)/ outputs\n",
    "        \n",
    "        self.dinputs = self.dinputs / samples\n",
    "         \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "    \n",
    "    def forward(self, y_pred, y_target):\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        # clipping the values so we don't end up with inf error\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "\n",
    "        # checking if 1D array so we just need to get that index value\n",
    "        if len(y_target.shape) == 1:\n",
    "            correct = y_pred_clipped[range(samples), y_target]\n",
    "            \n",
    "        # checking if one-hot-encoded values, then multiplying and summing up\n",
    "        elif len(y_target.shape) == 2:\n",
    "            correct = np.sum(y_pred_clipped * y_target, axis=1)\n",
    "        \n",
    "        negative_log = -np.log(correct)\n",
    "        \n",
    "        return negative_log\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "        \n",
    "        if len(y_true.shape) ==1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "            \n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples\n",
    "    \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Accuracy:\n",
    "    \n",
    "    def calculate(self, predictions, y):\n",
    "        \n",
    "        comparisons = self.compare(predictions, y)\n",
    "        \n",
    "        accuracy = np.mean(comparisons)\n",
    "        \n",
    "        self.accumulated_sum += np.sum(comparisons)\n",
    "        self.accumulated_count += len(comparisons)\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def calculate_accumulated(self):\n",
    "        \n",
    "        accuracy = self.accumulated_sum / self.accumulated_count\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def new_pass(self):\n",
    "        self.accumulated_sum = 0\n",
    "        self.accumulated_count = 0\n",
    "    \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Accuracy_Regression(Accuracy):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.precision = None\n",
    "        \n",
    "    def init(self, y, reinit= False):\n",
    "        if self.precision is None or reinit:\n",
    "            self.precision = np.std(y) / 250\n",
    "            \n",
    "    def compare(self, predictions, y):\n",
    "        return np.absolute(predictions - y) < self.precision\n",
    "        \n",
    "\n",
    "        \n",
    "class Accuracy_Categorical(Accuracy):\n",
    "    \n",
    "    def init(self, y):\n",
    "        pass\n",
    "    \n",
    "    def compare(self, predictions, y):\n",
    "        if len(y.shape) ==2:\n",
    "            y = np.argmax(y, axis=1)\n",
    "            \n",
    "            \n",
    "        return predictions == y\n",
    "        \n",
    "\n",
    "#.......................................................................\n",
    "\n",
    "class Activation_Softmax_Loss_CategoricalCrossEntropy():\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        samples = len(dvalues)\n",
    "        \n",
    "#         # checking if the values are one-hot-encoded\n",
    "#         if len(y_true.shape) ==2:\n",
    "#             y_true = np.argmax(y_true, axis=1)\n",
    "            \n",
    "        \n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "        # calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -=1\n",
    "        \n",
    "        # normalizing gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "    \n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Loss_BinaryCrossentropy(Loss):\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        sample_losses = -(y_true * np.log(y_pred_clipped) + (1-y_true) * np.log(1- y_pred_clipped))\n",
    "        \n",
    "        sample_losses = np.mean(sample_losses, axis = -1)\n",
    "        \n",
    "        return sample_losses\n",
    "    \n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        self.dinputs = -(y_true / clipped_dvalues - (1-y_true) / (1-clipped_dvalues))/ outputs\n",
    "        \n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "813353ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class Model:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # creating a list of network objects\n",
    "        self.layers = []\n",
    "        self.softmax_classifier_output = None\n",
    "        \n",
    "    # adding layers to the model\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    # set loss and optimizer\n",
    "    def set(self, * , loss=None, optimizer=None, accuracy=None):\n",
    "        if loss:\n",
    "            self.loss = loss\n",
    "        if optimizer:\n",
    "            self.optimizer = optimizer\n",
    "        if accuracy:\n",
    "            self.accuracy = accuracy\n",
    "        \n",
    "    # Train the model\n",
    "    def train(self, X, y, *, epochs=1, batch_size=None, print_every=1, validation_data = None):\n",
    "        \n",
    "        self.accuracy.init(y)\n",
    "        \n",
    "        train_steps = 1\n",
    "        \n",
    "        if validation_data :\n",
    "            validation_steps =1\n",
    "            \n",
    "            X_val, y_val = validation_data\n",
    "            \n",
    "        \n",
    "        if batch_size is not None:\n",
    "            \n",
    "            train_steps = len(X) // batch_size\n",
    "            \n",
    "            if train_steps * batch_size < len(X):\n",
    "                train_steps +=1\n",
    "                \n",
    "            if validation_data is not None:\n",
    "                validation_steps = len(X_val) // batch_size\n",
    "                \n",
    "                if validation_steps * batch_size < len(X_val):\n",
    "                    validation_steps +=1\n",
    "        \n",
    "        \n",
    "        for epoch in range(1, epochs+1):\n",
    "            \n",
    "            print(\"Epoch :\" , epoch)\n",
    "            \n",
    "            # reset accumulated values in loss and accuracy objects\n",
    "            self.loss.new_pass()\n",
    "            self.accuracy.new_pass()\n",
    "            \n",
    "            for step in range(train_steps):\n",
    "                \n",
    "                # if batch is not set using the full dataset \n",
    "                if batch_size is None:\n",
    "                    batch_X = X\n",
    "                    batch_y = y\n",
    "                \n",
    "                # otherwise slice the dataset into batches\n",
    "                else:\n",
    "                    batch_X = X[step * batch_size:(step+1) * batch_size]\n",
    "                    batch_y = y[step * batch_size:(step+1) * batch_size]\n",
    "            \n",
    "                output = self.forward(batch_X, training=True)\n",
    "\n",
    "                data_loss, regularization_loss = self.loss.calculate(output, batch_y, include_regularization=True)\n",
    "\n",
    "                loss = data_loss + regularization_loss\n",
    "\n",
    "                predictions = self.output_layer_activation.predictions(output)\n",
    "\n",
    "                accuracy = self.accuracy.calculate(predictions, batch_y)\n",
    "\n",
    "                self.backward(output, batch_y)\n",
    "                \n",
    "                self.optimizer.pre_update_parameters()\n",
    "\n",
    "                for layer in self.trainable_layers:\n",
    "                    self.optimizer.update_params(layer)\n",
    "\n",
    "                self.optimizer.post_update_parameters()\n",
    "\n",
    "                if not step % print_every or step == train_steps - 1:\n",
    "                    print(f\"step: {step}, accuracy: {accuracy}, loss: {loss}, data_loss: {data_loss}, lr: {self.optimizer.current_learning_rate}\")\n",
    "                    \n",
    "                    \n",
    "        epoch_data_loss, epoch_regularization_loss = self.loss.calculate_accumulated(include_regularization = True)\n",
    "        \n",
    "        epoch_loss = epoch_data_loss + epoch_regularization_loss\n",
    "        epoch_accuracy = self.accuracy.calculate_accumulated()\n",
    "        \n",
    "        \n",
    "        print(f\"Training, acc: {epoch_accuracy}, loss: {epoch_loss}, data_loss: {epoch_data_loss}, lr: {self.optimizer.current_learning_rate} \")\n",
    "        \n",
    "        if validation_data is not None:\n",
    "            \n",
    "            self.evaluate(*validation_data, batch_size = batch_size)\n",
    "\n",
    "    # finalize the model\n",
    "    def finalize(self):\n",
    "        \n",
    "        #create and set the input layer\n",
    "        self.input_layer = Layer_Input()\n",
    "        \n",
    "        # count all objects\n",
    "        layer_count = len(self.layers)\n",
    "        \n",
    "        # initializing a list to store trainable layers\n",
    "        self.trainable_layers = []\n",
    "        \n",
    "        for i in range(layer_count):\n",
    "            \n",
    "            if i == 0:\n",
    "                self.layers[i].prev = self.input_layer\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "                \n",
    "            elif i < layer_count -1:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "                \n",
    "            else:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.loss\n",
    "                self.output_layer_activation = self.layers[i]\n",
    "                \n",
    "            # checking if the layer has an attribute weights\n",
    "            # then adding it to the trainable layers list\n",
    "            if hasattr(self.layers[i], \"weights\"):\n",
    "                self.trainable_layers.append(self.layers[i])\n",
    "                \n",
    "            if self.loss is not None:   \n",
    "                self.loss.remember_trainable_layers(self.trainable_layers)\n",
    "            \n",
    "        if isinstance(self.layers[-1], Activation_Softmax) and isinstance(self.loss, Loss_CategoricalCrossEntropy):\n",
    "                self.softmax_classifier_output =  Activation_Softmax_Loss_CategoricalCrossEntropy()\n",
    "            \n",
    "                \n",
    "                \n",
    "    # forward method || performs forward pass\n",
    "    def forward(self, X, training):\n",
    "        \n",
    "        # calling the forward method on the input layer with X data\n",
    "        # this will ensure the next layer or technically the first layer\n",
    "        # gets the data it needs from the previous layer\n",
    "        self.input_layer.forward(X, training)\n",
    "        \n",
    "        # looping over the layers and passing the previous layer output to each layer\n",
    "        for layer in self.layers:\n",
    "            layer.forward(layer.prev.output, training)\n",
    "            \n",
    "        # returning the latest layer that is the last activation function output\n",
    "        return layer.output\n",
    "    \n",
    "    \n",
    "    # performs backward pass\n",
    "    def backward(self, output, y):\n",
    "        \n",
    "        self.loss.backward(output, y)\n",
    "        \n",
    "        for layer in reversed(self.layers):\n",
    "            layer.backward(layer.next.dinputs)\n",
    "            \n",
    "            \n",
    "    # evaluate the model using passed in dataset\n",
    "    def evaluate(self, X_val, y_val, *, batch_size=None):\n",
    "        \n",
    "        validation_steps = 1\n",
    "        \n",
    "        if batch_size:\n",
    "            \n",
    "            validation_steps = len(X_val) // batch_size\n",
    "            \n",
    "            if validation_steps * batch_size < len(X_val):\n",
    "                validation_steps +=1\n",
    "        \n",
    "        self.loss.new_pass()\n",
    "        self.accuracy.new_pass()\n",
    "\n",
    "        for step in range(validation_steps):\n",
    "\n",
    "            if not batch_size:\n",
    "                batch_X = X_val\n",
    "                batch_y = y_val\n",
    "\n",
    "            else:\n",
    "                batch_X = X_val[step * batch_size:(step+1)* batch_size]\n",
    "                batch_y = y_val[step * batch_size:(step+1)* batch_size]\n",
    "\n",
    "        output = self.forward(batch_X,training= False)\n",
    "\n",
    "        loss = self.loss.calculate(output, batch_y)\n",
    "\n",
    "        predictions = self.output_layer_activation.predictions(output)\n",
    "\n",
    "        self.accuracy.calculate(predictions, batch_y)\n",
    "\n",
    "        validation_loss = self.loss.calculate_accumulated()\n",
    "        validation_accuracy = self.accuracy.calculate_accumulated()\n",
    "\n",
    "        print(f\"validation || acc : {validation_accuracy} , loss : {validation_loss}\")\n",
    "\n",
    "        \n",
    "    # retrieves and returns the parameters of trainable layers\n",
    "    def get_parameters(self):\n",
    "        \n",
    "        # list of parameters\n",
    "        parameters = []\n",
    "        \n",
    "        # looping over the trainable layers or the dense layers and storing\n",
    "        # their parameters\n",
    "        for layer in self.trainable_layers:\n",
    "            parameters.append(layer.get_parameters())\n",
    "            \n",
    "        return parameters\n",
    "\n",
    "    def set_parameters(self, parameters):\n",
    "        for parameter_set, layer in zip(parameters, self.trainable_layers):\n",
    "            layer.set_parameters(*parameter_set)\n",
    "            \n",
    "    def save_parameters(self, path):\n",
    "        \n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(self.get_parameters(), f)\n",
    "            \n",
    "            \n",
    "    def load_patameters(self, path):\n",
    "        with open(path, 'rb') as f:\n",
    "            self.set_parameters(pickle.load(f))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "61994c5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1\n",
      "step: 0, accuracy: 0.09375, loss: 2.3024964332580566, data_loss: 2.3024964332580566, lr: 0.001\n",
      "step: 100, accuracy: 0.75, loss: 0.6230136156082153, data_loss: 0.6230136156082153, lr: 0.0009900990099009901\n",
      "step: 200, accuracy: 0.8203125, loss: 0.4880841374397278, data_loss: 0.4880841374397278, lr: 0.000980392156862745\n",
      "step: 300, accuracy: 0.7890625, loss: 0.5615738034248352, data_loss: 0.5615738034248352, lr: 0.0009708737864077671\n",
      "step: 400, accuracy: 0.8125, loss: 0.4772418737411499, data_loss: 0.4772418737411499, lr: 0.0009615384615384615\n",
      "step: 468, accuracy: 0.8541666666666666, loss: 0.3476001024246216, data_loss: 0.3476001024246216, lr: 0.0009552923194497518\n",
      "Epoch : 2\n",
      "step: 0, accuracy: 0.8515625, loss: 0.41050422191619873, data_loss: 0.41050422191619873, lr: 0.0009552010698251983\n",
      "step: 100, accuracy: 0.796875, loss: 0.48743391036987305, data_loss: 0.48743391036987305, lr: 0.0009461633077869241\n",
      "step: 200, accuracy: 0.8671875, loss: 0.3416329622268677, data_loss: 0.3416329622268677, lr: 0.0009372949667260287\n",
      "step: 300, accuracy: 0.875, loss: 0.44579148292541504, data_loss: 0.44579148292541504, lr: 0.0009285913269570063\n",
      "step: 400, accuracy: 0.8125, loss: 0.41347819566726685, data_loss: 0.41347819566726685, lr: 0.0009200478424878093\n",
      "step: 468, accuracy: 0.875, loss: 0.2943856716156006, data_loss: 0.2943856716156006, lr: 0.0009143275121148394\n",
      "Epoch : 3\n",
      "step: 0, accuracy: 0.8359375, loss: 0.39114680886268616, data_loss: 0.39114680886268616, lr: 0.0009142439202779302\n",
      "step: 100, accuracy: 0.828125, loss: 0.4276655316352844, data_loss: 0.4276655316352844, lr: 0.0009059612248595759\n",
      "step: 200, accuracy: 0.8828125, loss: 0.30033016204833984, data_loss: 0.30033016204833984, lr: 0.0008978272580355541\n",
      "step: 300, accuracy: 0.8828125, loss: 0.42390334606170654, data_loss: 0.42390334606170654, lr: 0.0008898380494749957\n",
      "step: 400, accuracy: 0.8203125, loss: 0.38849467039108276, data_loss: 0.38849467039108276, lr: 0.0008819897689186807\n",
      "step: 468, accuracy: 0.8854166666666666, loss: 0.26204657554626465, data_loss: 0.26204657554626465, lr: 0.000876731544800982\n",
      "Epoch : 4\n",
      "step: 0, accuracy: 0.8359375, loss: 0.3510820269584656, data_loss: 0.3510820269584656, lr: 0.0008766546857192952\n",
      "step: 100, accuracy: 0.8359375, loss: 0.3743671774864197, data_loss: 0.3743671774864197, lr: 0.0008690362388111585\n",
      "step: 200, accuracy: 0.8828125, loss: 0.269037127494812, data_loss: 0.269037127494812, lr: 0.0008615490652192642\n",
      "step: 300, accuracy: 0.859375, loss: 0.3982892334461212, data_loss: 0.3982892334461212, lr: 0.0008541898009737763\n",
      "step: 400, accuracy: 0.8359375, loss: 0.3707309067249298, data_loss: 0.3707309067249298, lr: 0.0008469551960701278\n",
      "step: 468, accuracy: 0.9166666666666666, loss: 0.23108774423599243, data_loss: 0.23108774423599243, lr: 0.0008421052631578947\n",
      "Epoch : 5\n",
      "step: 0, accuracy: 0.8359375, loss: 0.32630455493927, data_loss: 0.32630455493927, lr: 0.0008420343550016842\n",
      "step: 100, accuracy: 0.84375, loss: 0.3473011255264282, data_loss: 0.3473011255264282, lr: 0.0008350033400133601\n",
      "step: 200, accuracy: 0.8984375, loss: 0.2561999559402466, data_loss: 0.2561999559402466, lr: 0.0008280887711162637\n",
      "step: 300, accuracy: 0.8828125, loss: 0.3388643264770508, data_loss: 0.3388643264770508, lr: 0.0008212877792378449\n",
      "step: 400, accuracy: 0.84375, loss: 0.35547077655792236, data_loss: 0.35547077655792236, lr: 0.0008145975887911372\n",
      "step: 468, accuracy: 0.9166666666666666, loss: 0.2068738490343094, data_loss: 0.2068738490343094, lr: 0.0008101101749837978\n",
      "Epoch : 6\n",
      "step: 0, accuracy: 0.84375, loss: 0.3016948103904724, data_loss: 0.3016948103904724, lr: 0.0008100445524503849\n",
      "step: 100, accuracy: 0.8671875, loss: 0.3164799213409424, data_loss: 0.3164799213409424, lr: 0.0008035355564483729\n",
      "step: 200, accuracy: 0.90625, loss: 0.25290411710739136, data_loss: 0.25290411710739136, lr: 0.0007971303308090873\n",
      "step: 300, accuracy: 0.8984375, loss: 0.302079439163208, data_loss: 0.302079439163208, lr: 0.0007908264136022144\n",
      "step: 400, accuracy: 0.859375, loss: 0.3386179506778717, data_loss: 0.3386179506778717, lr: 0.0007846214201647706\n",
      "step: 468, accuracy: 0.9166666666666666, loss: 0.20126225054264069, data_loss: 0.20126225054264069, lr: 0.0007804573480059316\n",
      "Epoch : 7\n",
      "step: 0, accuracy: 0.8671875, loss: 0.27602654695510864, data_loss: 0.27602654695510864, lr: 0.0007803964413922272\n",
      "step: 100, accuracy: 0.8828125, loss: 0.29181647300720215, data_loss: 0.29181647300720215, lr: 0.0007743534148985598\n",
      "step: 200, accuracy: 0.9296875, loss: 0.23767603933811188, data_loss: 0.23767603933811188, lr: 0.0007684032580298141\n",
      "step: 300, accuracy: 0.890625, loss: 0.2900643050670624, data_loss: 0.2900643050670624, lr: 0.0007625438462711606\n",
      "step: 400, accuracy: 0.8671875, loss: 0.3274562954902649, data_loss: 0.3274562954902649, lr: 0.0007567731194187983\n",
      "step: 468, accuracy: 0.9166666666666666, loss: 0.18301792442798615, data_loss: 0.18301792442798615, lr: 0.0007528986598403856\n",
      "Epoch : 8\n",
      "step: 0, accuracy: 0.875, loss: 0.2613942623138428, data_loss: 0.2613942623138428, lr: 0.0007528419784687194\n",
      "step: 100, accuracy: 0.90625, loss: 0.26817381381988525, data_loss: 0.26817381381988525, lr: 0.0007472166180975864\n",
      "step: 200, accuracy: 0.9296875, loss: 0.22459819912910461, data_loss: 0.22459819912910461, lr: 0.0007416747014759327\n",
      "step: 300, accuracy: 0.90625, loss: 0.26772427558898926, data_loss: 0.26772427558898926, lr: 0.0007362143856290952\n",
      "step: 400, accuracy: 0.859375, loss: 0.313853919506073, data_loss: 0.313853919506073, lr: 0.0007308338814587444\n",
      "step: 468, accuracy: 0.9166666666666666, loss: 0.17470140755176544, data_loss: 0.17470140755176544, lr: 0.0007272198385571959\n",
      "Epoch : 9\n",
      "step: 0, accuracy: 0.890625, loss: 0.2474835067987442, data_loss: 0.2474835067987442, lr: 0.0007271669575334498\n",
      "step: 100, accuracy: 0.9296875, loss: 0.24790289998054504, data_loss: 0.24790289998054504, lr: 0.000721917412647993\n",
      "step: 200, accuracy: 0.9296875, loss: 0.20822086930274963, data_loss: 0.20822086930274963, lr: 0.0007167431192660551\n",
      "step: 300, accuracy: 0.921875, loss: 0.23813371360301971, data_loss: 0.23813371360301971, lr: 0.0007116424708226587\n",
      "step: 400, accuracy: 0.8671875, loss: 0.30934572219848633, data_loss: 0.30934572219848633, lr: 0.0007066139061616733\n",
      "step: 468, accuracy: 0.9270833333333334, loss: 0.17458398640155792, data_loss: 0.17458398640155792, lr: 0.0007032348804500702\n",
      "Epoch : 10\n",
      "step: 0, accuracy: 0.90625, loss: 0.23500999808311462, data_loss: 0.23500999808311462, lr: 0.0007031854299978904\n",
      "step: 100, accuracy: 0.9453125, loss: 0.23484516143798828, data_loss: 0.23484516143798828, lr: 0.0006982752601075343\n",
      "step: 200, accuracy: 0.9453125, loss: 0.19019435346126556, data_loss: 0.19019435346126556, lr: 0.000693433187712364\n",
      "step: 300, accuracy: 0.921875, loss: 0.23830437660217285, data_loss: 0.23830437660217285, lr: 0.0006886578059362303\n",
      "step: 400, accuracy: 0.859375, loss: 0.2976591885089874, data_loss: 0.2976591885089874, lr: 0.0006839477463921757\n",
      "step: 468, accuracy: 0.9270833333333334, loss: 0.17308972775936127, data_loss: 0.17308972775936127, lr: 0.000680781537204711\n",
      "Training, acc: 0.9077666666666667, loss: 0.25265609679222106, data_loss: 0.25265609679222106, lr: 0.000680781537204711 \n",
      "validation || acc : 0.96875 , loss : 0.09934473037719727\n",
      "validation || acc : 0.91055 , loss : 0.24055826822916668\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "\n",
    "model.add(Layer_Dense(X.shape[1], 128))\n",
    "model.add(Activation_Relu())\n",
    "model.add(Layer_Dense(128,128))\n",
    "model.add(Activation_Relu())\n",
    "model.add(Layer_Dense(128, 10))\n",
    "model.add(Activation_Softmax())\n",
    "\n",
    "model.set(loss=Loss_CategoricalCrossEntropy(), optimizer= Optimizer_Adam(decay=1e-4), accuracy= Accuracy_Categorical())\n",
    "\n",
    "model.finalize()\n",
    "\n",
    "model.train(X,y, validation_data=(X_test, y_test), epochs=10,  batch_size=128, print_every=100)\n",
    "\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e34b3b82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array([[-0.03946078,  0.03837414, -0.02135864, ...,  0.01119648,\n",
      "        -0.00555523,  0.02385409],\n",
      "       [-0.04027164,  0.03780807, -0.01047406, ...,  0.03286309,\n",
      "         0.00940319,  0.00955247],\n",
      "       [-0.04077249,  0.02484431, -0.02417967, ...,  0.02751668,\n",
      "        -0.00067079,  0.0169488 ],\n",
      "       ...,\n",
      "       [-0.05137451,  0.02041244,  0.02438609, ...,  0.04216436,\n",
      "         0.04135995,  0.01238294],\n",
      "       [-0.04309473,  0.02510377,  0.02106452, ...,  0.01386279,\n",
      "         0.03239899,  0.01778853],\n",
      "       [-0.04167083,  0.0339663 , -0.00555694, ...,  0.01847898,\n",
      "         0.01010656,  0.01332747]], dtype=float32), array([[ 0.03966366, -0.04056697,  0.01498523, -0.0360437 , -0.01053787,\n",
      "         0.00547004, -0.01001751,  0.01064655, -0.00336151,  0.01958312,\n",
      "        -0.01899779,  0.04203601,  0.06543857, -0.01419953, -0.00294141,\n",
      "        -0.00222431, -0.03363403, -0.00387943, -0.02602347,  0.02975911,\n",
      "         0.00440716,  0.03404903, -0.01728227,  0.02588595, -0.02147042,\n",
      "         0.00331848, -0.00780834, -0.0365137 , -0.01032323, -0.0361524 ,\n",
      "        -0.0079414 ,  0.04184762, -0.05373289,  0.00673768, -0.05062092,\n",
      "        -0.00032856, -0.01844006, -0.00105482, -0.0102665 , -0.00773638,\n",
      "         0.04117533,  0.10003332,  0.01644557, -0.04428697, -0.04021017,\n",
      "        -0.0387345 , -0.04445177, -0.00082969,  0.0174196 ,  0.01730705,\n",
      "        -0.03000082, -0.01441258, -0.01864785, -0.0357992 , -0.03954402,\n",
      "         0.00760621, -0.03479451,  0.09814713,  0.00200768, -0.0052791 ,\n",
      "        -0.01574694, -0.05704316,  0.03011312, -0.02891376, -0.00471005,\n",
      "        -0.01545577,  0.02182856, -0.01317962, -0.01517383,  0.03136558,\n",
      "        -0.02137809,  0.01615965, -0.01863471,  0.00286166, -0.03713626,\n",
      "        -0.01704469,  0.01243225, -0.0383151 , -0.00162709,  0.01804574,\n",
      "         0.0205775 , -0.00745611,  0.00625273,  0.00990883,  0.01205886,\n",
      "         0.01129166, -0.00861516, -0.0331999 ,  0.00128852, -0.00414885,\n",
      "        -0.0424601 , -0.01474984, -0.04180844, -0.04218933,  0.00224973,\n",
      "         0.00671063,  0.01529212, -0.00324976,  0.02720116, -0.01456286,\n",
      "        -0.00888645,  0.05000176, -0.0010708 , -0.07107928,  0.03680115,\n",
      "        -0.00336695, -0.03135722, -0.01502577,  0.04206159,  0.03695793,\n",
      "        -0.01115817, -0.00334528, -0.0096134 , -0.00435616, -0.02514077,\n",
      "        -0.03702348, -0.01560394, -0.01584214,  0.05460418, -0.0143157 ,\n",
      "        -0.01703763, -0.02642287, -0.01249524,  0.01739122, -0.01744764,\n",
      "        -0.02603904, -0.01182375, -0.01553276]], dtype=float32)), (array([[-0.10559503,  0.1114011 , -0.06098643, ..., -0.0231583 ,\n",
      "        -0.01278379, -0.07965362],\n",
      "       [ 0.13086347, -0.06546075,  0.05909456, ..., -0.00905635,\n",
      "        -0.01756333,  0.0197538 ],\n",
      "       [ 0.00168071, -0.01037341, -0.01416547, ..., -0.03248274,\n",
      "        -0.08917873, -0.2172313 ],\n",
      "       ...,\n",
      "       [ 0.050077  ,  0.00072151, -0.11543705, ...,  0.00157676,\n",
      "         0.09013988, -0.08153535],\n",
      "       [-0.1954018 ,  0.05646558, -0.05360391, ..., -0.02312384,\n",
      "        -0.08674541, -0.06961655],\n",
      "       [ 0.00555166,  0.08825196, -0.01198822, ..., -0.02938693,\n",
      "        -0.05400608, -0.06515769]], dtype=float32), array([[-0.00586863,  0.02481111,  0.02142921, -0.01721613,  0.07096662,\n",
      "        -0.0081834 , -0.00935373,  0.03793865,  0.05198844,  0.03950665,\n",
      "         0.01670776,  0.00228672, -0.00627347, -0.01595791,  0.04099626,\n",
      "        -0.00124109, -0.00599901, -0.00340556,  0.03718255,  0.02819112,\n",
      "         0.07182597, -0.0223428 ,  0.02254241, -0.03078259,  0.00633517,\n",
      "        -0.02414699,  0.00270262,  0.01677839,  0.01485289,  0.04878037,\n",
      "        -0.00774269,  0.03559437, -0.00044247, -0.00599994,  0.02048506,\n",
      "        -0.00770906,  0.06733339, -0.00694594,  0.03216407,  0.02886458,\n",
      "         0.05098709,  0.02008709, -0.02147506,  0.05595235, -0.00638547,\n",
      "         0.01450792,  0.02878696, -0.01454169,  0.05421418,  0.06949043,\n",
      "        -0.01156074,  0.05789369, -0.0475137 , -0.03490883,  0.00318734,\n",
      "        -0.00275125, -0.00606196,  0.03157691,  0.04416461,  0.0047226 ,\n",
      "         0.0347769 , -0.0104362 ,  0.01478861, -0.00697421, -0.01542234,\n",
      "         0.03526771,  0.01012415,  0.06711078, -0.00853594, -0.0091748 ,\n",
      "         0.00421693,  0.03811219,  0.01436233,  0.02588391, -0.00325222,\n",
      "         0.00568505,  0.06134586, -0.006982  , -0.03012558, -0.01932976,\n",
      "         0.04940528,  0.00160809,  0.02817001, -0.02151172,  0.0198911 ,\n",
      "        -0.00599883, -0.00838999, -0.01470845, -0.00854379, -0.00652775,\n",
      "         0.0378982 ,  0.01935507, -0.03024034,  0.03309172, -0.00599466,\n",
      "         0.00043633,  0.00907242,  0.01171416,  0.014479  , -0.01211812,\n",
      "         0.00775624,  0.02709324,  0.04225402,  0.04778194, -0.00599938,\n",
      "         0.01278912,  0.05615862,  0.00632628,  0.05804732, -0.00914345,\n",
      "         0.02758397, -0.02356446, -0.01069491,  0.03795568,  0.03681185,\n",
      "        -0.01257592, -0.00485167,  0.00475619,  0.03075507, -0.00190514,\n",
      "        -0.03951925,  0.00895386, -0.00840211, -0.00733298, -0.00184457,\n",
      "        -0.01775043,  0.02236086,  0.01808698]], dtype=float32)), (array([[-0.01030906,  0.05069389, -0.12207469, ..., -0.16756223,\n",
      "         0.02977622, -0.14418267],\n",
      "       [ 0.04456325, -0.0956431 ,  0.02620798, ..., -0.120056  ,\n",
      "        -0.01605808, -0.20716788],\n",
      "       [ 0.03172839, -0.09951159, -0.10188608, ...,  0.06052406,\n",
      "         0.11169796, -0.08598892],\n",
      "       ...,\n",
      "       [-0.03405806,  0.00543167, -0.02928524, ..., -0.00103566,\n",
      "         0.03140658, -0.0122682 ],\n",
      "       [-0.0276511 , -0.11488529, -0.00942936, ...,  0.01584849,\n",
      "         0.03347573,  0.0384221 ],\n",
      "       [-0.21753024,  0.06263594, -0.05718414, ...,  0.05676274,\n",
      "        -0.14857897,  0.06424662]], dtype=float32), array([[-0.03869005, -0.00795385,  0.0262191 , -0.02065309, -0.02018616,\n",
      "        -0.01481227,  0.03448509, -0.02226435,  0.11576479, -0.03085169]],\n",
      "      dtype=float32))]\n"
     ]
    }
   ],
   "source": [
    "parameters  = model.get_parameters()\n",
    "print(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5da51e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation || acc : 0.91055 , loss : 0.24055826822916668\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "\n",
    "model.add(Layer_Dense(X.shape[1], 128))\n",
    "model.add(Activation_Relu())\n",
    "model.add(Layer_Dense(128,128))\n",
    "model.add(Activation_Relu())\n",
    "model.add(Layer_Dense(128, 10))\n",
    "model.add(Activation_Softmax())\n",
    "\n",
    "model.set(loss=Loss_CategoricalCrossEntropy(), accuracy= Accuracy_Categorical())\n",
    "\n",
    "model.finalize()\n",
    "\n",
    "# Setting the parameters from the previous run\n",
    "model.set_parameters(parameters)\n",
    "\n",
    "# don't need to train the model again as we have prebuilt parameters\n",
    "# evaluating the model \n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd25008",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7790674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import copy\n",
    "\n",
    "class Model:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # creating a list of network objects\n",
    "        self.layers = []\n",
    "        self.softmax_classifier_output = None\n",
    "        \n",
    "    # adding layers to the model\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    # set loss and optimizer\n",
    "    def set(self, * , loss=None, optimizer=None, accuracy=None):\n",
    "        if loss:\n",
    "            self.loss = loss\n",
    "        if optimizer:\n",
    "            self.optimizer = optimizer\n",
    "        if accuracy:\n",
    "            self.accuracy = accuracy\n",
    "        \n",
    "    # Train the model\n",
    "    def train(self, X, y, *, epochs=1, batch_size=None, print_every=1, validation_data = None):\n",
    "        \n",
    "        self.accuracy.init(y)\n",
    "        \n",
    "        train_steps = 1\n",
    "        \n",
    "        if validation_data :\n",
    "            validation_steps =1\n",
    "            \n",
    "            X_val, y_val = validation_data\n",
    "            \n",
    "        \n",
    "        if batch_size is not None:\n",
    "            \n",
    "            train_steps = len(X) // batch_size\n",
    "            \n",
    "            if train_steps * batch_size < len(X):\n",
    "                train_steps +=1\n",
    "                \n",
    "            if validation_data is not None:\n",
    "                validation_steps = len(X_val) // batch_size\n",
    "                \n",
    "                if validation_steps * batch_size < len(X_val):\n",
    "                    validation_steps +=1\n",
    "        \n",
    "        \n",
    "        for epoch in range(1, epochs+1):\n",
    "            \n",
    "            print(\"Epoch :\" , epoch)\n",
    "            \n",
    "            # reset accumulated values in loss and accuracy objects\n",
    "            self.loss.new_pass()\n",
    "            self.accuracy.new_pass()\n",
    "            \n",
    "            for step in range(train_steps):\n",
    "                \n",
    "                # if batch is not set using the full dataset \n",
    "                if batch_size is None:\n",
    "                    batch_X = X\n",
    "                    batch_y = y\n",
    "                \n",
    "                # otherwise slice the dataset into batches\n",
    "                else:\n",
    "                    batch_X = X[step * batch_size:(step+1) * batch_size]\n",
    "                    batch_y = y[step * batch_size:(step+1) * batch_size]\n",
    "            \n",
    "                output = self.forward(batch_X, training=True)\n",
    "\n",
    "                data_loss, regularization_loss = self.loss.calculate(output, batch_y, include_regularization=True)\n",
    "\n",
    "                loss = data_loss + regularization_loss\n",
    "\n",
    "                predictions = self.output_layer_activation.predictions(output)\n",
    "\n",
    "                accuracy = self.accuracy.calculate(predictions, batch_y)\n",
    "\n",
    "                self.backward(output, batch_y)\n",
    "                \n",
    "                self.optimizer.pre_update_parameters()\n",
    "\n",
    "                for layer in self.trainable_layers:\n",
    "                    self.optimizer.update_params(layer)\n",
    "\n",
    "                self.optimizer.post_update_parameters()\n",
    "\n",
    "                if not step % print_every or step == train_steps - 1:\n",
    "                    print(f\"step: {step}, accuracy: {accuracy}, loss: {loss}, data_loss: {data_loss}, lr: {self.optimizer.current_learning_rate}\")\n",
    "                    \n",
    "                    \n",
    "        epoch_data_loss, epoch_regularization_loss = self.loss.calculate_accumulated(include_regularization = True)\n",
    "        \n",
    "        epoch_loss = epoch_data_loss + epoch_regularization_loss\n",
    "        epoch_accuracy = self.accuracy.calculate_accumulated()\n",
    "        \n",
    "        \n",
    "        print(f\"Training, acc: {epoch_accuracy}, loss: {epoch_loss}, data_loss: {epoch_data_loss}, lr: {self.optimizer.current_learning_rate} \")\n",
    "        \n",
    "        if validation_data is not None:\n",
    "            \n",
    "            self.evaluate(*validation_data, batch_size = batch_size)\n",
    "\n",
    "    # finalize the model\n",
    "    def finalize(self):\n",
    "        \n",
    "        #create and set the input layer\n",
    "        self.input_layer = Layer_Input()\n",
    "        \n",
    "        # count all objects\n",
    "        layer_count = len(self.layers)\n",
    "        \n",
    "        # initializing a list to store trainable layers\n",
    "        self.trainable_layers = []\n",
    "        \n",
    "        for i in range(layer_count):\n",
    "            \n",
    "            if i == 0:\n",
    "                self.layers[i].prev = self.input_layer\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "                \n",
    "            elif i < layer_count -1:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "                \n",
    "            else:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.loss\n",
    "                self.output_layer_activation = self.layers[i]\n",
    "                \n",
    "            # checking if the layer has an attribute weights\n",
    "            # then adding it to the trainable layers list\n",
    "            if hasattr(self.layers[i], \"weights\"):\n",
    "                self.trainable_layers.append(self.layers[i])\n",
    "                \n",
    "            if self.loss is not None:   \n",
    "                self.loss.remember_trainable_layers(self.trainable_layers)\n",
    "            \n",
    "        if isinstance(self.layers[-1], Activation_Softmax) and isinstance(self.loss, Loss_CategoricalCrossEntropy):\n",
    "                self.softmax_classifier_output =  Activation_Softmax_Loss_CategoricalCrossEntropy()\n",
    "            \n",
    "                \n",
    "                \n",
    "    # forward method || performs forward pass\n",
    "    def forward(self, X, training):\n",
    "        \n",
    "        # calling the forward method on the input layer with X data\n",
    "        # this will ensure the next layer or technically the first layer\n",
    "        # gets the data it needs from the previous layer\n",
    "        self.input_layer.forward(X, training)\n",
    "        \n",
    "        # looping over the layers and passing the previous layer output to each layer\n",
    "        for layer in self.layers:\n",
    "            layer.forward(layer.prev.output, training)\n",
    "            \n",
    "        # returning the latest layer that is the last activation function output\n",
    "        return layer.output\n",
    "    \n",
    "    \n",
    "    # performs backward pass\n",
    "    def backward(self, output, y):\n",
    "        \n",
    "        self.loss.backward(output, y)\n",
    "        \n",
    "        for layer in reversed(self.layers):\n",
    "            layer.backward(layer.next.dinputs)\n",
    "            \n",
    "            \n",
    "    # evaluate the model using passed in dataset\n",
    "    def evaluate(self, X_val, y_val, *, batch_size=None):\n",
    "        \n",
    "        validation_steps = 1\n",
    "        \n",
    "        if batch_size:\n",
    "            \n",
    "            validation_steps = len(X_val) // batch_size\n",
    "            \n",
    "            if validation_steps * batch_size < len(X_val):\n",
    "                validation_steps +=1\n",
    "        \n",
    "        self.loss.new_pass()\n",
    "        self.accuracy.new_pass()\n",
    "\n",
    "        for step in range(validation_steps):\n",
    "\n",
    "            if not batch_size:\n",
    "                batch_X = X_val\n",
    "                batch_y = y_val\n",
    "\n",
    "            else:\n",
    "                batch_X = X_val[step * batch_size:(step+1)* batch_size]\n",
    "                batch_y = y_val[step * batch_size:(step+1)* batch_size]\n",
    "\n",
    "        output = self.forward(batch_X,training= False)\n",
    "\n",
    "        loss = self.loss.calculate(output, batch_y)\n",
    "\n",
    "        predictions = self.output_layer_activation.predictions(output)\n",
    "\n",
    "        self.accuracy.calculate(predictions, batch_y)\n",
    "\n",
    "        validation_loss = self.loss.calculate_accumulated()\n",
    "        validation_accuracy = self.accuracy.calculate_accumulated()\n",
    "\n",
    "        print(f\"validation || acc : {validation_accuracy} , loss : {validation_loss}\")\n",
    "\n",
    "        \n",
    "    # retrieves and returns the parameters of trainable layers\n",
    "    def get_parameters(self):\n",
    "        \n",
    "        # list of parameters\n",
    "        parameters = []\n",
    "        \n",
    "        # looping over the trainable layers or the dense layers and storing\n",
    "        # their parameters\n",
    "        for layer in self.trainable_layers:\n",
    "            parameters.append(layer.get_parameters())\n",
    "            \n",
    "        return parameters\n",
    "\n",
    "    def set_parameters(self, parameters):\n",
    "        for parameter_set, layer in zip(parameters, self.trainable_layers):\n",
    "            layer.set_parameters(*parameter_set)\n",
    "            \n",
    "    def save_parameters(self, path):\n",
    "        \n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(self.get_parameters(), f)\n",
    "            \n",
    "            \n",
    "    def load_patameters(self, path):\n",
    "        with open(path, 'rb') as f:\n",
    "            self.set_parameters(pickle.load(f))\n",
    "    \n",
    "    def save(self, path):\n",
    "        \n",
    "        # making a deep copy of the current model instance\n",
    "        model = copy.deepcopy(self)\n",
    "        \n",
    "        model.loss.new_pass()\n",
    "        model.accuracy.new_pass()\n",
    "        \n",
    "        model.input_layer.__dict__.pop('output', None)\n",
    "        model.loss.__dict__.pop('dinputs', None)\n",
    "        \n",
    "        for layer in model.layers:\n",
    "            for property in ['inputs', 'output', 'dinputs', 'dweights', 'dbiases']:\n",
    "                \n",
    "                layer.__dict__.pop(property, None)\n",
    "        \n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        \n",
    "        with open(path, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "            \n",
    "        return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3822c9ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1\n",
      "step: 0, accuracy: 0.1875, loss: 2.3022680282592773, data_loss: 2.3022680282592773, lr: 0.001\n",
      "step: 100, accuracy: 0.734375, loss: 0.630014181137085, data_loss: 0.630014181137085, lr: 0.0009900990099009901\n",
      "step: 200, accuracy: 0.8203125, loss: 0.45043158531188965, data_loss: 0.45043158531188965, lr: 0.000980392156862745\n",
      "step: 300, accuracy: 0.796875, loss: 0.5518173575401306, data_loss: 0.5518173575401306, lr: 0.0009708737864077671\n",
      "step: 400, accuracy: 0.828125, loss: 0.44665712118148804, data_loss: 0.44665712118148804, lr: 0.0009615384615384615\n",
      "step: 468, accuracy: 0.875, loss: 0.3246632516384125, data_loss: 0.3246632516384125, lr: 0.0009552923194497518\n",
      "Epoch : 2\n",
      "step: 0, accuracy: 0.8359375, loss: 0.3953976333141327, data_loss: 0.3953976333141327, lr: 0.0009552010698251983\n",
      "step: 100, accuracy: 0.8125, loss: 0.4560910761356354, data_loss: 0.4560910761356354, lr: 0.0009461633077869241\n",
      "step: 200, accuracy: 0.8671875, loss: 0.31984949111938477, data_loss: 0.31984949111938477, lr: 0.0009372949667260287\n",
      "step: 300, accuracy: 0.890625, loss: 0.4515243172645569, data_loss: 0.4515243172645569, lr: 0.0009285913269570063\n",
      "step: 400, accuracy: 0.828125, loss: 0.3896371126174927, data_loss: 0.3896371126174927, lr: 0.0009200478424878093\n",
      "step: 468, accuracy: 0.875, loss: 0.27744320034980774, data_loss: 0.27744320034980774, lr: 0.0009143275121148394\n",
      "Epoch : 3\n",
      "step: 0, accuracy: 0.8359375, loss: 0.36671698093414307, data_loss: 0.36671698093414307, lr: 0.0009142439202779302\n",
      "step: 100, accuracy: 0.8203125, loss: 0.4075939357280731, data_loss: 0.4075939357280731, lr: 0.0009059612248595759\n",
      "step: 200, accuracy: 0.90625, loss: 0.27635276317596436, data_loss: 0.27635276317596436, lr: 0.0008978272580355541\n",
      "step: 300, accuracy: 0.890625, loss: 0.39771854877471924, data_loss: 0.39771854877471924, lr: 0.0008898380494749957\n",
      "step: 400, accuracy: 0.84375, loss: 0.36664655804634094, data_loss: 0.36664655804634094, lr: 0.0008819897689186807\n",
      "step: 468, accuracy: 0.8854166666666666, loss: 0.24137432873249054, data_loss: 0.24137432873249054, lr: 0.000876731544800982\n",
      "Epoch : 4\n",
      "step: 0, accuracy: 0.8515625, loss: 0.3419020473957062, data_loss: 0.3419020473957062, lr: 0.0008766546857192952\n",
      "step: 100, accuracy: 0.8515625, loss: 0.3719860911369324, data_loss: 0.3719860911369324, lr: 0.0008690362388111585\n",
      "step: 200, accuracy: 0.90625, loss: 0.25664475560188293, data_loss: 0.25664475560188293, lr: 0.0008615490652192642\n",
      "step: 300, accuracy: 0.875, loss: 0.36595702171325684, data_loss: 0.36595702171325684, lr: 0.0008541898009737763\n",
      "step: 400, accuracy: 0.859375, loss: 0.34502482414245605, data_loss: 0.34502482414245605, lr: 0.0008469551960701278\n",
      "step: 468, accuracy: 0.90625, loss: 0.21812553703784943, data_loss: 0.21812553703784943, lr: 0.0008421052631578947\n",
      "Epoch : 5\n",
      "step: 0, accuracy: 0.8828125, loss: 0.31448957324028015, data_loss: 0.31448957324028015, lr: 0.0008420343550016842\n",
      "step: 100, accuracy: 0.8671875, loss: 0.32551589608192444, data_loss: 0.32551589608192444, lr: 0.0008350033400133601\n",
      "step: 200, accuracy: 0.8828125, loss: 0.24918094277381897, data_loss: 0.24918094277381897, lr: 0.0008280887711162637\n",
      "step: 300, accuracy: 0.8828125, loss: 0.3301542103290558, data_loss: 0.3301542103290558, lr: 0.0008212877792378449\n",
      "step: 400, accuracy: 0.8515625, loss: 0.32867997884750366, data_loss: 0.32867997884750366, lr: 0.0008145975887911372\n",
      "step: 468, accuracy: 0.90625, loss: 0.20641911029815674, data_loss: 0.20641911029815674, lr: 0.0008101101749837978\n",
      "Epoch : 6\n",
      "step: 0, accuracy: 0.8984375, loss: 0.29758021235466003, data_loss: 0.29758021235466003, lr: 0.0008100445524503849\n",
      "step: 100, accuracy: 0.8984375, loss: 0.28731974959373474, data_loss: 0.28731974959373474, lr: 0.0008035355564483729\n",
      "step: 200, accuracy: 0.90625, loss: 0.2506025433540344, data_loss: 0.2506025433540344, lr: 0.0007971303308090873\n",
      "step: 300, accuracy: 0.8828125, loss: 0.3001222610473633, data_loss: 0.3001222610473633, lr: 0.0007908264136022144\n",
      "step: 400, accuracy: 0.875, loss: 0.31907209753990173, data_loss: 0.31907209753990173, lr: 0.0007846214201647706\n",
      "step: 468, accuracy: 0.90625, loss: 0.19413165748119354, data_loss: 0.19413165748119354, lr: 0.0007804573480059316\n",
      "Epoch : 7\n",
      "step: 0, accuracy: 0.8828125, loss: 0.2855992913246155, data_loss: 0.2855992913246155, lr: 0.0007803964413922272\n",
      "step: 100, accuracy: 0.921875, loss: 0.25348779559135437, data_loss: 0.25348779559135437, lr: 0.0007743534148985598\n",
      "step: 200, accuracy: 0.8984375, loss: 0.2406858652830124, data_loss: 0.2406858652830124, lr: 0.0007684032580298141\n",
      "step: 300, accuracy: 0.890625, loss: 0.2848450541496277, data_loss: 0.2848450541496277, lr: 0.0007625438462711606\n",
      "step: 400, accuracy: 0.890625, loss: 0.31286555528640747, data_loss: 0.31286555528640747, lr: 0.0007567731194187983\n",
      "step: 468, accuracy: 0.9166666666666666, loss: 0.17478828132152557, data_loss: 0.17478828132152557, lr: 0.0007528986598403856\n",
      "Epoch : 8\n",
      "step: 0, accuracy: 0.890625, loss: 0.2754181921482086, data_loss: 0.2754181921482086, lr: 0.0007528419784687194\n",
      "step: 100, accuracy: 0.921875, loss: 0.23191966116428375, data_loss: 0.23191966116428375, lr: 0.0007472166180975864\n",
      "step: 200, accuracy: 0.890625, loss: 0.2383144646883011, data_loss: 0.2383144646883011, lr: 0.0007416747014759327\n",
      "step: 300, accuracy: 0.8984375, loss: 0.2642216980457306, data_loss: 0.2642216980457306, lr: 0.0007362143856290952\n",
      "step: 400, accuracy: 0.890625, loss: 0.308139443397522, data_loss: 0.308139443397522, lr: 0.0007308338814587444\n",
      "step: 468, accuracy: 0.9375, loss: 0.16349202394485474, data_loss: 0.16349202394485474, lr: 0.0007272198385571959\n",
      "Epoch : 9\n",
      "step: 0, accuracy: 0.90625, loss: 0.2597709894180298, data_loss: 0.2597709894180298, lr: 0.0007271669575334498\n",
      "step: 100, accuracy: 0.9296875, loss: 0.20441694557666779, data_loss: 0.20441694557666779, lr: 0.000721917412647993\n",
      "step: 200, accuracy: 0.8984375, loss: 0.2334176003932953, data_loss: 0.2334176003932953, lr: 0.0007167431192660551\n",
      "step: 300, accuracy: 0.8984375, loss: 0.2621130645275116, data_loss: 0.2621130645275116, lr: 0.0007116424708226587\n",
      "step: 400, accuracy: 0.890625, loss: 0.30509471893310547, data_loss: 0.30509471893310547, lr: 0.0007066139061616733\n",
      "step: 468, accuracy: 0.9375, loss: 0.15237535536289215, data_loss: 0.15237535536289215, lr: 0.0007032348804500702\n",
      "Epoch : 10\n",
      "step: 0, accuracy: 0.9140625, loss: 0.24422648549079895, data_loss: 0.24422648549079895, lr: 0.0007031854299978904\n",
      "step: 100, accuracy: 0.9296875, loss: 0.18966954946517944, data_loss: 0.18966954946517944, lr: 0.0006982752601075343\n",
      "step: 200, accuracy: 0.8984375, loss: 0.2191607654094696, data_loss: 0.2191607654094696, lr: 0.000693433187712364\n",
      "step: 300, accuracy: 0.890625, loss: 0.25889986753463745, data_loss: 0.25889986753463745, lr: 0.0006886578059362303\n",
      "step: 400, accuracy: 0.8984375, loss: 0.295074999332428, data_loss: 0.295074999332428, lr: 0.0006839477463921757\n",
      "step: 468, accuracy: 0.9375, loss: 0.1438216269016266, data_loss: 0.1438216269016266, lr: 0.000680781537204711\n",
      "Training, acc: 0.9079833333333334, loss: 0.2523421206315358, data_loss: 0.2523421206315358, lr: 0.000680781537204711 \n",
      "validation || acc : 0.96875 , loss : 0.12175341447194417\n",
      "validation || acc : 0.9080666666666667 , loss : 0.245620263671875\n",
      "Saved model..\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "\n",
    "model.add(Layer_Dense(X.shape[1], 128))\n",
    "model.add(Activation_Relu())\n",
    "model.add(Layer_Dense(128,128))\n",
    "model.add(Activation_Relu())\n",
    "model.add(Layer_Dense(128, 10))\n",
    "model.add(Activation_Softmax())\n",
    "\n",
    "model.set(loss=Loss_CategoricalCrossEntropy(), optimizer= Optimizer_Adam(decay=1e-4), accuracy= Accuracy_Categorical())\n",
    "\n",
    "model.finalize()\n",
    "\n",
    "model.train(X,y, validation_data=(X_test, y_test), epochs=10,  batch_size=128, print_every=100)\n",
    "\n",
    "model.evaluate(X_test, y_test)\n",
    "\n",
    "model.save(\"fashion_mnist.model\")\n",
    "\n",
    "print(\"Saved model..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558da5ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2f540730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation || acc : 0.9080666666666667 , loss : 0.245620263671875\n"
     ]
    }
   ],
   "source": [
    "model2 = Model.load(\"fashion_mnist.model\")\n",
    "\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2212f31e",
   "metadata": {},
   "source": [
    "# Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20f9cbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import matplotlib.pyplot as plt\n",
    "from nnfs.datasets import sine_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "class Layer_Dense:\n",
    "    \n",
    "    # getting the number of inputs and neurons\n",
    "    def __init__(self, n_inputs, n_neurons,\n",
    "                 weight_regularizer_l1=0,\n",
    "                bias_regularizer_l1=0,\n",
    "                weight_regularizer_l2=0,\n",
    "                bias_regularizer_l2=0):\n",
    "        \n",
    "        #initializing random weights in range \n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "        self.weight_regularizer_l1= weight_regularizer_l1\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        \n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "        \n",
    "    # getting the output with (mx +b)\n",
    "    def forward(self, inputs, training):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    \n",
    "    # backward pass which takes the derivative values from the next function \n",
    "    def backward(self, dvalues):\n",
    "        # getting the partial derivative with the inputs\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims = True)\n",
    "        \n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dl1 = np.ones_like(self.weights)\n",
    "            dl1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dl1\n",
    "            \n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "            \n",
    "        if self.bias_regularizer_l1 >0:\n",
    "            dl1 = np.ones_like(self.biases)\n",
    "            dl1[self.biases < 0] = -1\n",
    "            self.biases += self.bias_regularizer_l1 * dl1\n",
    "            \n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
    "        \n",
    "        \n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "        \n",
    "    def get_parameters(self):\n",
    "        return self.weights, self.biases\n",
    "    \n",
    "    def set_parameters(self, weights, biases):\n",
    "        self.weights = weights\n",
    "        self.biases = biases\n",
    "        \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Layer_Dropout:\n",
    "    \n",
    "    def __init__(self, rate):\n",
    "        self.rate = 1- rate\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs, training):\n",
    "        self.intputs = inputs\n",
    "        \n",
    "        if not training:\n",
    "            self.output = inputs.copy()\n",
    "            return\n",
    "        \n",
    "        self.binary_mask = np.random.binomial(1, self.rate, size=inputs.shape) / self.rate\n",
    "        \n",
    "        self.output = inputs * self.binary_mask\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * self.binary_mask\n",
    "        \n",
    "class Layer_Input:\n",
    "    \n",
    "    # forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        self.output = inputs\n",
    "        \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Activation_Relu:\n",
    "    \n",
    "    # The method which takes the input\n",
    "    def forward(self, inputs, training):\n",
    "        \n",
    "        self.inputs = inputs\n",
    "        # returns output with the maximum of 0 and the input value\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <=0] = 0\n",
    "        \n",
    "    def predictions(self, outputs):\n",
    "        return outputs\n",
    "        \n",
    "#---------------------------------------------------------------------\n",
    "        \n",
    "# Creating an Activation function class\n",
    "class Activation_Softmax:\n",
    "    \n",
    "    # The method which takes the input\n",
    "    def forward(self, inputs, training):\n",
    "        self.inputs = inputs\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        \n",
    "        norm_vals = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        \n",
    "        self.output = norm_vals\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        \n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "                \n",
    "                single_output = single_output.reshape(-1,1)\n",
    "                \n",
    "                jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "                \n",
    "                self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "                \n",
    "    def predictions(self, outputs):\n",
    "        return np.argmax(outputs, axis=1)\n",
    "                \n",
    "#---------------------------------------------------------------------\n",
    "                \n",
    "class Activation_Linear:\n",
    "    \n",
    "    # forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        self.inputs = inputs\n",
    "        self.output = inputs\n",
    "        \n",
    "    # backward pass\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "    def predictions(self, outputs):\n",
    "        return outputs\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "    \n",
    "class Activation_Sigmoid:\n",
    "    \n",
    "    def forward(self, inputs, training):\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * (1 - self.output) * self.output\n",
    "        \n",
    "    def predictions(self, outputs):\n",
    "        return (outputs > 0.5) * 1\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "        \n",
    "class Optimizer_SGD:\n",
    "    \n",
    "    # setting learning rate default as 1.\n",
    "    def __init__(self, learning_rate = 1.0, decay=0., momentum =0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def pre_update_parameters(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "        \n",
    "    def update_params(self, layer):\n",
    "        \n",
    "        # if we use momentum\n",
    "        if self.momentum:\n",
    "            # if the layer does not contain momentum arrays\n",
    "            if not hasattr(layer, \"weight_momentums\"):\n",
    "                # creating weight_momentums and bias_momentums arrays initialized with zero\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            \n",
    "            # getting the weight updates with momentum - take previous factors\n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "            \n",
    "            # getting bias updates\n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "        \n",
    "        else:\n",
    "\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates =  -self.current_learning_rate * layer.dbiases\n",
    "            \n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "        \n",
    "    def post_update_parameters(self):\n",
    "        self.iterations +=1\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "        \n",
    "class Optimizer_AdaGrad:\n",
    "    \n",
    "    # setting learning rate default as 1.\n",
    "    def __init__(self, learning_rate = 1.0, decay=0., epsilon =1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def pre_update_parameters(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "        \n",
    "    def update_params(self, layer):\n",
    "        # checking if layer has attribute weight_cache\n",
    "        if not hasattr(layer, \"weight_cache\"):\n",
    "            # creating weight_cache and bias_cache\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        # getting the square of the gradients\n",
    "        layer.weight_cache += layer.dweights **2\n",
    "        layer.bias_cache += layer.dbiases **2\n",
    "        \n",
    "        # calculating the result and adding it to the weights and biases\n",
    "        layer.weights += - self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += - self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "        \n",
    "    def post_update_parameters(self):\n",
    "        self.iterations +=1\n",
    "                \n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class Optimizer_Adam:\n",
    "    \n",
    "    def __init__(self, learning_rate = 0.001, decay=0. ,\n",
    "                 epsilon=1e-7, beta_1=0.9, beta_2 = 0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        \n",
    "    def pre_update_parameters(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "        \n",
    "    def update_params(self, layer):\n",
    "        # checking if layer has attribute weight_cache\n",
    "        if not hasattr(layer, \"weight_cache\"):\n",
    "            \n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            \n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        \n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1- self.beta_1) * layer.dweights\n",
    "        \n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1- self.beta_1) * layer.dbiases\n",
    "        \n",
    "        \n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        \n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "\n",
    "        \n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights **2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases **2\n",
    "        \n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations +1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations +1))\n",
    "        \n",
    "        \n",
    "        layer.weights += - self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected)+ self.epsilon)\n",
    "        layer.biases += - self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected)+ self.epsilon)\n",
    "\n",
    "        \n",
    "    def post_update_parameters(self):\n",
    "        self.iterations +=1\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "                \n",
    "class Loss:\n",
    "    \n",
    "    def regularization_loss(self):\n",
    "        \n",
    "        regularization_loss = 0\n",
    "        \n",
    "        #------------------L1 Regularization------------------------\n",
    "\n",
    "        \n",
    "        for layer in self.trainable_layers:\n",
    "            \n",
    "            if layer.weight_regularizer_l1 > 0:\n",
    "                regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "\n",
    "            if layer.bias_regularizer_l1 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
    "\n",
    "            #------------------L2 Regularization------------------------\n",
    "\n",
    "            if layer.weight_regularizer_l2 > 0:\n",
    "                regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
    "\n",
    "            if layer.bias_regularizer_l2 > 0 :\n",
    "                regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
    "\n",
    "        return regularization_loss\n",
    "    \n",
    "    # set / remember trainable layers\n",
    "    def remember_trainable_layers(self, trainable_layers):\n",
    "        self.trainable_layers = trainable_layers\n",
    "        \n",
    "        \n",
    "    def calculate(self, output, y, *, include_regularization= False):\n",
    "        \n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        \n",
    "        self.accumulated_sum += np.sum(sample_losses)\n",
    "        self.accumulated_count += len(sample_losses)\n",
    "        \n",
    "        \n",
    "        if not include_regularization:\n",
    "            return data_loss\n",
    "        \n",
    "        return data_loss, self.regularization_loss()\n",
    "    \n",
    "    def calculate_accumulated(self, *, include_regularization=False):\n",
    "        \n",
    "        data_loss = self.accumulated_sum / self.accumulated_count\n",
    "        \n",
    "        if not include_regularization:\n",
    "            return data_loss\n",
    "        \n",
    "        return data_loss, self.regularization_loss()\n",
    "    \n",
    "    def new_pass(self):\n",
    "        self.accumulated_sum = 0\n",
    "        self.accumulated_count = 0\n",
    "    \n",
    "    \n",
    "#---------------------------------------------------------------------\n",
    "    \n",
    "class Loss_MeanSquaredError(Loss):\n",
    "    \n",
    "    # forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        sample_losses = np.mean((y_true - y_pred)**2, axis=-1)\n",
    "        \n",
    "        return sample_losses\n",
    "    \n",
    "    # backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        # gradient on values\n",
    "        self.dinputs = -2 * (y_true - dvalues) / outputs\n",
    "        \n",
    "        # normalizing gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "        \n",
    "class Loss_MeanAbsoluteError(Loss):\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        sample_losses = np.mean(np.abs(y_true - y_pred), axis=-1)\n",
    "        \n",
    "        return sample_losses\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        self.dinputs = np.sign(y_true - dvalues)/ outputs\n",
    "        \n",
    "        self.dinputs = self.dinputs / samples\n",
    "         \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "    \n",
    "    def forward(self, y_pred, y_target):\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        # clipping the values so we don't end up with inf error\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "\n",
    "        # checking if 1D array so we just need to get that index value\n",
    "        if len(y_target.shape) == 1:\n",
    "            correct = y_pred_clipped[range(samples), y_target]\n",
    "            \n",
    "        # checking if one-hot-encoded values, then multiplying and summing up\n",
    "        elif len(y_target.shape) == 2:\n",
    "            correct = np.sum(y_pred_clipped * y_target, axis=1)\n",
    "        \n",
    "        negative_log = -np.log(correct)\n",
    "        \n",
    "        return negative_log\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "        \n",
    "        if len(y_true.shape) ==1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "            \n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples\n",
    "    \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Accuracy:\n",
    "    \n",
    "    def calculate(self, predictions, y):\n",
    "        \n",
    "        comparisons = self.compare(predictions, y)\n",
    "        \n",
    "        accuracy = np.mean(comparisons)\n",
    "        \n",
    "        self.accumulated_sum += np.sum(comparisons)\n",
    "        self.accumulated_count += len(comparisons)\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def calculate_accumulated(self):\n",
    "        \n",
    "        accuracy = self.accumulated_sum / self.accumulated_count\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def new_pass(self):\n",
    "        self.accumulated_sum = 0\n",
    "        self.accumulated_count = 0\n",
    "    \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Accuracy_Regression(Accuracy):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.precision = None\n",
    "        \n",
    "    def init(self, y, reinit= False):\n",
    "        if self.precision is None or reinit:\n",
    "            self.precision = np.std(y) / 250\n",
    "            \n",
    "    def compare(self, predictions, y):\n",
    "        return np.absolute(predictions - y) < self.precision\n",
    "        \n",
    "\n",
    "        \n",
    "class Accuracy_Categorical(Accuracy):\n",
    "    \n",
    "    def init(self, y):\n",
    "        pass\n",
    "    \n",
    "    def compare(self, predictions, y):\n",
    "        if len(y.shape) ==2:\n",
    "            y = np.argmax(y, axis=1)\n",
    "            \n",
    "            \n",
    "        return predictions == y\n",
    "        \n",
    "\n",
    "#.......................................................................\n",
    "\n",
    "class Activation_Softmax_Loss_CategoricalCrossEntropy():\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        samples = len(dvalues)\n",
    "        \n",
    "#         # checking if the values are one-hot-encoded\n",
    "#         if len(y_true.shape) ==2:\n",
    "#             y_true = np.argmax(y_true, axis=1)\n",
    "            \n",
    "        \n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "        # calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -=1\n",
    "        \n",
    "        # normalizing gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "    \n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class Loss_BinaryCrossentropy(Loss):\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        sample_losses = -(y_true * np.log(y_pred_clipped) + (1-y_true) * np.log(1- y_pred_clipped))\n",
    "        \n",
    "        sample_losses = np.mean(sample_losses, axis = -1)\n",
    "        \n",
    "        return sample_losses\n",
    "    \n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        self.dinputs = -(y_true / clipped_dvalues - (1-y_true) / (1-clipped_dvalues))/ outputs\n",
    "        \n",
    "        self.dinputs = self.dinputs / samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b88b1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import copy\n",
    "\n",
    "class Model:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # creating a list of network objects\n",
    "        self.layers = []\n",
    "        self.softmax_classifier_output = None\n",
    "        \n",
    "    # adding layers to the model\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    # set loss and optimizer\n",
    "    def set(self, * , loss=None, optimizer=None, accuracy=None):\n",
    "        if loss:\n",
    "            self.loss = loss\n",
    "        if optimizer:\n",
    "            self.optimizer = optimizer\n",
    "        if accuracy:\n",
    "            self.accuracy = accuracy\n",
    "        \n",
    "    # Train the model\n",
    "    def train(self, X, y, *, epochs=1, batch_size=None, print_every=1, validation_data = None):\n",
    "        \n",
    "        self.accuracy.init(y)\n",
    "        \n",
    "        train_steps = 1\n",
    "        \n",
    "        if validation_data :\n",
    "            validation_steps =1\n",
    "            \n",
    "            X_val, y_val = validation_data\n",
    "            \n",
    "        \n",
    "        if batch_size is not None:\n",
    "            \n",
    "            train_steps = len(X) // batch_size\n",
    "            \n",
    "            if train_steps * batch_size < len(X):\n",
    "                train_steps +=1\n",
    "                \n",
    "            if validation_data is not None:\n",
    "                validation_steps = len(X_val) // batch_size\n",
    "                \n",
    "                if validation_steps * batch_size < len(X_val):\n",
    "                    validation_steps +=1\n",
    "        \n",
    "        \n",
    "        for epoch in range(1, epochs+1):\n",
    "            \n",
    "            print(\"Epoch :\" , epoch)\n",
    "            \n",
    "            # reset accumulated values in loss and accuracy objects\n",
    "            self.loss.new_pass()\n",
    "            self.accuracy.new_pass()\n",
    "            \n",
    "            for step in range(train_steps):\n",
    "                \n",
    "                # if batch is not set using the full dataset \n",
    "                if batch_size is None:\n",
    "                    batch_X = X\n",
    "                    batch_y = y\n",
    "                \n",
    "                # otherwise slice the dataset into batches\n",
    "                else:\n",
    "                    batch_X = X[step * batch_size:(step+1) * batch_size]\n",
    "                    batch_y = y[step * batch_size:(step+1) * batch_size]\n",
    "            \n",
    "                output = self.forward(batch_X, training=True)\n",
    "\n",
    "                data_loss, regularization_loss = self.loss.calculate(output, batch_y, include_regularization=True)\n",
    "\n",
    "                loss = data_loss + regularization_loss\n",
    "\n",
    "                predictions = self.output_layer_activation.predictions(output)\n",
    "\n",
    "                accuracy = self.accuracy.calculate(predictions, batch_y)\n",
    "\n",
    "                self.backward(output, batch_y)\n",
    "                \n",
    "                self.optimizer.pre_update_parameters()\n",
    "\n",
    "                for layer in self.trainable_layers:\n",
    "                    self.optimizer.update_params(layer)\n",
    "\n",
    "                self.optimizer.post_update_parameters()\n",
    "\n",
    "                if not step % print_every or step == train_steps - 1:\n",
    "                    print(f\"step: {step}, accuracy: {accuracy}, loss: {loss}, data_loss: {data_loss}, lr: {self.optimizer.current_learning_rate}\")\n",
    "                    \n",
    "                    \n",
    "        epoch_data_loss, epoch_regularization_loss = self.loss.calculate_accumulated(include_regularization = True)\n",
    "        \n",
    "        epoch_loss = epoch_data_loss + epoch_regularization_loss\n",
    "        epoch_accuracy = self.accuracy.calculate_accumulated()\n",
    "        \n",
    "        \n",
    "        print(f\"Training, acc: {epoch_accuracy}, loss: {epoch_loss}, data_loss: {epoch_data_loss}, lr: {self.optimizer.current_learning_rate} \")\n",
    "        \n",
    "        if validation_data is not None:\n",
    "            \n",
    "            self.evaluate(*validation_data, batch_size = batch_size)\n",
    "\n",
    "    # finalize the model\n",
    "    def finalize(self):\n",
    "        \n",
    "        #create and set the input layer\n",
    "        self.input_layer = Layer_Input()\n",
    "        \n",
    "        # count all objects\n",
    "        layer_count = len(self.layers)\n",
    "        \n",
    "        # initializing a list to store trainable layers\n",
    "        self.trainable_layers = []\n",
    "        \n",
    "        for i in range(layer_count):\n",
    "            \n",
    "            if i == 0:\n",
    "                self.layers[i].prev = self.input_layer\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "                \n",
    "            elif i < layer_count -1:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "                \n",
    "            else:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.loss\n",
    "                self.output_layer_activation = self.layers[i]\n",
    "                \n",
    "            # checking if the layer has an attribute weights\n",
    "            # then adding it to the trainable layers list\n",
    "            if hasattr(self.layers[i], \"weights\"):\n",
    "                self.trainable_layers.append(self.layers[i])\n",
    "                \n",
    "            if self.loss is not None:   \n",
    "                self.loss.remember_trainable_layers(self.trainable_layers)\n",
    "            \n",
    "        if isinstance(self.layers[-1], Activation_Softmax) and isinstance(self.loss, Loss_CategoricalCrossEntropy):\n",
    "                self.softmax_classifier_output =  Activation_Softmax_Loss_CategoricalCrossEntropy()\n",
    "            \n",
    "                \n",
    "                \n",
    "    # forward method || performs forward pass\n",
    "    def forward(self, X, training):\n",
    "        \n",
    "        # calling the forward method on the input layer with X data\n",
    "        # this will ensure the next layer or technically the first layer\n",
    "        # gets the data it needs from the previous layer\n",
    "        self.input_layer.forward(X, training)\n",
    "        \n",
    "        # looping over the layers and passing the previous layer output to each layer\n",
    "        for layer in self.layers:\n",
    "            layer.forward(layer.prev.output, training)\n",
    "            \n",
    "        # returning the latest layer that is the last activation function output\n",
    "        return layer.output\n",
    "    \n",
    "    \n",
    "    # performs backward pass\n",
    "    def backward(self, output, y):\n",
    "        \n",
    "        self.loss.backward(output, y)\n",
    "        \n",
    "        for layer in reversed(self.layers):\n",
    "            layer.backward(layer.next.dinputs)\n",
    "            \n",
    "            \n",
    "    # evaluate the model using passed in dataset\n",
    "    def evaluate(self, X_val, y_val, *, batch_size=None):\n",
    "        \n",
    "        validation_steps = 1\n",
    "        \n",
    "        if batch_size:\n",
    "            \n",
    "            validation_steps = len(X_val) // batch_size\n",
    "            \n",
    "            if validation_steps * batch_size < len(X_val):\n",
    "                validation_steps +=1\n",
    "        \n",
    "        self.loss.new_pass()\n",
    "        self.accuracy.new_pass()\n",
    "\n",
    "        for step in range(validation_steps):\n",
    "\n",
    "            if not batch_size:\n",
    "                batch_X = X_val\n",
    "                batch_y = y_val\n",
    "\n",
    "            else:\n",
    "                batch_X = X_val[step * batch_size:(step+1)* batch_size]\n",
    "                batch_y = y_val[step * batch_size:(step+1)* batch_size]\n",
    "\n",
    "        output = self.forward(batch_X,training= False)\n",
    "\n",
    "        loss = self.loss.calculate(output, batch_y)\n",
    "\n",
    "        predictions = self.output_layer_activation.predictions(output)\n",
    "\n",
    "        self.accuracy.calculate(predictions, batch_y)\n",
    "\n",
    "        validation_loss = self.loss.calculate_accumulated()\n",
    "        validation_accuracy = self.accuracy.calculate_accumulated()\n",
    "\n",
    "        print(f\"validation || acc : {validation_accuracy} , loss : {validation_loss}\")\n",
    "\n",
    "        \n",
    "    # retrieves and returns the parameters of trainable layers\n",
    "    def get_parameters(self):\n",
    "        \n",
    "        # list of parameters\n",
    "        parameters = []\n",
    "        \n",
    "        # looping over the trainable layers or the dense layers and storing\n",
    "        # their parameters\n",
    "        for layer in self.trainable_layers:\n",
    "            parameters.append(layer.get_parameters())\n",
    "            \n",
    "        return parameters\n",
    "\n",
    "    def set_parameters(self, parameters):\n",
    "        for parameter_set, layer in zip(parameters, self.trainable_layers):\n",
    "            layer.set_parameters(*parameter_set)\n",
    "            \n",
    "    def save_parameters(self, path):\n",
    "        \n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(self.get_parameters(), f)\n",
    "            \n",
    "            \n",
    "    def load_patameters(self, path):\n",
    "        with open(path, 'rb') as f:\n",
    "            self.set_parameters(pickle.load(f))\n",
    "            \n",
    "            \n",
    "    def predict(self, X, *, batch_size=None):\n",
    "        \n",
    "        prediction_steps= 1\n",
    "        \n",
    "        if batch_size:\n",
    "            prediction_steps = len(X) // batch_size\n",
    "            \n",
    "            if prediction_steps * batch_size < len(X):\n",
    "                prediction_steps +=1\n",
    "        \n",
    "        output = []\n",
    "        \n",
    "        for step in range(prediction_steps):\n",
    "            \n",
    "            if batch_size is None:\n",
    "                batch_X = X\n",
    "            \n",
    "            else:\n",
    "                batch_X = X[step * batch_size:(step+1)* batch_size]\n",
    "                \n",
    "            batch_output = self.forward(batch_X, training = False)\n",
    "            \n",
    "            output.append(batch_output)\n",
    "            \n",
    "            \n",
    "        return np.vstack(output)\n",
    "        \n",
    "    \n",
    "    def save(self, path):\n",
    "        \n",
    "        # making a deep copy of the current model instance\n",
    "        model = copy.deepcopy(self)\n",
    "        \n",
    "        model.loss.new_pass()\n",
    "        model.accuracy.new_pass()\n",
    "        \n",
    "        model.input_layer.__dict__.pop('output', None)\n",
    "        model.loss.__dict__.pop('dinputs', None)\n",
    "        \n",
    "        for layer in model.layers:\n",
    "            for property in ['inputs', 'output', 'dinputs', 'dweights', 'dbiases']:\n",
    "                \n",
    "                layer.__dict__.pop(property, None)\n",
    "        \n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        \n",
    "        with open(path, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "            \n",
    "        return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "498f6f06",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      4\u001b[0m X, y, X_test, y_test \u001b[38;5;241m=\u001b[39m create_data_mnist()\n\u001b[1;32m----> 6\u001b[0m X_test \u001b[38;5;241m=\u001b[39m (\u001b[43mX_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m(X_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m127.5\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m127.5\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "X, y, X_test, y_test = create_data_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "554ffab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "X_test = (X_test.reshape(X_test.shape[0], -1).astype(np.float32) - 127.5)/127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f4a0413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 3 0 0]\n"
     ]
    }
   ],
   "source": [
    "model = Model.load(\"fashion_mnist.model\")\n",
    "\n",
    "confidence = model.predict(X_test[:5])\n",
    "predictions = model.output_layer_activation.predictions(confidence)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927a3bd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f0fe66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
